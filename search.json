[{"title":"网易牛马日志-week4","url":"/2024/12/15/网易牛马日志-week4/","content":"\n## 需求5：优化prompt\n\n### 1.提示词优化\n\n> 学习了prompt工程，主要的优化思路有（[参考连接](https://mp.weixin.qq.com/s/QDzbfUkvSEalLxm3XX9rtA)）：\n>\n> 1. 角色提示：“假设我是一个刚入行的电商买家”，类似这种可以将gpt带入角色，以该角色的视角进行输出\n> 2. 思维链提示：让gpt一步一步进行思考，例如计算1+2+3，先让gpt算出1+2=3，然后再去算3+3=6，有了思维链能够使ai清晰思考路径，犯错的可能性降低\n> 3. 思维树提示：跟上面的思维链比起来，思维树要考虑不同的分叉，在执行过程中要考虑不同的情况\n> 4. 自一致性提示：将模型的温度升高，多次去执行一个问题，返回其中最高频率的答案。对于推荐词的场景不太适用，因为调用次数有限而且时延较大，被mentor否决。\n> 5. ReAct提示：用python的api进行调用，功能强大但用不了。\n     >    综合以上的方法，只有思维链可以作为优化。\n\n1. 近似词：中译英\n\n```text\nString synonymsPrompt = MessageFormat.format(\"与\\\"{0}\\\"意思相近的产品有哪些，\n        请用{1}列举10个产品名。要求不包含\\\"{0}\\\"，\n        输出格式为：jsonArray\", formatValue, language) + \",[]\";\n```\n\n```text\nString synonymsPrompt = MessageFormat.format(\"What are the products with the similar meaning of \\\"{0}\\\",\\n\" +\n        \"Please list 10 product names in {1}. My request is not to include \\\"{0}\\\",\\n\" +\n        \"The output format is jsonArray\", formatValue, language) + \",[]\";\n```\n\n2. 电商词：转为中文思维链\n\n```text\nString platformPrompt = MessageFormat.format(\"假设我是一个新入行的电商卖家，\n        主要经营的是\\\"{0}\\\"。我希望能有更多的用户在amazon.com等同类电商网站中搜索到我，\n        我可以使用怎样的{1}产品名对自己的产品进行描述？要求不包含\\\"{0}\\\"，\n        请去掉尽可能多的长尾词，提炼共同的产品描述词。帮我用{2}同义替换10个不同的产品描述。仅输出替换后的数据，\n        格式为：jsonArray\", formatValue, language, language) + \",[]\";   \n```\n\n```text\nString platformPrompt = MessageFormat.format(\"请按照如下提示一步步推理：\\n\" +\n        \"第一步，请将产品\\\"{0}\\\"从品牌、型号、材料、用途等方面进行扩展，尽量细分且具体到某个品牌的某个产品。\\n\" +\n        \"第二步，思考该产品在amazon.com等同类电商网站中显示给用户的商品词是什么。\\n\" +\n        \"第三步，提取商品词中的关键信息，不允许出现品牌和型号，长度严格限制在30个字母以内。\\n\" +\n        \"综合以上三步，仅用{1}输出十个这样的产品名，要求不包含\\\"{0}\\\"格式为：jsonArray,[]\", formatValue, language) + \",[]\";\n```\n\n3. 行业词：中译英\n\n```text\nprompt = \"我是一个新入行的外贸供应商，主要经营的是\"+ formatValue +\"。我的产品可以应用于哪些行业的哪些产品？帮我列举3个行业，并描述推荐这些行业的理由，在每个行业下列举5个使用该产品制造出来的产品名称。\\n\" +\n        \"用json输出，格式为：\\n\" +\n        \"行业1\\n\" +\n        \"行业名：xxx\\n\" +\n        \"推荐理由：xxx\\n\" +\n        \"相关产品：xxx\\n\" +\n        \"要求：\\n\" +\n        \"1、行业名称用{\"+languageVersion+\"}。\\n\" +\n        \"2、推荐理由用{\"+languageVersion+\"}。\\n\" +\n        \"3、相关产品用{\"+language+\"}。\\n\" +\n        \"4、产品名称不能重复。\\n\" +\n        \"5、产品名称不需要包含材质等限制。\\n\" +\n        \"6、相关产品用#格开。\" +\n        \"7、不包含\\\"\"+formatValue+\"\\\"。\";\n```\n\n```text\nprompt = \"I am a new foreign trade supplier, the main business is \\\"\"+formatValue+\"\\\". Which products can my product be used in which industries? Give me a list of 3 industries and describe the reasons why you recommend them, and list 5 names of products made with that product under each industry.\\n\" +\n        \"Output in json format:\\n\" +\n        \" 行业1\\n\" +\n        \" 行业名：xxx\\n\" +\n        \" 推荐理由：xxx\\n\" +\n        \" 相关产品：xxx\\n\" +\n        \"Requirement: \\\"\\n\" +\n        \"1. the industry name with {\"+languageVersion+\"}.\\n\" +\n        \"2. recommended reasons use {\"+languageVersion+\"+}.\\n\" +\n        \"3. Use {\"+language+\"} for related products.\\n\" +\n        \"4. the product name can not be repeated.\\n\" +\n        \"5. the product name does not need to include material and other restrictions.\\n\" +\n        \"6. related products are delimited with #\\n\" +\n        \"7. \\\"\"+formatValue+\"\\\" is not included.\";\n```\n\n三个小小的优化耗费了我一周的时间，实际上也没多少提升。\n\n### 2.代码逻辑\n\n#### 近似词和电商词\n\n- 首先判断是否为空，或者是否为敏感词（调用网易易盾），随后传入用户信息，访问服务。\n- 服务内部：传入参数中语言为空默认为英语，然后用CompletableFuture.supplyAsync()搭配线程池完成gpt的两次请求。线程池如下：\n\n```java\n//线程工厂仅仅改名，拒绝策略是发起者自己消化\nprivate static final ThreadFactory GPT_THREAD_FACTORY = new ThreadFactoryBuilder().setNameFormat(\"GptGrpcWrapper-gpt-pool-%d\").build();\nprivate static final ExecutorService GPT_REQUEST_EXECUTOR = new ThreadPoolExecutor(20,\n        40, 60 * 5L, TimeUnit.SECONDS, new LinkedBlockingDeque<>(3000), GPT_THREAD_FACTORY, new ThreadPoolExecutor.CallerRunsPolicy());\n```\n\n- 请求成功的结果缓存在redis中100天，缓存的key是\"cacheKeyPreFix + \"v6\" + \":\" + gptRecommendReqVO.getLanguage()+\":\"+gptRecommendReqVO.getValue();\"\n\n```java\nprivate Set<String> getRecommendWordV1(GptRecommendReqVO gptRecommendReqVO, AuthInfo authInfo, String prompt, String cacheKeyPreFix) {\n    String cacheValue = \"\";\n    try {\n        if (StringUtils.isBlank(gptRecommendReqVO.getValue())) {\n            return Collections.emptySet();\n        }\n        String formatValue = FormatUtil.escapeFormat(gptRecommendReqVO.getValue());\n        if(StringUtils.isBlank(formatValue) || formatValue.length()<=1){\n            log.info(\"GptGrpcWrapper value:{},formatValue:{}\",gptRecommendReqVO.getValue(),formatValue);\n            return Collections.emptySet();\n        }\n        if(gptRecommendReqVO.getLanguage()==null){\n            gptRecommendReqVO.setLanguage(\"英语\");\n        }\n\n        String language = gptRecommendReqVO.getLanguage();\n        String cacheKey = cacheKeyPreFix + \"v6\" + \":\" + gptRecommendReqVO.getLanguage()+\":\"+gptRecommendReqVO.getValue();\n        cacheValue = recommendWordCache.get(cacheKey);\n        //命中\n        if(StringUtils.isNotBlank(cacheValue)){\n            log.info(\"GptGrpcWrapper call success,param:{},result:{}\",JSON.toJSONString(gptRecommendReqVO),cacheValue);\n            //大段文字中的[\"xxx1\",\"xxx2\",\"xxx3\"]\n            if(cacheValue.contains(\"[\") && cacheValue.contains(\"]\")){\n                cacheValue = cacheValue.substring(cacheValue.indexOf(\"[\"), cacheValue.lastIndexOf(\"]\") + 1);\n            }\n            //只取前6个，这个数值是前端定的\n            Set<String> synonyms = getLimitSize(Lists.newArrayList(JSON.parseArray(cacheValue, String.class)), gptRecommendReqVO.getSize());\n            return synonyms;\n        }\n        //未命中\n        log.info(\"GptGrpcWrapper call param:{},prompt:{}\",JSON.toJSONString(gptRecommendReqVO),prompt);\n        CompletableFuture<String> future = CompletableFuture.supplyAsync(\n                () -> gptRequest(authInfo.getOrgId(),authInfo.getAccId(),authInfo.getEmail(),prompt, GPTModelVersionEnum.GPT_4O_MINI.getVersion()), GPT_REQUEST_EXECUTOR);\n        //这里实际上还是同步请求，\"gpt-recommard-future\"是一个日志的名称，超时时间120s\n        String res = (String)FutureResultUtil.getResult(\"gpt-recommard-future\",future,120, TimeUnit.SECONDS);\n        if (StringUtils.isBlank(res)) {\n            log.info(\"getRecommend failed.name:{},language:{}\",formatValue, language);\n            return Collections.emptySet();\n        }\n        cacheValue = res;\n        // 推荐词缓存100天\n        recommendWordCache.set(cacheKey,cacheValue,TimeUnit.DAYS.toMillis(100));\n        log.info(\"GptGrpcWrapper call success,param:{},result:{}\",JSON.toJSONString(gptRecommendReqVO),cacheValue);\n        if(cacheValue.contains(\"[\") && cacheValue.contains(\"]\")){\n            cacheValue = cacheValue.substring(cacheValue.indexOf(\"[\"), cacheValue.lastIndexOf(\"]\") + 1);\n        }\n        Set<String> synonyms = getLimitSize(Lists.newArrayList(JSON.parseArray(cacheValue, String.class)), gptRecommendReqVO.getSize());\n        return synonyms;\n    }catch (Exception e) {\n        //gpt出的词如果不常规。例如是“好的，接下来为您输出....”，在JSON.parseArray就会报异常不返回给前端。\n        log.error(\"getGptRecommend param:{}\",JSON.toJSONString(gptRecommendReqVO), e);\n        return Collections.emptySet();\n    }\n}\n```\n\n> 1. redis只存gpt原始回答，可以改成处理后的回答\n> 2. 线程池实际上只将近似词和电商词做了异步，在这两个单独的请求里面用future实际还是同步。\n> 3. gpt如果输出不理想有两重处理，首先看是否包含了\"[\"xxx1\",\"xxx2\",\"xxx3\"]\"，用subString提取出来，实在是没有那在parseObject就会报错，结果返回为空。\n\n- 最后处理近似词和电商词的结果，用一个大的hashset去重（但是大小写敏感，这里可以优化成全部小写再比较）\n- 调用GRPC（有道翻译）兜底，先判断是否出现中文，有中文就翻译。批量翻译返回值是map，key是原文，value是翻译后的。最后进行组装返回。\n\n**总结：调用易盾进行敏感词检测，线程池+CompletableFuture完成两次gpt查询，gpt查询做了一系列异常处理，在redis中缓存100天，最后调用有道翻译的grpc进行兜底。**\n\n\n#### 行业词\n\n- 首先判断是否为空，或者是否为敏感词（调用网易易盾），随后传入用户信息，访问服务。\n- 服务内部：处理逻辑跟上文类似，在redis中存储gpt原始回答，但是由于输出的是json，相比于近似词和行业词的数组，这里有改变。\n- 去除\"\\`\\`\\`json\"和\"\\`\\`\\`\"这类md语法，再根据\"行业一，二，三\"json形式包装。\n- 这里要做一些异常处理以增加可用性和鲁棒性，虽然提示词里面写的是用\"\\#\"隔开，但是实际上可能返回的还是\",\"或者\"、\"，用一个函数对其进行拆分。\n- 最后调用有道翻译兜底。\n\n## 需求6：订阅更新任务\n\n订阅更新链路：\n\n1. 用户前端点击页面就会更新订阅公司的watchTime字段，一天之内只更新一次。\n2. xxljob每周扫库，搜索的是订阅公司的那个表，将watchTime在前一周的公司发送kafka（自增id而不是公司id）\n3. kafka消费者拿到这些id去数据库找对应的公司，将订阅公司表项和公司实体项（从es来的，如果前者有companyId就直接查询，如果没有就需要根据名字和地区来查询并保存）作为参数传分别分析**facebook信息，facebook提及信息，海关信息，联系人信息**，开了一个线程池并行处理这些。\n\n### 1.facebook信息\n\n代码太长了，梳理一下就这几部分：\n\n1. 根据公司实体中的相关facebook链接去请求GRPC接口，返回facebook实体，其中包含个人信息，点赞数量，最近推文等信息。\n2. 从这里开始要注意逻辑，比对的是**GRPC请求来的数据**和**当前数据库中最新的log**。\n3. 去数据库中查询7种type的log，先全部查出来，然后再通过stream和sort得到最近时间的log。\n4. 将这些log分别与GRPC请求来的数据进行比较，例如判断facebook是否更新了，将最近的一条相关log的内容拉出来与GRPC请求来的数据进行比较，如果不一样就需要更新，在库里面新增一条。\n\n> 1. facebook地址初始化/变更\n> 2. facebook邮箱初始化/变更\n> 3. facebook网站地址初始化/变更\n> 4. facebook电话初始化/变更\n> 5. facebook点赞、关注初始化\n> 6. facebook中发布了新帖子\n> 7. facebook获得1个新评论\n\n### 2.facebook提及信息\n\n1. 类似上面的，提及消息也是从GRPC来的，如果返回为null直接返回，这里是一个list，只关注最新的，所以也有stream+sort。\n2. 去数据库查询当前最新的log，主要关注的是这个记录的createTime。\n3. 如果数据库没有，那就说明是最新更新的，向数据库插入一条新的log表明有被提及，直接返回。\n4. 如果数据库有但log的时间比GRPC来的早，说明有新提及，进行更新。\n\n### 3.海关信息\n\n1. 将公司名称格式化，随后去海关数据里面查找新增进出口数据，分别放在两个list里面。\n2. 如果有进出口就分别新增log。\n\n```java\npublic void customProcess(TbGlobalCollectEntity record, CompanySearchBO companyBO, Date startDate, Date endDate) {\n    //海关那边的统一格式\"CompanyName-COMBINE-TANZANIA\"例如\"101 INVESTMENT-COMBINE-TANZANIA\"\n    String nameAndCountry = CompanyFormatUtils.combineCompanyNameAndCountry(companyBO.getName(), companyBO.getCountry());\n    //去取出昨天的新增hscode，这个表主要是海关系统在维护\n    Map<String, CustomInfoBO> map = customDataComponent.batchGetCompanyNewTrxHscode(Lists.newArrayList(nameAndCountry), startDate, endDate);\n    List<TbGlobalCollectLogEntity> tbGlobalCollectLogEntities = Lists.newArrayList();\n    if(map.containsKey(nameAndCountry)){\n        CustomInfoBO customInfoBO = map.get(nameAndCountry);\n        //出口\n        if(!CollectionUtils.isEmpty(customInfoBO.getExportHsCodeList())){\n            TbGlobalCollectLogEntity t = new TbGlobalCollectLogEntity();\n            t.setCollectId(record.getId());\n            t.setLogDesc(\"新供应了hscode为\" + Strings.join(customInfoBO.getExportHsCodeList(), \",\") + \"等的商品\");\n            t.setContent(JSON.toJSONString(customInfoBO.getExportHsCodeList()));\n            t.setType(CollectLogTypeEnum.CUSTOM_SHN.getType());\n            t.setCreateTime(new Date());\n            t.setUpdateTime(new Date());\n            t.setInit12(0);\n            tbGlobalCollectLogEntities.add(t);\n        }\n        //进口\n        if(!CollectionUtils.isEmpty(customInfoBO.getImportHsCodeList())){\n            TbGlobalCollectLogEntity t = new TbGlobalCollectLogEntity();\n            t.setCollectId(record.getId());\n            t.setLogDesc(\"新采购了hscode为\" + Strings.join(customInfoBO.getImportHsCodeList(), \",\") + \"等的商品\");\n            t.setContent(JSON.toJSONString(customInfoBO.getImportHsCodeList()));\n            t.setType(CollectLogTypeEnum.CUSTOM_CON.getType());\n            t.setCreateTime(new Date());\n            t.setUpdateTime(new Date());\n            t.setInit12(0);\n            tbGlobalCollectLogEntities.add(t);\n        }\n    }\n    if(!CollectionUtils.isEmpty(tbGlobalCollectLogEntities)){\n        tbGlobalCollectLogEntityService.saveAll(tbGlobalCollectLogEntities);\n    }\n}\n```\n\n### 4.联系人信息\n"},{"title":"网易牛马日志-week3","url":"/2024/12/05/网易牛马日志-week3/","content":"# 需求3：优化应用侧分页查询收藏接口\n\n利用上watchTime字段，xxljob只更新上一周查看过的条目，可以减少带宽。\n\n## 1.改造判断逻辑\n\n保存watchTime，首先需要获得该用户的accountId，还要获取某一条推荐对象的旧watchTime，判断是否是今天，如果是一天那就不用更新，如果是一天就需要更新。\n\n```text\nif (!pageRes.getContent().isEmpty()){\n    Date oldWatchTime = pageRes.getContent().get(0).getWatchTime();\n    watchTimeJudgeAndUpdate(Long.parseLong(AuthInfoUtils.getContext().getAuthInfo().getAccId()),oldWatchTime);\n}\n```\n\n> 这个方法按道理来说要变成异步的，看后续用多线程或者@Async解决\n\n```java\nprivate void watchTimeJudgeAndUpdate(Long accountId,Date oldWatchTime){\n    try {\n        //一天只保存一次watchTime\n        if (oldWatchTime == null || !isSameDay(oldWatchTime,new Date())){\n            tbGlobalCollectEntityService.updateWatchTime(accountId);\n        }\n        log.info(\"watchTimeUpdate success , accountId : {}\",accountId);\n    }\n    catch (Throwable e){\n        log.error(\"watchTimeUpdate failed , accountId : {}\",accountId,e);\n    }\n}\n\nprivate boolean isSameDay(Date date1, Date date2) {\n    SimpleDateFormat sdf = new SimpleDateFormat(\"yyyyMMdd\");\n    return sdf.format(date1).equals(sdf.format(date2));\n}\n```\n\n这里刚开始是用了jpa的抽象仓库一个一个更新，但是mentor说效率太低了，不如直接写sql，在这个方法里面支持跟mybatis那样直接执行sql。\n\n```java\n@Service\npublic class TbGlobalCollectEntityService extends AbstractEasyEntityService<TbGlobalCollectEntity> {\n\n    @Autowired\n    private TbGlobalCollectRepository repository;\n    public void updateWatchTime(Long accountId){\n        repository.updateWatchTime(accountId);\n    }\n}\n```\n\nsql是更新所有当前accountId下，没有取消掉的推荐条目，把他们的watchTime都改为现在。\n\n这里不需要返回当前时间，就更加说明这个判断逻辑跟前端没什么关系了，所以应该用异步。\n\n```text\n@Modifying\n@Transactional\n@Query(value = \"update TbGlobalCollectEntity p set p.watchTime=NOW() where p.status != -1 and p.accountId = :accountId\")\nint updateWatchTime(@Param(\"accountId\") Long accountId);\n```\n\n## 2.xxljob改造\n\n保存了watchTime，每周执行一次的定时任务需要改成查询所有watchTime为上周的条目。\n\nDateUtil这个工具可以找到今天0点和上周0点的Date对象，然后再用jpa进行查询，把所有的数据投入kafka里。\n\n> 优化的本意是减少部分挖掘facebook的流量。但是因为在这个topic里面不止有这个任务，还有部分通讯录挖掘的任务。减少了这部分流量同时也会减少通讯录挖掘的流量，但是后者更加有价值，并不应该减少。所以按照道理来说应该分为两个topic进行操作。\n\n```java\npublic void collectLogUpdate(int param) {\n    Date endDate = DateUtil.trimAndIncreDate(new Date(),0);\n    Date startDate = DateUtil.trimAndDecreDate(new Date(), param);\n    List<Specification<TbGlobalCollectEntity>> specifications = Lists.newArrayList();\n    specifications.add(of(TbGlobalCollectEntity.class, \"status\", NEQ, GlobalSubscribeStatusEnum.DELETE.getStatus()));\n    specifications.add(of(TbGlobalCollectEntity.class, \"watchTime\", GTE, startDate));\n    specifications.add(of(TbGlobalCollectEntity.class, \"watchTime\", LTE, endDate));\n    List<TbGlobalCollectEntity> records = tbGlobalCollectEntityService.findAll(specifications);\n    if (CollectionUtils.isEmpty(records)) {\n        return;\n    }\n\n    for (TbGlobalCollectEntity record : records) {\n        kafkaProducer.send(KafkaConstants.COWORK_GLOBAL_COLLECT_DETAIL_CHANGE, String.valueOf(record.getId()), String.valueOf(record.getId()));\n    }\n}\n```\n\n## 需求4：查验500条数据\n\n这个的工作量不在代码上而在于excel上，学到了一些技巧。\n\n```text\nGET cowork_global_domain_info/_search\n{\n  \"query\": {\n    \"bool\": {\n      \" must \": [\n        {\n          \"exists\": {\n            \"field\": \"recommendData\"\n          }\n        }\n      ]\n    }\n  }\n    ,\"sort\": {\n    \"_script\": {\n      \"script\": \"Math.random()\",\n      \"type\": \"number\",\n      \"order\": \"asc\"\n    }\n  }\n  ,\"_source\": [\"recommendData\"]\n}\n```\n\n需要注意的点：\n\n1. 随机从917w中找，这里sort要用random脚本\n2. 最好不要用滚动查询，有多少条是多少条\n3. searchHit对象是一个大的对象DomainIndexBO，recommendData只是其中的一个字段。所以getSourceAsString包装的是DomainIndexBO，刚开始类型转换没弄明白耽误很久。\n4. excel小技巧：在这种需要保留很多字段的时候可以用￥(dollar符)分开，因为很少有业务里面会用，防止误操作。例如在这个场景里面日志可以打印成\"{recommendData.domain}￥{recomendData.other}\"，然后在excel按照￥分行。\n\n```java\npublic void esGet() {\n    Script script = new Script(\"Math.random()\");\n    ScriptSortBuilder sortBuilder = new ScriptSortBuilder(script, ScriptSortBuilder.ScriptSortType.NUMBER)\n            .order(SortOrder.ASC);\n    BoolQueryBuilder boolQuery = QueryBuilders.boolQuery();\n    boolQuery.must(QueryBuilders.existsQuery(\"recommendData\"));\n    SearchSourceBuilder sourceBuilder = new SearchSourceBuilder()\n            .query(boolQuery) // 查询参数\n            .fetchSource(new String[]{\"recommendData\"}, null)\n            .sort(sortBuilder)\n            .size(500);\n    SearchHit[] searchHits = domainDataIndex.query(sourceBuilder);\n    int i = 1;\n    for (SearchHit searchHit : searchHits) {\n        DomainIndexBO domainIndexBO = JSON.parseObject(searchHit.getSourceAsString(),DomainIndexBO.class);\n        RecommendDataBO recommendData = domainIndexBO.getRecommendData();\n        StringBuilder stringBuilder = new StringBuilder();\n        stringBuilder.append(\"$\");\n        stringBuilder.append(recommendData.getDomain()).append(\"$\");\n        stringBuilder.append(String.join(\",\", recommendData.getProducts())).append(\"$\");\n        if (recommendData.getAliCategories().isEmpty()){\n            stringBuilder.append(\"null\");\n        }\n        else {\n            for (RecommendDataBO.AliCategory aliCategory : recommendData.getAliCategories()) {\n                stringBuilder.append(aliCategory.getName()).append(\":\").append(aliCategory.getScore()).append(\",\");\n            }\n        }\n\n        stringBuilder.append(\"$\").append(recommendData.getIsBizP()).append(\"$\");\n        stringBuilder.append(recommendData.getIsBizVersion()).append(\"$\");\n        stringBuilder.append(recommendData.getProductsVersion()).append(\"$\");\n        if (StringUtils.isBlank(recommendData.getAliCategoryVersion())){\n            stringBuilder.append(\"null\");\n        }\n        else {\n            stringBuilder.append(recommendData.getAliCategoryVersion());\n        }\n\n\n        String result = stringBuilder.toString();\n        log.info(\"第\"+i+\"个:\"+result);\n        i++;\n    }\n}\n```\n\n。。。未完待续","tags":["Java"]},{"title":"网易牛马日志-week1-2","url":"/2024/12/02/网易牛马日志-week1-2/","content":"\n# 需求1：与推荐侧数据同步\n\n## 1.配置kafka\n\nkafka批量消费，设置消费者组，使得每次消费的时候从头消费\n\n> 消费者组能从头开始消费必须得有消息，也就是说如果消息过期了还是消费不了的，这个过期时间在delete.retention.ms里面配置，当前topic的过期时间是604800000，也就是一个礼拜。\n>\n> 还有一种办法是通过kafka给定的某个脚本参数来指定offset的消费位置。\n\n```java\n@Bean\npublic KafkaListenerContainerFactory<ConcurrentMessageListenerContainer<String, String>>\nrecommendDomainDataListenerContainer(KafkaProperties kafkaProperties) {\n    ConcurrentKafkaListenerContainerFactory<String, String> factory = new ConcurrentKafkaListenerContainerFactory<>();\n    Map<String, Object> map = kafkaProperties.buildConsumerProperties();\n    //批量消费50个\n    map.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, 50);\n    map.put(ConsumerConfig.MAX_POLL_INTERVAL_MS_CONFIG, 10 * 60 * 1000);\n    map.put(ConsumerConfig.HEARTBEAT_INTERVAL_MS_CONFIG, 3000); // 3秒上报一次心跳\n    map.put(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG, 10 * 60 * 1000);\n    map.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\");//消费者组每次都从头开始消费\n    ConsumerFactory<String, String> consumerFactory = new DefaultKafkaConsumerFactory<>(map);\n    factory.setConsumerFactory(consumerFactory);\n    factory.setConcurrency(2); // 2个线程\n    factory.setBatchListener(true);\n    return factory;\n}\n```\n\n## 2.配置kafka的消费者\n\n由于是批量请求rpc接口，因此需要解决请求失败带来的**消息丢失**问题，意思是如果请求失败了，这些domain还会被消费，这里的思路是用一个set来保存没有消费的domain，如果在某处出错了，就会重新往kafka里面发消息，实现不漏消息。\n\n> 2024.12.2 在这上面栽了跟头\n>\n> 想法很美好但是实际上线还是出现了漏消息，成功率只有90%。\n>\n> 首先这个domainSet不能设置成全局变量，因为所有的线程都会更改这个变量，会有复杂的并发安全问题。其次是可能出现异常的时间点：\n>\n> - 消费的时候就出现问题，可能性很小\n> - 拿着所有50条去请求rpc接口的时候\n> - rpc接口失败或者别的状态码\n> - es更新出错\n>\n> 由此可见需要在rpc接口请求之前就要初始化domainSet，所以在消费消息的时候就需要同时加入domainSet。但如果rpc接口请求成功了，需要清空domainSet，为什么呢？因为有可能domain是50个，那边只有40个可返回域名，如果再按照50个就会有消息被重复发送，例如虽然消息里面有www.baidu.com，但是rpc接口里面死活没有，这个时候再去重新往kafka里面投重新消费是不合适的，所以在这里需要遍历response，然后保存进清空了的domainSet里面。\n\n```java\n@KafkaListener(\n        //这里是{}，可以配置多个topic\n        topics = {TOPIC},\n        groupId = \"recommendation-ready-group-bugfix-v1\",\n        containerFactory = \"recommendDomainDataListenerContainer\"\n)\npublic void listener(List<ConsumerRecord<String, String>> records) {\n    log.info(\"RecommendDomainDataListener | size:{}\", records.size());\n    Set<String> domains = new HashSet<>();\n    Set<String> domainSet = new HashSet<>();\n    StopWatch sw = new StopWatch();\n    sw.start();\n    try {\n        for(ConsumerRecord<String, String> record :records) {\n            JSONObject kafkaMsg = JSON.parseObject(record.value());\n            String domain = kafkaMsg.getString(\"domain\");\n            domain = DomainUtil.format(domain);\n            if(StringUtils.isBlank(domain)) {\n                continue;\n            }\n            domains.add(domain);\n            domainSet.add(domain);\n        }\n        batchGetRecommendationDetail(domains,domainSet);\n    }\n    catch (Throwable e){\n        for (String domain : domainSet){\n            Map<String,String> map = new HashMap<>();\n            map.put(\"domain\",domain);\n            kafkaProducer.send(TOPIC, domain, JSON.toJSONString(map));\n        }\n        log.error(\"RecommendDomainDataListener | consumer message error. put in queue {}.\", JSON.toJSONString(domains),e);\n    }\n    finally {\n        sw.stop();\n        log.info(\"RecommendDomainDataListener | {}, cost: {}\", domains.size(), sw.getTime());\n    }\n}\n```\n\n## 3.GRPC请求\n\ngrpc是让远程方法变得像本地方法一样简单，这里用了推荐侧的一个接口，确实只用引用一个jar包就行，背后就是一些复杂的网络请求。这个接口的可以一次传50个domain，状态码主要关注429，这里是触发了限流，需要按照上面的方法进行重新消费，消费多少发送多少相同的域名。\n\n> grpc方法的qps是1，触发了限流要降速，Thread.sleep(1000);\n\n```java\npublic void batchGetRecommendationDetail(Set<String> domains,Set<String> domainSet) throws InterruptedException {\n    Recommendation.GetDomainInfosRequest request = Recommendation.GetDomainInfosRequest.newBuilder()\n            .addAllDomains(domains)\n            .setDomainSourceValue(DOMAIN_SOURCE_GRPC_VALUE).build();\n    Recommendation.GetDomainInfosResponse response = recommendationClient.getDomainInfos(request);\n    if (response != null) {\n        if (!response.getSuccess() || response.getCode() != GRPC_SUCCESS_CODE){\n            //触发grpc限流，该方法降速，重新消费\n            Thread.sleep(1000);\n            throw new RuntimeException(String.valueOf(response.getCode()));\n        }\n        log.info(\"getRecommendationDetail. code:{}\",response.getCode());\n        if(!CollectionUtils.isEmpty(response.getDataList())){\n            List<Recommendation.DomainInfo> domainInfoList = response.getDataList();\n            for (Recommendation.DomainInfo domainInfo : domainInfoList){\n                domainSet.add(domainInfo.getDomain());\n            }\n            for (Recommendation.DomainInfo domainInfo : domainInfoList){\n                convert(domainInfo,domainSet);\n            }\n        }\n    }\n    else {\n        throw new RuntimeException(\"Recommendation.GetDomainInfosResponse returns null\");\n    }\n}\n\n```\n\n## 4.转换对象\n\n这个没什么好说的，就是拷贝属性到当前项目的bean\n\n## 总结\n\n主要熟悉了测试环境和线上环境，以及kafka的一些配置和机制，对于异常处理的理解加深了。\n\n\n# 需求2：定时发送新域名\n\n需要每天的00:00去es里面检查昨天新增的域名，通过kafka发送消息。\n\n1. 计算昨天00:00到23：59的时间戳：由于字段里面只有updateTime没有createTime，而且也是用时间戳来保存的，要先计算一下时间。\n2. 拼es的查询语句：range来查询时间，synFlag是防止重复发送的，有这个字段的可以认为已经发过了。\n3. scrollSearch循环查找，不了解，但是大家都是这样用的。\n\n> es的操作跟数据库很像，而且网易封装的很好，index就相当于数据库表名，DomainDataIndex可以看作mybatis的实现类，AbstractIndex就是BaseMapper那种用来基础的\n\n```java\n@XxlJob(\"newDomainSyncRecommend\")\npublic ReturnT<String> newDomainSyncRecommend (String param) throws ParseException {\n    //昨天的同一时刻\n    long yesterday = System.currentTimeMillis() - 1000 * 3600 * 24;\n    //昨天零点零分零秒的毫秒数\n    long zero = yesterday / (1000 * 3600 * 24) * (1000 * 3600 * 24) - TimeZone.getDefault().getRawOffset();//今天零点零分零秒的毫秒数\n    //昨天23点59分59秒的毫秒数\n    long twelve = zero + 24 * 60 * 60 * 1000 - 1;\n    BoolQueryBuilder boolQuery = QueryBuilders.boolQuery();\n    boolQuery.filter(QueryBuilders.rangeQuery(\"updateTime\")\n            .gte(zero)\n            .lte(twelve));\n    boolQuery.mustNot(QueryBuilders.existsQuery(\"synFlag\"));\n    SearchSourceBuilder sourceBuilder = new SearchSourceBuilder()\n            .query(boolQuery) // 查询参数\n            .fetchSource(new String[]{\"domain\"},\n                    null)\n            .size(100);\n    if (domainDataIndex == null) {\n        log.error(\"domainDataIndex is null\");\n    }\n    domainDataIndex.scrollSearch(sourceBuilder, searchHits -> {\n        for (SearchHit searchHit : searchHits) {\n            String domain = searchHit.getId();\n            try {\n                //更新同步标识\n                Map<String,Boolean> updateMap = new HashMap<>();\n                updateMap.put(\"synFlag\",true);\n                domainDataIndex.updateByProcessor(domain,updateMap);\n                log.info(\"domainDataIndex upsert success ,domain : {}\",domain);\n            }\n            catch (Throwable e){\n                log.error(\"domainDataIndex upsert failed ,domain : {}\",domain,e);\n                continue;\n            }\n            //组织kafka消息\n            JSONObject jsonObject = new JSONObject();\n            jsonObject.put(\"domain\",domain);\n            kafkaProducer.send(\"new_domain_sync_recommend_topic\",domain,JSON.toJSONString(jsonObject));\n            log.info(\"kafka send success ,domain : {}\",domain);\n        }\n        return false;\n    });\n    return ReturnT.SUCCESS;\n}\n\n```\n"},{"title":"项目面试","url":"/2024/10/19/项目面试/","content":"## 登录流程和登录状态保存\n- 乘客登录：首先根据前端微信小程序传来的code进行乘客openid的解析，根据这个openid进行落库。在我们的项目中不会对外暴露这个openid，使用的是自己系统的自增id。如果没有注册就insert一条。接着用redis存储登录状态，key是UUID，value是自己数据库的自增id。我们这里用的aop+threadlocal+注解类似网关的prehandle，处理前端每次都携带的token字段，然后在redis里根据这个key进行查询，如果是没有这个key就是没有登录过期了，如果是key为空就是没有登录。然后根据redis里的这个value存储到threadlocal中，后续就可以从threadlocal中get这个字段进行操作。\n- 司机登录：流程与上面的类似，只不过多了阿里云的身份校验。\n\n**threadLocal的原理**\n\nthreadlocal是根据线程独立的变量，也就是说每一个线程可以存储不同的threadlocal，彼此是隔离的。其原理是每一个Thread类都有一个threadLocalMap，这个map相当于一个hashmap，key是threadLocal对象，value是存储的值。threadLocal查询的时候就会先获取当前线程，根据这个线程找到threadLocalmap，再找到其中的threadLocal对应的值。\n\n**threadLocal的内存泄露问题**\n\nthreadLocalmap的key是弱引用，value是强引用，也就是说如果threadlocal没有外部强引用就会被回收，而value不会被回收。这个时候就会有内存泄露问题了。解决方法是不再使用threadLocal就手动remove\n\n## 搜寻周围司机和订单推送\n在司机开始接单之后会把自己的位置上传到redis，然后进行等待，在这个过程中司机会不断轮询自己redis的list是否有数据。同时乘客端下单之后会新增一个定时任务，使用redis的gredius命令查询方圆5km内的符合要求的司机。并且为这些司机的list都添加该订单号。\n\n**为什么要用定时任务**\n\n因为执行一次并不一定就能及时发现司机，轮询又太消耗cpu，所以采用了定时任务。使用xxl-job可以管理这些定时任务。\n\n**异常怎么处理的**\n\n有一个兜底策略就是超时15分钟自动取消，如果在运行过程中出现失败首先xxl-job会显示执行失败，同时记录日志。\n\n**如果周围只有一个司机，但是一直不接单，如何保证他的list中消息不重复**\n\n这里使用了redis的set集合，key是订单的id，每当任务调度搜索到最近的司机之后首先会校验司机的id是否在set中，如果不在才可以加入自己的list。如果在的话说明是重复提交了。\n\n**任务调度何时终止**\n\n在查询周围司机之前都会先检查当前订单的状态，通过另一个微服务的调用。如果这个订单的状态发生改变说明已经有司机接单了，这个任务就会提前终止。\n\n**为什么不用mq实现这一部分逻辑**\n\nredis的速度较快，毕竟用户的规模不会很大，list的规模也不会很大，暂时不会存在大key问题。而且也需要批量处理这些消息和频繁的删除，如果为每一个司机都建立一个queue有一点奢侈，综合下来还是redis中list能够简化操作。\n\n## 司机抢单\n每个司机在接单的过程中都会轮询自己在redis中的list是否有数据，如果有前端就会显示并且可以选择订单进行抢单。主要逻辑是首先会检查是否还有订单，这个是通过在redis的一个标识实现的，再下单的时候会给redis一个标识。目的是不用去数据库查找当前订单的状态节约数据库开销。为了防止超卖要用分布式锁，为当前订单id加锁。加锁之后再次校验是否有标识，进行接单操作后删除这个标识和释放锁，完成抢单逻辑。\n\n**分布式锁用的是什么**\n\n基于redision的分布式锁，锁住的是司机list中的订单id。本质还是基于setnx的，只不过redission有看门狗机制能够延长执行时间，保证任务能够进行完成而不会出现删除别人的锁的情况。这里可以优化的地方可以做成lua脚本，反正标识也是在redis里面的。逻辑是查询标识然后异步下单给mq。也会缩短司机抢单的响应时间，\n\n## 司乘同显\n在司机正在前往起始点和进行服务的时候，乘客也需要知道司机在哪里。司机会定时将自己的数据上传到mongodb中，乘客端会找到最近时间的司机点坐标，然后调用map微服务渲染出路径。\n\n**为什么使用mongodb，用redis或者mysql存储呢**\n\n- 路径数据是需要持久化保存的，用redis毕竟不是落库，而且存储的数据量大且只需要找出最近的一个点，大部分的数据没有用，会浪费珍贵的缓存空间，所以最好不用redis。\n- mysql也不太适合，存储的点数据量很大而mysql超过1kw查询效率就会很低，也会存在深度分页问题，如果多个订单同时进行并发也是mysql的问题。多一个少一个坐标点不会有很大的问题，所以使用另一个数据库进行存储，经过调研mongo能够满足。\n\n**基于rocketmq的~~延迟队列~~完成批量写是怎么实现的**\n> 这里真是犯了一个大错误。\n\n为了实现高效率写可以合并多次写请求，实现批量写。具体是设置rocketmq的批量处理功能。但是会增加代码的复杂度，更加轻量级的做法是线程池和阻塞队列。\n\n## 异步编排\n在完成订单后会有一系列的计算过程，例如防刷单校验，分账计算，推送账单等。\n```text\n[Start]\n   |\n   +--> [获取订单信息]  ----+\n   |                        |\n   +--> [获取司机位置] ------+--> [等待两个任务完成]\n                                 |\n                                 v\n                       [计算距离]\n                                 |\n               +-----------------+-----------------+\n               |                                   |\n        [距离 > 2公里]                       [距离 <= 2公里]\n               |                                   |\n         [抛出异常]                         +----+----+\n                                               |         |\n                                       [计算实际里程]   [获取订单数量]\n                                               |         |\n                                       [计算费用]    [计算奖励]\n                                               \\         /\n                                                \\       /\n                                        [计算分账信息]\n                                               |\n                                      [封装并更新账单]\n                                               |\n                                            [End]\n```\n\n考虑将获取订单信息和获取目的地距离进行并行处理，使用supplyAsync实现两个线程，最后合并一起进行计算得到结果；再校验距离是否再可以接收的范围内，如果是不是刷单就并行获取订单数量和计算实际里程，这个是根据mongodb中的数据计算的，得到这两个信息进行账单计算和推送账单。主要是将几个不相关的微服务通过多线程并行获取了。\n\n**如何测试的40%提升**\n\n测试的该方法的执行时间发现比原来快了40%。\n\n## 分布式事务\n\n**TCC的流程**\n\ntry-confirm-cancel，是一种业务入侵的分布式事务方法。try阶段会预留资源，confirm阶段确定分布式事务下所有的执行器都已经成功预留，如果全部成功就执行confirm提交事务，如果失败了就执行补偿措施cancel。这种方式比较灵活，可以自己控制事务，但是也入侵了业务，会造成一些别的问题，例如幂等性问题，空回滚和资源倒挂。\n\n**解释一下这三个问题**\n\n- 幂等性：由于网络抖动等问题导致在confirm阶段可能出现超时重试，但是最终两个请求都收到了。在seata的解决方式是用一个事务状态表，在confirm/cancel的时候提交给事务状态表，在开始的时候又去检查字段状态，如果是已经confirm了的，第二个请求就会被忽略，从而达到了幂等性\n- 空回滚：在try阶段没有成功的微服务仍然执行了回滚，需要对失败的微服务进行区别。在seata里仍然是用事务状态表，在try成功了才会有status，回滚阶段有status的才能回滚\n- 资源倒挂：由于网络原因try阻塞了，结果回滚了，但是后续网络恢复了仍然会预留资源，使得资源被倒挂。解决方案仍然是事务状态表，在回滚的时候插入一条suspend=4，在try的时候先校验是不是倒挂了，如果=4就不用预留资源。\n\n**具体在代码里是怎么做的**\n\n例如我们的支付模块，需要修改金额和商品数量，然后下单。这个时候就要用到分布式事务了。每一个微服务模块都要实现tcc三个接口，在try的接口上使用@TwoPhaseBusinessAction，指定在seata服务中的id以及回滚和提交的方法名，接着在代码实现响应逻辑。对于数据库而言增加一个frozen字段，try阶段首先减少数量，frozen增加，cancel了就做相反操作，如果confirm了就减少frozen实际提交。\n\n## 规则引擎\n用规则引擎做了一些计算\n- 例如根据距离和实际预估出价格，例如在7点之前起步价是多少多少钱，每公里的价格计算。使用规则引擎防止将规则和价格硬编码在代码里，方便实际部署动态变更。\n- 计算账单也使用到了规则引擎，例如按照什么比例分成，根据订单多少给予奖励\n\n**但是实际工作里面都不太会用到这个你知道是为什么吗**\n\n我觉得首先是学习成本的问题，这个东西语法确实也很冗杂，虽然在java代码里面不用硬编码了，但是几个ifelse却要用更长的代码表示。某些时候这个服务也会成为瓶颈。\n\n\n## 实习项目\n### 业务是什么\n实现人工智能检测平台和主平台的接口，具体是定时发送指令到视频流平台，截图入库后通过这个接口的一系列定时任务提交给人工智能检测平台返回结果进行统计。统计完成后用kafka异步落库并返回给前端。除此之外还有一些文档和增删改查的工作。\n\n\n### xxl-job是怎么用的，你的项目里面有哪些定时任务会需要用到这个\nxxl-job是一个开箱即用的分布式调度平台，与传统的quataz相比可以动态改变cron表达式，还具有可视化的界面。由于实习单位后端有很多的定时任务，写死cron表达式不利于管理\n\n1. 首先是截图指令，会传递几个参数通过http控制视频流平台，其中参数会包含间隔时间，视频流平台也会根据这个进行定时落库。\n2. 接着是定时给人工智能平台传输，因为平台有很多的检测算法，接口都不是很统一。这里有一些很长的if判定过程。例如未佩戴安全帽 未佩戴绝缘手套  靠近变压器告警、工作服，或者是画面无人。这里用了一个线程池来并发完成这些图片的校验和传输，还用了策略模式把这些校验的很长的ifelse给简化了。具体是用一个公用的策略接口，然后上下文类中替换这个接口的实现类来完成策略选择。最后在策略工厂里面用哈希表来保存类型和具体策略的映射关系。\n3. 还有一个定时任务是每天早上1点钟删除保存了3天以上的照片\n4. 人脸注册每分钟从数据库抽200张给人工智能平台，人工智能平台自己也有数据库，在这个系统里面也有，这个时候虽然是通过http传输的，但是也可以看作一个分布式的事务，由于这个系统业务范围不是我们的，所以用分布式事务也不太合理。所以就想到用kafka保证最终一致性。\n\n### kafka怎么保证最终一致性的\n主要依赖kafka的exactly once语义。要实现只提交一次且只能消费一次需要三方都协调。对于生产者也就是我们这个代码里面，要使用kafka的幂等和事务，配置kafka的幂等性可以使生产者的消息只会被kafka接收一次，配置事务使得消费者在消费消息的时候只能看到提交事务的，对于kafka而言，由于我们公司也没配置kafka集群，所以只设置了acks=1，也就是写入成功后返回回调函数，这样能保证kafka不丢消息。消费者为了不重复消费消息也需要做到幂等，这里的幂等校验是redis进行去重的，然后再手动提交ack。这样能保证如果前面的平台执行失败了，这里的接口也不会落库；如果执行成功，通过kafka的这个exactly once就可以只落库一次。\n\n## 自我介绍\n\n面试官您好，我叫杨逸帆，通过本科的不懈努力保研至本校计算机学硕，现就读于大连理工大学计算机科学与技术学院，目前是研一刚入学。我主要的技术栈是java和一系列相关中间件以及数据库，在今年的寒假进入了一家公司做南方电网的一个项目，对现有代码进行优化和重构，例如用kafka来保证了消息最终一致性以及用策略模式优化了大量的ifelse语句。在暑假根据网上的开源项目自学了一个嗡嗡代驾项目，是一个类似滴滴代驾的司乘平台，运用到技术主要有springcloud，redis，mongodb，rocketmq等。希望通过此次的面试机会，得到贵公司的认可。此外我可以立即到岗，可以实习到明年过年。\n\n"},{"title":"网络和操作系统面试篇","url":"/2024/10/08/网络和操作系统面试篇/","content":"## http和https\nhttps主要有非对称加密，对称加密和数字签名三部分。具体流程是服务器先向证书颁发机构注册和验证。客户端请求服务器的时候，服务器会把自己非对称加密的钥匙发给客户端，这个钥匙是需要数字签名进行盖章的，这样客户端就知道这个网站是合法的而不是钓鱼网站。接收到了服务器非对称加密的钥匙，再把对称加密的钥匙通过这个公钥发给服务器，服务器用私钥进行解密。后续就根据对称加密进行传输。\n\n## http的状态码\n- 1xx：正在处理的状态，一般见不到\n- 2xx：成功状态码\n- 3XX：301永久重定向，302临时重定向\n- 4XX：401资源未授权，403被屏蔽了，404找不到\n- 5XX：500服务器问题。502网关收到了，但是服务器转发有问题\n\n## http版本\n- http1.0：短链接，一个资源一次tcp。\n- http1.1：长连接，但是还是有hol问题，即大文件传输会挡住小文件传输从而影响体验\n- http2.0：长连接，通过二进制帧解决了hol的问题，将大文件分成小份传输。同时长连接也改进成了多路复用\n- http3.0：tcp+quic（udp）优化了握手，安全性也进行了更新\n\n## tcp三次握手\n- 客户端先发送同步信号SYN，再带上自己的seq，进入syn-send状态\n- 服务器接收到了发送ack和sys，进入syn-recv\n- 客户端收到了之后在发送一个ack，建立连接\n\n**为什么是三次握手**\n\n因为只有三次握手才能使双方都知道各方面正常。\n- 第一次握手，客户端什么都不能确认，服务器可以确认自己接受正常，客户端发送正常。\n- 第二次握手，客户端可以确认自己发送和接收正常，服务器发送和接收正常；服务器可以确认自己接收正常，客户端发送正常\n- 第三次握手，服务器可以确认自己发送和接收正常，客户端发送和接收正常\n\n## tcp四次握手\n- 客户端发送fin和seq，进入fin-wait-1\n- 服务器接收到发送ack和seq，进入close-wait，客户端接收到了变为fin-wait-2\n- 可能服务器还会有要发送的，发送完了发送fin，变为last-ack\n- 客户端接收到了变为time-wait，等到2ttl时间，如果没有等到服务器的消息就自己关闭。","tags":["Java"]},{"title":"分布式事务-seata","url":"/2024/10/05/分布式事务-seata/","content":"## TCC模式\nTCC模式是try，confirm，cancel。不依赖本地事务，通过业务代码解决。try先预留事务所需的资源，confirm确保任务的执行并且消耗第一阶段预留的资源，如果在这个阶段出现差错就进入cancel。\n\n优点：\n- 不依赖数据库的事务，可以用在非事务型数据库\n- 不用全局锁，无需生成快照，效率较高。\n\n缺点：\n- 代码侵入：需要程序员手动设计事务逻辑，增加了开发的复杂度\n- 中间有软状态，是最终一致性，confirm和cancel出现问题会有一致性问题。\n- 幂等问题：如果由于在confirm和cancel阶段出现网络波动，可能多次释放资源，出现严重后果。所以为了保证信息只被回滚一次需要保证幂等性。可以用状态表来实现（但是我觉得还是会有并发问题）\n- 空回滚：try的期间阻塞一条，回滚的时候也会回滚这个空的。可能造成资源无故释放。解决方法也是状态表，如果第一阶段有记录成功过了就执行回滚。如果没有就不用\n- 资源倒挂：也是网络环境变化，try阶段某个分支事务阻塞了直到最后回滚了才执行，是空回滚的另一个结果。解决方案是空回滚的时候加一条状态记录。在try阶段如果没有这个就执行，如果有分支事务的id。说明已经被空回滚发现，不再执行。\n\n## saga模式\n\nSaga模式是一种用于处理分布式事务的模式，它通过将长时间的、复杂的事务分解为多个小的、可逆的事务片段，以实现事务的一致性和可靠性。\n\n在Saga模式中，每个事务片段称为一个补偿操作。每个补偿操作都与一个正向操作相对应，正向操作是事务的一部分，而补偿操作是用于撤销或修复正向操作的。Saga模式通过按照事务执行的顺序，依次执行正向操作和补偿操作，来确保事务在发生失败或异常时能够进行回滚或恢复。\n\nSaga模式的执行过程如下：\n\n执行正向操作：按照事务的逻辑顺序，依次执行正向操作。每个正向操作都会记录事务的执行状态。\n如果所有的正向操作都成功执行，则事务提交完成。\n如果某个正向操作失败，将会触发相应的补偿操作。补偿操作会撤销或修复正向操作的影响。\n执行补偿操作：按照逆序依次执行已经触发的补偿操作。补偿操作应该具备幂等性，以便可以多次执行而不会造成副作用。\n如果所有的补偿操作都成功执行，则事务回滚完成。\n如果补偿操作也失败，需要人工介入或其他手段来解决事务的一致性问题。\n\nSeata的Saga模式：\n\nSeata的Saga模式通过Seata框架来管理和协调分布式事务，提供了对事务的编排和状态管理的支持。它与Seata的其他特性（如AT模式、TCC模式）结合在一起，构成了Seata全面的分布式事务解决方案。\n\nSeata的Saga模式相对于传统的Saga模式，具有以下特点：\n\n- 集成性：Seata的Saga模式与Seata框架紧密集成，可以与Seata的其他特性一起使用，如分布式事务日志和分布式锁等。\n- 强一致性：Seata的Saga模式提供了强一致性的事务支持，确保事务的执行顺序和一致性。\n- 可靠性：Seata的Saga模式在补偿操作的执行过程中，支持重试和恢复机制，提高了事务的可靠性和恢复能力。\n\n**适用场景：**\n\n- 业务流程长、业务流程多\n- 参与者包含其它公司或遗留系统服务，无法提供 TCC 模式要求的三个接口\n\n**优点：**\n\n- 一阶段提交本地事务，无锁，高性能\n- 事件驱动架构，参与者可异步执行，高吞吐\n- 补偿服务易于实现，不用编写TCC中的三个阶段，实现简单\n\n**缺点：**\n\n- 没有锁，不保证隔离性，会有脏写；\n- 软状态持续时间不确定，时效性差；\n\n## XA模式\n就是二阶段提交2pc模式。保证强一致性，只能一起提交或者失效。效率最低。","tags":["seata"]},{"title":"场景题","url":"/2024/10/04/场景题/","content":"## 如何设计一个秒杀系统\n- 高性能：高性能核心就是响应时间快，一般来说都会把热点数据放在redis中，但是秒杀高峰的时候有可能连redis都应付不了。有几种解决方案：首先是二级缓存，redis之外用jvm的缓存例如caffeine，一定要控制空间大小，所以必须上淘汰策略例如lru；或者在业务层面进行UV统计，但是需要改写redis的jar；或者直接使用开源解决方案比如京东的hotkey。此外为了提高响应速度还可以将静态资源放到cdn上，例如七牛云阿里云，图片不会再占用服务器的带宽，但是这种属于钞能力的解决方案。\n- 高可用：保证节点的故障恢复。需要用集群。包括但不限redis-sentinel，redis-cluster；此外还可以用分布式限流框架sentinel，实现服务限流和降级；优化异步请求可以用mq，mq对于高可用的贡献是流量削峰，将请求打入消息队列，对于高性能的贡献是将减库存缓存和减数据异步化，更快返回请求。\n- 一致性：如何防止超卖，可以用lua脚本查询redis中的余量，但是lua的原子性保证的是脚本内都是一条语句，而不是事务上的原子性，出错了是不会回滚的；redis减库存后需要同步到mysql中，可以用mq完成。扣减余额也要考虑并发问题，可以用mysql的排他锁。\n- 幂等性：也就是常说的一人一单，可以考虑用分布式锁来锁住当前用户。\n\n## 大文件断点续传问题\n- 前端文件分片，并且通过sha算法生成一个校验和。\n- 后端根据这个校验和先去查minio是否有当前分片，如果有就直接跳过\n- 如果没有先检验sha校验和，如果匹配说明有效。可以采用并行的方式加快传输\n- 所有分片发送完毕后，前端会进行校验，发送整个的sha，与后端合并后的文件进行校验\n\n## 40亿qq号，如何去重\n哈希表和哈希集肯定是不行的，可以考虑用位图。或者数据结构那种外部归并排序\n\n## 动态线程池\n要用到nacos动态配置核心参数","tags":["Java"]},{"title":"JVM面试篇","url":"/2024/09/30/JVM面试篇/","content":"## JVM如何解决跨代问题\n如果在年轻代的某个对象引用了老年代的对象，在触发Young GC的时候只会检查年轻代而不会检查老年代，此时如果老年代引用了一个年轻代的对象就不会被YGC发现而回收了此对象。但是这种情况出现的比较少，除非是老年代动态的引用了一个刚产生的对象。\n\n**记忆集和卡表**\n如果为了小概率事件每次YGC都去扫描一遍永久代，开销会很大。此时我们只需要保存从非扫描区域到扫描区域的指针就可以了。这个保存的扫描指针逻辑上叫做记忆集，放在新生代中。\n\n卡表是hotspot虚拟机对于记忆集的具体实现，类似位图，把非扫描区域分块，然后用一个卡表数组来记录对应块是否变脏，如果记录的为1说明变脏，在YGC的时候把对应块加入扫描。本质上是用空间换时间。\n\n**写屏障**\n这里的写屏障跟volatile的不一样，这里仅仅是对引用操作做了一层aop，在写后屏障加入添加卡表的操作，在写前屏障添加该页是否脏的判断，可以解决伪共享问题。\n\n## 三色标记算法\n三色标记算法是可达性分析的一个扫描过程，例如CMS，先做初始标记标记出GCroot，然后再并发标记，这个过程是通过三色标记算法实现的。最后扫描到的白色节点就被视为没有引用，在下一轮GC会被回收。\n\n- 黑色，直接或者间接连接到GCroot并且自身所有引用都被扫描过了\n- 灰色：从黑色节点扩散，但是自己的子节点还没有被完全扫描\n- 白色：没有被染指的节点，也就是不可达区域\n\n**三色标记的缺点**\n\n1. 多标问题：在一个节点被标记成灰色之后，上一个节点断开联系，最终这个节点还是会被染黑，但实际上是垃圾。这个现象称为浮动垃圾，在下一轮会被回收。\n2. 漏标问题：多标至少可以通过下一轮垃圾回收清除，漏标就严重多了。当一个灰色节点与一个白色节点断掉之后立刻有黑色节点相连。这个时候三色标记算法还是会认为这个节点是白色的，就会被垃圾回收，而此时确实有黑色节点的引用，会报空指针异常。\n\n\n<div style=\"text-align: center;\">\n  <img src=\"../img/jvm/img_5.png\" alt=\"\" />\n</div>\n\n**如何解决**\n两个条件同时发生才会有漏标现象：首先是有灰色节点与白色节点断开联系，又有黑色节点与白色节点产生联系。\n\n破坏一个就不会产生漏标：\n\n- 增量更新：增量更新破坏了第一个条件：「至少有一个黑色对象新增了对白色对象的引用」，在并发标记阶段，黑色对象D指向了白色对象G，这时会把黑色对象D记录下来，在重新标记阶段，会把黑色对象D标记为灰色对象D，然后以灰色对象D为根节点，扫描整个引用链，白色对象G就会被依次标记为灰色、黑色，白色对象G漏标的问题就解决了。 缺点是会重新扫描以黑色对象为根节点的子树，时间长（CMS解决方案）\n- 原始快照：原始快照破坏了第二个条件：「所有灰色对象指向该白色对象的引用都断开了」，在并发标记阶段，灰色对象E断开了对白色对象G的引用，这是会把白色对象G记录下来，在最终标记阶段，会把白色对象G标记为灰色，然后以灰色对象G为根节点，扫描整个引用链，如此以来原来的白色对象G就会被依次标记为灰色、黑色，白色对象G漏标的问题就解决了。缺点是如果没有黑色对象来引用就会变成浮动垃圾（G1解决方案）\n\n\n## 类的回收\n类的元数据在方法区也就是元空间中，在fullGC是有可能会被垃圾回收的。一个类能被垃圾回收需要满足三个要求：\n1. 没有实例对象\n2. 没有静态方法的引用\n3. 类加载器已经被GC\n\n**private static final int I = 0，这个i会回收吗**\n\n看情况，跟着类走，本身这就是一个GCroot，一共有四种GCroot（类的常量引用类似这种，静态引用，虚拟机栈的引用，本地方法栈的引用）。如果不是类被卸载的话是不能被GC的。\n","tags":["Java"]},{"title":"设计模式面试篇","url":"/2024/09/28/设计模式面试篇/","content":"\n> 设计模式一般可以分为三大类\n> - 创建型：工厂模式，单例模式\n> - 结构型：装饰器模式，适配器模式\n> - 行为型：策略模式，观察者模式\n> 设计原则：\n> - 单一职责原则\n> - 开放封闭原则\n> - 里式替换原则\n> - 接口隔离原则\n> - 依赖倒置原则\n> - 迪米特原则\n> - 合成复用原则\n\n\n## 代理模式\n\n代理模式是在不改变原先对象的前提下，通过生成一个代理对象，扩展原先对象的一些功能。比如aop，不入侵业务代码实现方法前后逻辑。\n\n### 静态代理\n静态代理是写死的，也就是说需要程序员手动写一个proxy代理类，与原先类共同继承一个接口（个人理解其实从实现层面是可以不共同实现一个接口的，但是这样就会导致管理混乱，代理对象如果能复用这个接口那就会逻辑上通顺很多）\n\n静态代理实现步骤:\n- 定义一个接口及其实现类；\n- 创建一个代理类同样实现这个接口\n- 将目标对象注入进代理类，然后在代理类的对应方法调用目标类中的对应方法。这样的话，我们就可以通过代理类屏蔽对目标对象的访问，并且可以在目标方法执行前后做一些自己想做的事情。\n\n一旦要新增方法，代理对象也会改变，不满足设计模式中开放封闭的原则。这些代理类实际都是在编译过程中产生的class文件\n\n### 动态代理\n分为jdk动态代理和gclib。前者必须要求代理类实现了某个接口，后者没有这点限制。但是相对于jkd而言，gclib效率较低速度较慢。gclib的原理是生成一个子类来代理父类，由于不能继承final类，所以gclib不能代理final类\n\n**jdk动态代理**\n\n1. Proxy.newProxyInstance方法，得到动态代理对象。传入参数是对象的类加载器，类的接口和一个InvocationHandler自定义的处理逻辑\n2. 新建一个类实现InvocationHandler，这里面只有一个方法invoke，包装了反射中的method.invoke，我们需要在这句调用前后加上自己的逻辑就可以简单实现aop。\n3. 新建一个工厂类的静态方法来获取这个动态代理对象\n4. 使用\n\n```java\nimport java.lang.reflect.InvocationHandler;\nimport java.lang.reflect.Method;\nimport java.lang.reflect.Proxy;\n\n// 定义一个接口\npublic interface HelloService {\n    void sayHello(String name);\n}\n\n// 接口的实现类\npublic class HelloServiceImpl implements HelloService {\n    @Override\n    public void sayHello(String name) {\n        System.out.println(\"你好，\" + name + \"！\");\n    }\n}\n\n// InvocationHandler实现类\npublic class HelloInvocationHandler implements InvocationHandler {\n    private Object target;\n\n    public HelloInvocationHandler(Object target) {\n        this.target = target;\n    }\n\n    @Override\n    public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {\n        System.out.println(\"方法调用前的处理逻辑\");\n        Object result = method.invoke(target, args);\n        System.out.println(\"方法调用后的处理逻辑\");\n        return result;\n    }\n}\n\n// 使用动态代理\npublic class ProxyDemo {\n    public static void main(String[] args) {\n        // 被代理的对象\n        HelloService helloService = new HelloServiceImpl();\n\n        // 创建InvocationHandler\n        InvocationHandler handler = new HelloInvocationHandler(helloService);\n\n        // 创建代理对象\n        HelloService proxyInstance = (HelloService) Proxy.newProxyInstance(\n                helloService.getClass().getClassLoader(),\n                helloService.getClass().getInterfaces(),\n                handler\n        );\n\n        // 调用代理对象的方法\n        proxyInstance.sayHello(\"张三\");\n    }\n}\n\n```\n**gclib动态代理**\n\n1. 首先自定义MethodInterceptor，这一点与上面的InvocationHandler是一致的。但是注意这里不是method.invoke来调用实际逻辑了。这里用methodProxy来调用原始方法。\n2. 使用的时候首先创建增强类Enhancer\n3. 设置类加载器和父类（也就是被代理对象）\n4. 设置拦截器，就是第一点创建的对象\n5. 最后获取这个代理类\n\n```java\nimport net.sf.cglib.proxy.Enhancer;\nimport net.sf.cglib.proxy.MethodInterceptor;\nimport net.sf.cglib.proxy.MethodProxy;\n\nimport java.lang.reflect.Method;\n\n// 目标类\npublic class Person {\n    public void sayHello(String name) {\n        System.out.println(\"你好，\" + name + \"！\");\n    }\n}\n\n// 自定义的MethodInterceptor\npublic class PersonMethodInterceptor implements MethodInterceptor {\n    @Override\n    public Object intercept(Object obj, Method method, Object[] args, MethodProxy proxy) throws Throwable {\n        // 在方法执行前添加逻辑\n        System.out.println(\"代理类：方法调用前的处理\");\n\n        // 调用目标方法\n        Object result = proxy.invokeSuper(obj, args);\n\n        // 在方法执行后添加逻辑\n        System.out.println(\"代理类：方法调用后的处理\");\n\n        return result;\n    }\n}\n\n// 客户端代码\npublic class CglibProxyDemo {\n    public static void main(String[] args) {\n        // 创建Enhancer对象\n        Enhancer enhancer = new Enhancer();\n        enhancer.setSuperclass(Person.class); // 设置被代理类的父类\n        enhancer.setCallback(new PersonMethodInterceptor()); // 设置回调\n\n        // 创建代理对象\n        Person proxyPerson = (Person) enhancer.create();\n\n        // 调用代理对象的方法\n        proxyPerson.sayHello(\"张三\");\n    }\n}\n\n```\n在spring aop中，如果某个类实现了接口，则默认使用jdk动态代理；否则使用cglib\n\n## 单例模式\n保证只有一个全局访问点，无论怎么访问都只有一个对象。\n\n**饿汉式单例**\n\n```java\npublic class singleton {\n    private static final singleton INSTANCE = new singleton();\n    private singleton(){}\n    public static singleton getInstance(){\n        return INSTANCE;\n    }\n}\n```\n\n在类加载初始化的时候就直接创建单例对象，适合于程序运行期间始终都需要这个对象的场景。\n\n**懒汉式单例**\n\n```java\npublic class singleton {\n    private static singleton INSTANCE;\n    private singleton(){}\n    public static singleton getINSTANCE(){\n        if (INSTANCE == null){\n            INSTANCE = new singleton();\n        }\n        return INSTANCE;\n    }\n}\n```\n但是可能会有并发安全问题，如果两个都没有获取到当前对象就会产生覆盖或者数据不一致，可以使用synchronized来锁住这个方法，但是会影响性能。\n\n**双重校验锁**\n\n上面那种情况下无论是否已经实例化了都需要获取锁，但其实如果其实已经实例化了就不需要上锁。还有一个关键问题是会指令重排。一个对象在创建1过程中会经历三步：\n1. 分配内存空间\n2. 初始化对象\n3. 指针指向该对象\n如果3重排就会变成132，此时还没有进行初始化。两个线程并发就会导致报空指针异常。为了解决这个问题需要volatile禁止指令重排。\n\n```java\npublic class Singleton {\n    // 私有构造函数\n    private Singleton() {}\n\n    // volatile 关键字确保实例的可见性\n    private static volatile Singleton instance;\n\n    // 提供全局访问点\n    public static Singleton getInstance() {\n        if (instance == null) {\n            synchronized (Singleton.class) {\n                if (instance == null) {\n                    instance = new Singleton();\n                }\n            }\n        }\n        return instance;\n    }\n}\n```\n\n**内部静态类**\n\n```java\npublic class Singleton {\n    // 私有构造函数\n    private Singleton() {}\n\n    // 静态内部类\n    private static class Holder {\n        private static final Singleton INSTANCE = new Singleton();\n    }\n\n    // 提供全局访问点\n    public static Singleton getInstance() {\n        return Holder.INSTANCE;\n    }\n}\n\n```\n\n内部静态类在被调用的时候才会被加载，而且类加载不会有线程安全问题，天然解决并发问题。\n\n> 单例模式是可以被破坏的，只要通过反射拿到private的构造器就可以了\n\n## 策略模式\n一种方法可以用多种算法实现就可以使用策略模式，根据不同情况下用不同的算法。也可以消除\"if-else\"这样的控制语句。\n\n- 上下文：持有一个策略的引用，并提供一个接口来调用策略\n- 策略接口：定义了一系列算法的公共接口\n- 具体实现：策略的具体实现\n\n```java\n/**\n * 回执处理策略接口\n **/\npublic interface ReceiptHandleStrategy {\n\n    void handleReceipt(Receipt receipt);\n}\n/**\n * 策略实现类\n **/\npublic class Mt1011ReceiptHandleStrategy implements ReceiptHandleStrategy {\n\n    @Override\n    public void handleReceipt(Receipt receipt) {\n        System.out.println(\"解析报文MT1011: \" + receipt.getMessage());\n    }\n}\n\npublic class Mt2101ReceiptHandleStrategy implements ReceiptHandleStrategy {\n\n    @Override\n    public void handleReceipt(Receipt receipt) {\n        System.out.println(\"解析报文MT2101: \" + receipt.getMessage());\n    }\n}\n\n/**\n * 上下文类,持有策略接口\n **/\npublic class ReceiptStrategyContext {\n\n    private ReceiptHandleStrategy receiptHandleStrategy;\n\n    public void setReceiptHandleStrategy(ReceiptHandleStrategy receiptHandleStrategy) {\n        this.receiptHandleStrategy = receiptHandleStrategy;\n    }\n\n    //调用策略类中的方法\n    public void handleReceipt(Receipt receipt){\n        if(receipt != null){\n            receiptHandleStrategy.handleReceipt(receipt);\n        }\n    }\n}\n/**\n * 策略工厂，用哈希表保存策略，根据参数选择策略\n **/\npublic class ReceiptHandleStrategyFactory {\n\n    public ReceiptHandleStrategyFactory() {\n    }\n\n    //使用Map集合存储策略信息,彻底消除if...else\n    private static Map<String,ReceiptHandleStrategy> strategyMap;\n\n    //初始化具体策略,保存到map集合\n    public static void init(){\n        strategyMap = new HashMap<>();\n        strategyMap.put(\"MT1011\",new Mt1011ReceiptHandleStrategy());\n        strategyMap.put(\"MT2101\",new Mt2101ReceiptHandleStrategy());\n    }\n\n    //根据回执类型获取对应策略类对象\n    public static ReceiptHandleStrategy getReceiptHandleStrategy(String receiptType){\n        return strategyMap.get(receiptType);\n    }\n}\n\npublic class Client {\n\n    public static void main(String[] args) {\n\n        //模拟回执\n        List<Receipt> receiptList = ReceiptBuilder.genReceiptList();\n\n\n        //策略上下文\n        ReceiptStrategyContext context = new ReceiptStrategyContext();\n\n        //策略模式将策略的 定义、创建、使用这三部分进行了解耦\n        for (Receipt receipt : receiptList) {\n            //获取置策略\n            ReceiptHandleStrategyFactory.init();\n            ReceiptHandleStrategy strategy = ReceiptHandleStrategyFactory.getReceiptHandleStrategy(receipt.getType());\n            //设置策略\n            context.setReceiptHandleStrategy(strategy);\n            //执行策略\n            context.handleReceipt(receipt);\n        }\n    }\n}\n\n```\n\n## 适配器模式\n\n让一个实现类满足统一的接口，使不同的实现类可以一起工作。满足开放封闭原则。\n1. 例如在Spring AOP中不同的Advice实现的方式不一样类也不一样，可能实现的时候这个类的某个方法叫run，另一个叫start，为了是这些实现类完成统一，就需要加一层适配器，把名字都叫做advicerun，给客户端呈现的就是这个名字。\n2. SpringMVC也是，不同的控制器种类有很多，例如有注解的有自己写的，需要用适配器统一起来\n\n## 装饰器模式\n\n扩展类的功能，比起直接继承，装饰器模式可以实现单一责任原则，也便于功能的组合，避免类的层次结构复杂。\n\n```java\n// 组件接口\npublic interface Notification {\n    void send(String message);\n}\n\n// 具体组件\npublic class SimpleNotification implements Notification {\n    @Override\n    public void send(String message) {\n        System.out.println(\"Sending notification: \" + message);\n    }\n}\n\n// 装饰器抽象类\npublic abstract class NotificationDecorator implements Notification {\n    protected Notification decoratedNotification;\n\n    public NotificationDecorator(Notification notification) {\n        this.decoratedNotification = notification;\n    }\n\n    @Override\n    public void send(String message) {\n        decoratedNotification.send(message);\n    }\n}\n\n// 具体装饰器 - 邮件\npublic class EmailNotificationDecorator extends NotificationDecorator {\n    public EmailNotificationDecorator(Notification notification) {\n        super(notification);\n    }\n\n    @Override\n    public void send(String message) {\n        super.send(message);\n        sendEmail(message);\n    }\n\n    private void sendEmail(String message) {\n        System.out.println(\"Sending email with message: \" + message);\n    }\n}\n\n// 具体装饰器 - 短信\npublic class SMSNotificationDecorator extends NotificationDecorator {\n    public SMSNotificationDecorator(Notification notification) {\n        super(notification);\n    }\n\n    @Override\n    public void send(String message) {\n        super.send(message);\n        sendSMS(message);\n    }\n\n    private void sendSMS(String message) {\n        System.out.println(\"Sending SMS with message: \" + message);\n    }\n}\n\n// 使用示例\npublic class Main {\n    public static void main(String[] args) {\n        Notification notification = new SimpleNotification();\n\n        // 添加邮件功能\n        notification = new EmailNotificationDecorator(notification);\n        notification.send(\"Hello!\");\n\n        System.out.println(\"----\");\n\n        // 添加短信功能\n        notification = new SMSNotificationDecorator(notification);\n        notification.send(\"Hello!\");\n    }\n}\n\n```\n\n## 观察者模式\n观察者模式通过定义一对多的依赖关系，使得一个对象状态的改变能够自动通知并更新所有相关对象。它在软件开发中具有广泛的应用，尤其适用于事件驱动系统和需要解耦组件的场景。在Spring框架中，观察者模式通过事件发布和监听机制得到了高效的实现，增强了应用程序的灵活性和可维护性。\n\n尽管观察者模式带来了许多优点，但在使用时也需注意避免过多的观察者导致系统复杂性增加，以及潜在的性能问题。合理地设计和管理观察者列表，确保事件通知的有效性和高效性，是成功应用观察者模式的关键。","tags":["Java"]},{"title":"消息队列面试篇","url":"/2024/09/25/kafka面试篇/","content":"## 零拷贝\nrocketMq的架构是类似于kafka的，实现了许多kafka没有的功能，但是依旧没有在市场上打败kafka，kafka的性能这么高的原因得益于零拷贝技术。\n\n在传统io方式中是需要经过cpu的，用户调用read()方法，数据通过dma传输到内存缓冲区，然后通过cpu拷贝到用户态的用户缓冲区，得到了用户能见的数据。再通过write()将数据拷贝到操作系统内核态的socket写缓冲区，最后通过网卡发送数据。这个过程一共经历了四次用户态和内核态的切换：\n1. 系统调用read，用户态->内核态\n2. dma后，cpu将数据从内核态拷贝回用户态\n3. 系统调用write，用户态->内核态\n4. write方法返回，重新回到用户态\n\n以及四次数据的拷贝。\n\n**mmap**\n\nmmap是一种将内存映射到用户态空间的系统调用。简单来说就是用户态和内核态使用同一片空间，不用进行上下文切换。因此整个流程就会变为：\n1. mmap系统调用，用户态->内核态\n2. dma将磁盘数据写到内核态的缓冲区，返回给用户态，同时用户态也能看到这片缓冲区，可以进行操作。需要进行上下文切换但不用拷贝\n3. 操作完了之后系统调用write写到网卡发送，用户态->内核态\n4. 拷贝两次，从内核态缓冲区拷贝到socket缓冲区，再用dma方式写回磁盘或者网卡，最后回到用户态\n\n综上所述，总共经历了3次io和四次上下文切换。减少的这一次叫做零拷贝。\n\n**sendfile**\n\nsendfile不会给用户态返回具体的数据，直接从内核态走socket传输走了。\n1. sendfile系统调用，用户态->内核态\n2. dma拷贝到内核态的缓冲区，这里不会返回给用户态具体的信息，也就是说用户态操作不了，直接拷贝给socket\n3. 再从socket拷贝到网卡，返回用户态\n\n综上所述，一共两次上下文切换和三次io，对比 mmap+write，主要是没有再用户态进行拷贝。\n\n说了这么多，到底跟kafka和rocketmq有什么关系呢？kafka是基于sendfile的，而rocketmq是基于mmap的，都使用了零拷贝技术，但是由于rocketmq新增了一些高级功能，需要直到具体的数据信息而不是直接发送，就用了能看见数据的mmap方式。kafka为了追求极致性能就没有这些花里胡哨的功能，使用了效率更高的sendfile\n\n## 消息队列的作用\n- 异步：不用等待同步操作，把消息扔给mq然后就直接返回，mq连接的另一端会异步进行操作\n- 解耦：可以将业务进行拆分，也是上面异步的思想\n- 削峰：到来的高并发请求先进入mq，类似漏桶限流算法，避免打倒服务器和数据库\n\n## Kafka的架构\nkafka是基于发布者-订阅者模型的，在这个模型中有发布者，订阅者，broker和服务发现中心。具体到kafka就是producer，consumer，broker，topic，partition，配置中心是zookeeper：\n- broker：就是一个物理上的kafka服务实体\n- topic：消息是按照主题划分的，例如业务消息和日志消息就可以用两个topic划分\n- partition：一个topic内也不只有一个队列，这个分区就是队列的意思。需要注意的是kafka在逻辑上是一个整体，但是物理上一个topic内所有分区不一定在同一个broker上。\n\n<div style=\"text-align: center;\">\n  <img src=\"../img/kafka/img.png\" alt=\"\" />\n</div>\n\n- 生产者组：生成消息放入消息队列，是多对多的关系，一个生产者可以生成很多个topic的消息，也可以生成一个topic内多个partition的消息\n- 消费者组：消费的进度用offset表示，消费者组并不会真的把消息队列的信息删除，而是用一个偏移量来表示消费到了哪里。每一个消费者组的偏移量可以是不同的，一般来说一个partition最好只有一个消费者，如果多了的话并发控制也会成为性能影响因素\n- zookeeper：对于如此庞大规模的kafka集群，实体之间是怎么互相发现对方的？这里使用了zookeeper作为服务发现框架。kafka高度依赖zookeeper\n\nkafka为了保证高可用，还有类似主从架构的副本机制，使得某个broker挂了之后，部署在其他地方的partition能够继续工作。总而言之，一个topic多个partition的机制能够使消息分散在多个broker上，实现负载均衡。partition的副本也会保存在不同的机器上。从逻辑上看是整体的，物理上看都不会集中在一块区域。\n\n## Kafka的存储机制\n前情提要：一个topic可能横跨多个broker，每一个broker的文件系统只会保存自己实际物理存储的日志。只有通过zookeeper才能看到一个topic的全部分区信息。\n\n在一个broker下，文件信息是这么存储的：\n```text\n<kafka-log-dir>/\n    └── <topic-name>/\n        └── <partition-number>/\n            ├── 00000000000000000000.log        # 日志文件\n            ├── 00000000000000000000.index      # 偏移量索引文件\n            ├── 00000000000000000000.timeindex  # 时间索引文件\n            ├── 00000000000000000001.log        # 下一日志段\n            ├── 00000000000000000001.index      # 下一日志段的偏移量索引文件\n            ├── 00000000000000000001.timeindex  # 下一日志段的时间索引文件\n```\n- 日志段（segment）：一个分区下有多个segment，segment中保存了实际的数据，这些数据的key是offset，每一个segment以当前最小的offset命名。查询的时候通过二分查找找到对应的offset\n- 索引（index）：同时会生成一个与segment同名的索引。保存当前segment下offset的索引信息（类似二级索引），加快日志内的查找速度\n- 时间索引文件：存储offset对应的时间戳，便于根据时间查找信息。\n\n**生产者**\n\n当收到生产者数据的时候，用户态内存接收到之后，会给操作系统内核态的page cache，os再择机刷盘。这个写过程是顺序写，也就是说在对应的分区的最后一个log后面写刚刚的消息。\n\n**消费者**\n\n消费者消费数据的时候，根据offset先在文件名中二分查找到segment，再用对应的索引来加速搜索。\n\n**日志滚动**\n\n如果一个segment达到了默认的1GB或者超过了7天没有新增，那就会有一个新的segment\n\n**日志删除**\n\n可以配置kafka的消息删除时间，这个时候上面的日志索引文件就有用了，通过判断时间戳来删除。在删除过程中会使用copyonwrite以避免有消费者这个时候消费消息。\n\n## Kafka如何保证消息不丢失\n消息丢失不是Kafka一个管道能说了算的，可能在生产者传输的过程中丢失，也有可能消费者offset更新完了之后就掉了导致没有真正消费信息。\n- 生产者的信息丢失：类似tcp的各种握手，可以使用一个回调函数返回kafka是否接收到了信息，这里最好不要用同步操作get，用异步ListenableFuture<SendResult<String, Object>>开一个线程监听；如果失败了就重传，可以设置重传次数。\n- 消费者的信息丢失：offset默认是消费者接受了这个消息就更新，但是可能会发生拿到消息就掉电导致消息没有实际被消耗。可以采用offset手动更新，在消费者逻辑的最后手动提交offset，这样就能保证消费完了才更新offset\n- kafka的信息丢失：架构中都有这么多副本在兜底了，但是副本之间不一定时时刻刻都保持同步。设置几个参数就能保证kafka的高可用\n\n> - acks=all，表示所有副本都更新完写操作才返回，默认值是1，也就是主节点更新了就返回，但是这样依旧可能造成丢数据问题。因此为了绝对安全可以设置acks=all，但是会极大的影响性能。\n> - 设置副本数量>=3，虽然造成了数据冗余，但是可以保证高可用\n> - 设置更新的副本数量>1，这样配置代表消息至少要被写入到 2 个副本才算是被成功发送。min.insync.replicas 的默认值为 1 ，在实际生产中应尽量避免默认值 1，是acks=all的一种折中\n> - 设置 unclean.leader.election.enable = false，当发生主从数据不一致的时候，只有完全同步的数据副本才能被选举为leader\n\nkafka是ap架构还cp架构？从上面的各种默认参数来看，比如acks=1，默认的时候还是倾向于可用性的，也就是会牺牲一部分一致性。但是经过上面的种种配置之后，就可以使kafka集群是强一致性的，在某些场合下可以灵活改变配置。\n\n## 生产者向Kafka发送消息的执行流程\n1. 配置生产者，一般消息有三个值，topic，key和value（保证顺序的话就要指定partition）。还可以配置acks的参数，reties的次数，配置完这些之后产生一个生产者对象。\n2. 序列化消息，将key和value序列化，例如StringSerializer\n3. 消息需要选择进入哪个分区，如果key没有配置的话就轮询（RR），如果有的话就hash然后取模\n4. 如果配置了批处理，先把消息放在缓冲区，等达到了batchsize再传送\n5. 请求压缩：kafka集群主要的瓶颈在网络io而不是cpu，在进入的时候压缩，在出去的时候解压，能够减少网络延迟\n6. 当达到了batchsize，根据选择的分区发往broker。根据acks配置的参数返回消息\n7. 超时重试\n8. 如果有回调函数就会执行\n\n> kafka可以在分区内保证顺序，但是消费的整体顺序是无法保证的，如果需要严格的顺序消费，以下有两种方法解决：\n> - 指定key是相同的，在序列化的时候无法保证不同的key散列取余以后是在同一个分区，如果key是相同的就可以保证在一个分区\n> - key可以不同，指定partition相同就可以了，从根源上解决问题\n\n## 消费者消费失败会怎样\n消费者会有一个重试次数，如果由于网络导致消费失败，信息不会阻塞，会进入一个死信队列。可以在这个队列里面进一步分析原因，也可以继续消费这个队列的信息。\n\n重试次数默认是10次，并且间隔是0s，也就是立即重试10次，没有消费成功就进入死信队列，不会阻塞下一条信息。\n\n\n\n\n## Kafka如何保证不重复消费信息\n如上面所述，即便是手动提交offset还是可能出现重复消费，这个时候只能通过消费者的幂等进行校验了\n\n**三种消息投递保证**\n- At most once：至多一次，对应上面的消费者自动提交offset，这个时候掉电了这条消息就没有了\n- At least once：对应手动提交offset，可能会重复多次。需要消费者自己做幂等\n- Exactly once：消息精确传输一次。这不仅仅依赖于kafka，同时也要消费者的配合。\n\nkafka的生产者在发送的时候会用全局唯一id+offset来进行幂等发送，避免因为发送端超时重试而导致重复消费。同时开启事务，确保消息是原子性的。\n\n消费者只会消费已经提交过事务的消息（类似MVCC，未提交事务的信息是有可能回滚的，消费他们有风险）。同时类似At Least once，消费者也需要配合做幂等。\n\n\n### Exactly Once 的实现机制\n\n1. **生产者的角色**：\n    - **幂等性**：生产者通过启用幂等性功能（`enable.idempotence=true`）来确保在发送过程中，每条消息只有一个唯一的 ID。即使发生网络错误或重试，生产者也不会重复发送相同的消息。\n    - **事务支持**：生产者使用事务来发送消息。在一个事务中，生产者可以发送多条消息，这些消息在调用 `commitTransaction()` 之前对消费者是不可见的。如果在发送过程中出现错误，生产者可以调用 `abortTransaction()`，此时所有消息都将被丢弃，确保没有未提交的消息被消费。\n\n2. **消费者的角色**：\n    - **配置为 `read_committed`**：消费者配置为 `isolation.level=read_committed`，确保只读取那些已成功提交的事务消息。这意味着消费者只会处理生产者通过 `commitTransaction()` 提交的消息。\n    - **手动提交偏移量**：消费者在处理每条消息后手动提交偏移量（`commitSync()`），以标记这些消息已被成功处理。只有在确认消息处理成功后，才提交偏移量，以避免处理失败导致的重复消费。\n    - **实现幂等性处理**：消费者在业务逻辑中需要实现幂等性，以确保即使同一条消息被多次消费，最终的业务效果也应该一致。例如，使用数据库的唯一约束来避免重复插入。\n\n### 总结\n\n- **唯一性保证**：通过生产者的唯一 ID 和事务管理，确保在 Kafka 中发送的消息是唯一的，并且不会因重试而重复。\n- **消费的一致性**：消费者通过手动提交偏移量和实现幂等性来确保消息的消费是唯一的，并且只处理那些已经提交的消息，避免了未提交消息可能引起的回滚和不一致。\n- **消息可见性**：未提交的事务消息对消费者是不可见的，确保了只有经过确认的消息才能被消费。\n\n这种机制结合起来，使得在 Kafka 中能够实现 **Exactly Once** 的消息处理语义，从而在关键任务和需要高一致性的数据流动场景中得以应用。\n\n\n> 乱入一下rocketmq\n\n## RocketMQ\n阿里的rocketmq参考了kafka的架构，在架构上做减法，在功能上做加法。主要区别有：\n- 存储模式：kafka的数据存储是按照segment，一个partition有很多segment，做的是追加写，但是一个broker下面有很多的topic，访问量大起来就不是追加写会退化为随机写。rocketmq发现了这一点，他将所有的queue都放在一个文件上commitlog，而queue只放offset和对应索引。这样就能一直是追加写了\n- 同步模式：按照上面的逻辑，kafka那种replica就不行了，因为kafka是按照partition进行同步和冗余的，partition可以分布在不同的broker上的一个原因就是segment是分开来的。rocketmq把这一点优化了，所以rocketmq是类似redis和mysql那样，以broker为单位进行主从同步\n- 简化协调节点：kafka高度依靠zookeeper，太重了。rocketmq直接用nacos就可以了，作为微服务的一个模块\n- 零拷贝：kafka是sendfile而rocketmq是mmap+write\n- 功能增强：延迟队列，死信队列，按照tag过滤消息，事务\n\n> 乱入一下elasticsearch，估计问的也不多，简单了解一下\n\n## elasticsearch\n微观到宏观，最小单元segment->shard->node->集群。\n```text\n服务器实体 (Node 1)                服务器实体 (Node 2)\n+---------------------+            +---------------------+\n|  Primary Shard 1    |            |  Primary Shard 2    |\n|  (Lucene Instance 1)|            |  (Lucene Instance 2)|\n|     +-------------+ |            |     +-------------+ |\n|     | Segment 1   | |            |     | Segment 3   | |\n|     | Segment 2   | |            |     | Segment 4   | |\n|     +-------------+ |            |     +-------------+ |\n|                     |            |                     |\n|  Replica Shard 2    |            |  Replica Shard 1    |\n|  (Lucene Instance 3)|            |  (Lucene Instance 4)|\n|     +-------------+ |            |     +-------------+ |\n|     | Segment 5   | |            |     | Segment 6   | |\n|     | Segment 6   | |            |     | Segment 7   | |\n|     +-------------+ |            |     +-------------+ |\n+---------------------+            +---------------------+\n```\n### segment\n1. 倒排索引：用分词器将每个单词作为键，值为对应的句子id就可以方便关键词查找了\n2. 前缀索引：单靠上面的倒排索引查询起来还是很费劲。可以使用前缀树trie完成。放在内存里方便查询\n3. 句子实际的存储位置：倒排索引只是放了句子的id，真正的句子存储在这里\n4. 一种方便排序和聚合的数据结构：某些时候用户可能想要按照时间排序，但是这些字段放在3中是很不好的，需要回表再去排序。不如直接用空间换时间，将时间信息提取出来单独做一个表\n\n以上的四个部分共同构成了一个segment，这是搜索引擎的最小单元。但是由于前缀索引不好进行扩容，可以约定新增几条句子就开辟一个新的segment，同时也解决了读写问题。问题是文件会变多，可以采用定时合并的方法。这样多个segment就构成了lucene（shard）。如果有搜索请求就会并发在所有segment里面进行搜索。\n\n### shard\n其实上面的多个segment加起来就是一个lucene，在es中如果只有一个lucene那么读写效率会变得很慢：\n- 在垂直上可以参考kafka的topic设置多个index_name，例如分体育新闻和娱乐新闻。一个index对应一个lucene。\n- 在水平上可以将lucene再分小一点，每个段少分一点segment，这就变成了标题所说的shard，本质上shard就是lucene的一个实例。这样读写操作有可能会被分开，提高效率\n\n同时为了高可用还未每个shard提供了副本机制，分散在不同的node中\n### node\n一个node就是一个服务器节点，类似kafka的broker，可以放很多不同topic的不同partition，包括副本啥的。这里就会要一个类似zookeeper的服务发现中心，但在es里是去中心化的，通过raft协议选举。\n\nnode有很多职责，有负责管理集群的（类似哨兵负责故障转移和选主），有存储数据的。有实现restful接口的，在集群环境下可以分开。每一个node负责不同的职责。\n\n**数据如何查找**\n\n1. 客户端发送rest的http请求给集群中负责接收的node\n2. 在指定index_name下hash分片找到对应的shard\n3. 在shard中并行查找segment\n4. segment根据倒排索引找到对应句子的id返回\n5. 再去根据id进行回表，找到句子返回\n\n\n\n\n\n\n\n\n\n\n\n","tags":["Kafka"]},{"title":"Mysql面试篇","url":"/2024/09/22/Mysql面试篇/","content":"## Mysql的数据结构\n- 数字型：整型（tinyint,smallint,int,bigint），浮点型（float,double），定点型（decimal）\n- 字符串：char，varchar，tinytext，text，mediumtext，longtext，tinyblob，blob，mediumblob，longblob\n- 日期：date，timestamp\n\n> 定点型是指定小数点后几位的。char是定长的，varchar是可变的，一般设计的是最大长度。如果超过长度char是需要修改表结构的。而且相同字符串在存储层面两者是相同的。但是varchar内存占用稍高，某些高级操作是需要遍历。\n\n## Mysql查询流程\n1. 连接器：连接客户端的http请求，校验用户名密码\n2. 查询缓存，如果缓存里面有就直接返回（这里在8版本就废弃了，因为命中率实在太低了）\n3. 词法分析语法分析\n4. 执行器：包括预处理，例如将*换成表的字段；优化器，选择效率较高的执行计划执行，能用索引就不用全表扫描；执行器，执行sql然后返回结果给客户端\n\n## InnoDB和MyISAM\n1. 事务\n2. 行级锁\n3. 索引\n4. 支持外键\n5. 崩溃恢复redolog\n\n\n\n## InnoDB的存储形式\n众所周知InnoDB存储数据的数据结构是B+树，其中树的节点就是数据页。\n\n数据页内部：\n\n一个数据页上会有多条记录，对应着实际的记录。在数据页内部是按照主键id顺序排列的。除此之外还有一个页目录，将顺序排列的某些记录归为一个槽。例如一个数据页有12345，可能12是一个槽，345是另一个，槽只保存最大的那个节点。当搜索的时候，比如我要搜索4，由于4大于12这个页的最大值2，所以会在345里面搜索。这个时候需要遍历的成本就低很多了。一般来说槽只会包含不超过8条记录。使用二分法找到这个槽，然后在在这个槽进行遍历。页目录只是为了加快在页内的查找速度。实际上每一个页里面记录也是通过链表串起来的。对外只会暴露当前页主键最大值和最小值。方便b+树的查找。\n\nB+树：\n\nb+树与b树的不同点在于，b+只有底层真正存储数据，b树在非叶子节点也会存储数据。而且b+底层是一个双向链表，可以方便进行范围查询。对于innoDB而言，为索引节点还是数据节点根据页头的某个字段决定。索引节点中存储着下一个表对应的最小主键。我们可以把查找一次当作一次io，由于索引节点能存储的数据是很多的，千万级别的数据最多也不过三四层，也就是说io次数会很小。b+树的缺点是会由于添加数据导致分裂，影响性能。\n\n> InnoDB默认得要一个索引，可以不显式的给出，但是背后隐藏列会默认出一个索引作为数据页的排序参照\n\n## 为什么用B+树而不是B树，红黑树或者跳表\n- b树：由于非叶子节点也会存储数据，那么就没有b+树存储数据多。查询的效率也不稳定，可能在非底层就查询到数据了。而且不是双向链表，范围查询做不到\n- 红黑树：也是二叉树，io次数肯定比b+树多，跳表同理\n- 哈希表：不支持范围查询，这同样是redis为什么用跳表而不是哈希表的理由\n\n## 索引\n索引是为了加快搜索而建立的数据结构\n\n- 按照数据结构划分：B+树索引，Hash索引\n- 物理存储划分：聚簇索引（叶子节点是数据），非聚簇索引（叶子节点存的是主键id，后续还要回表去聚簇索引找）\n- 字段特性：主键索引，唯一索引（建立在唯一字段上的索引），普通索引，前缀索引（查电话号码）\n\n\n> 唯一索引：create unique index index_name on table(index_column_1)\n> \n> 普通索引：create index index_name on table(index_column_1)\n> \n> 前缀索引：create index index_name on table(length(index_column_1))\n\n- 字段个数：单列索引，联合索引\n\n> 联合索引：create index index_name on table(index_column_1,index_column_2)\n\n## 联合索引范围查询\n**select * from t_table where a > 1 and b = 1**\n\n这里只会用到a一个索引，因为只有a能缩小范围，b是无序的，无法缩小范围。如果没有索引下推只能一个个回表通过b进行筛选\n\n**select * from t_table where a >= 1 and b = 1**\n\n这里会用到两个索引，因为a=1的时候b是有序的，b能够缩小范围\n\n**select * from t_table where a between 1 and 2 and b = 1**\n\nmysql中between是左闭右闭区间，所以两个都能用\n\n**select * from t_table where a like 'm%' and b = 1**\n\nmysql中between是左闭区间，所以b也能用\n\n\n## 索引失效场景\n> 什么时候无法使用索引？就是通过某些手段找不到原先的顺序了\n- 左或左右模糊匹配：例如%like和%like%，索引在这个字段上是根据最左匹配的，如果第一个字符都不知道，那后续的就是无序的了\n- 对索引使用函数：例如count()某个索引，这个时候也是走不了的，因为count不是一个索引\n- 对索引进行表达式计算：只要是在等号左边的也用不了，mysql没做这部分优化\n- 对索引隐式类型转换：在mysql中字符串会自动变成数字进行比较，索引当索引为字符串，而查询的时候用id=1来比较，会自动类型转换，相当于索引使用了隐式函数变成数字于查询条件进行比较\n- 联合索引非最左匹配：联合索引（a,b,c）如果查询条件只有b，c，就不会走索引。但是如果是ac还是可以走a的索引，如果有索引下推的话在二级索引可以通过判断c减少回表次数；如果没有就只能用a减少判断，最后回到server也就是索引是id的那个表中一个个判断c，这样会有很多回表\n- where子句中的or：如果只有一个条件是有索引，那就会失效，因为在or条件下只用满足一个就可以了\n\n## Mysql事务\n首先是事务的ACID四个特性：\n- 原子性：事务内的东西是同时完成或者同时取消的。innodb的事务可以做到，redis的事务就没有原子性。\n- 一致性：事务完成前后，由一个一致的状态转移到另一个一致的状态。例如银行转账两个账户600+800，转账后总数是不能变的\n- 隔离性：并发导致的事务之间应该隔离，一个人购买商品的事务不应该影响另一个人的购买\n- 持久性：持久化下来，掉电也不会丢失\n\n\nInnoDB的持久性是通过redolog完成的，原子性是undolog，隔离性是MVCC完成的，一致性是上面三者都完成后就会保证一致性。\n\n## 事务隔离级别\n- 脏读：一个事务还没有提交，别的事务就能看到它的修改了。例如事务1修改了余额，事务2读取了这部分，但是事务1触发回滚，这就导致事务2读到了脏数据。\n- 不可重复读：在一个事务内读取两次数据，两次数据读取的不一样。这通常是由于另一个事务的提交，这个数据对于其他事务都是可见的（有点像volatile）\n- 幻读： 查询数量，事务内前后查询两次不一致，像发生了幻觉\n\n事务隔离级别：\n- 读未提交：\n- 读提交：解决脏读\n- 可重复读：解决脏读+不可重复读\n- 串行化：解决幻读+不可重复读+幻读\n\n## MVCC（多版本并发控制）\n在前面讲的记录中有两个隐藏字段：添加该条记录的事务id和指向上一个版本undolog指针。MVCC是依靠这两个字段完成的，更新数据时会在数据页中替换新的记录，然后将旧的记录作为指针保存下来，这时所有的版本都连成一条链，包含了当时进行更新的事务id。\n\nMVCC提供了ReadView这个数据结构，由于事务id是顺序递增的，ReadView将事务id分为三个部分，已经提交的事务，正在进行的事务和没有发生的事务。已经提交的事务是有访问权的，正在运行的事务是不允许查看的，此时如果发生并发现象，查询数据的时候会根据这三个范围和当前全部的事务id进行过滤，这样就能避免读到还没有提交事务的数据。从而解决了脏读问题\n\n**MVCC是如何解决不可重复读的**\n\n主要是ReadView生成的时机问题，如果每一次快照读都生成一个ReadView，有可能在两次快照读期间有别的事务提交了，这两次的ReadView范围就会不一样，还是会发生不可重复读。但是如果在事务刚开始的时候就生成ReadView，就能保证执行下来的可见性，即使在第二次读之前，在readview中的那个事务提交了，还能保证接下来的读操作不可见这个数据。\n\n**MVCC解决了幻读吗**\n\n在默认隔离级别下的innoDB很大程度能够避免幻读，范围查询也需要看版本号。在某些情况下还是能出现幻读，例如当前没有s=5的数据，此时进行查询是没有的，另一个事务添加了这条数据，此时事务id在readview里面，是不允许访问的。但是原来事务又更新了这条数据，现在事务id是自己的了。下次查询的时候就会查询出来。（虽然更新一条不存在的数据逻辑上很诡异）\n\n需要通过间隙锁才能解决幻读，也就是锁住一段范围内的，其他事务添加这段范围内的数据就会被阻塞。\n\n## redolog\n前面所讲的数据库事务的ACID中，持久性就是由redolog保证的。mysql在更新的时候先写redolog，然后再在合适的实际将redolog真正写在数据页上。首先读取操作的时候不会每次都从磁盘io，会缓存在buffer pool中，那么写操作也会在buffer pool中操作，此时就变成了一个脏页。redolog的左右就是把这个脏页的操作记录并持久化下来，这个过程是比刷数据页要快的，很多时候查询的并不是顺序读，有可能在很多数据页上。\n\n**刷盘时机:**\n\nredolog也是有缓冲区的，把buffer pool中的修改保存在redolog缓存区中，然后再在合适的时机刷盘，一般有以下几个时机：\n- mysql正常关闭\n- 后台线程会每隔1s刷盘\n- 当缓冲区满了一半就触发\n- 根据刷盘策略\n\n刷盘策略有以下三种：\n- 每次事务提交只写在缓冲区中，但是如果在1s内掉电，redolog buffer就会没有了，这种策略可能会掉最多1s的数据\n- 每次事务提交直接写磁盘，效率较低，但是安全\n- 每次事务提交给操作系统的page cache，交给操作系统写。这种情况如果mysql停机了还是能写，因为操作系统没有停止，但是如果掉点了还是会丢1s的数据，算是上面两种的一个折中。\n\n<div style=\"text-align: center;\">\n  <img src=\"../img/Mysql/img_16.png\" alt=\"\" />\n</div>\n\n**日志文件组：**\n\nredolog不是一个文件，而是多个文件，这些文件按照环形存储，会覆盖。头指针是checkpoint，尾指针是write pos。每次有缓冲区来的数据就移动pos。如果mysql进行了redolog到数据块的持久化，checkpoint就往后移动。\n\n如果这个环形文件满了，就需要阻塞mysql，进行部分redolog到数据页的操作。\n\n**总结：**\n\nredolog保证了mysql的持久性，这种操作比直接修改数据页要高效，由于直接修改数据页可能会存在随机写的情况，而redolog是追加写，就好像每次找东西随机写就是看目录找书页，而追加就是往后翻一页。\n\n然而redolog在某些策略下也不是万无一失的，因为redolog也是有缓存的，如果没有及时写在磁盘里就会丢失1s的数据。\n\n> 由此可见redolog和真正存在数据页中还有一段时间差，那这个时候来了一个读请求，怎么保证一致性？\n> 会在buffer pool找，而不是直接去磁盘找，本来redolog就是为了存buffer pool的脏页的，是内存同步到外存，\n\n## binlog\nbinlog是解决主从同步的一种日志形式，只记录写操作，select和show是不记录的。有三种格式形式：\n- statement：只记录更新的语句，不管前后状态，一般存储的数据量较小\n- row：不光记录更新的具体语句，更新前后的状态也保存，一般用在需要计算的sql语句，例如now()，用第一种方式就会造成数据不一致，第二种会保存在主机上面的结果。当然存储的数据量会更大\n- mix：根据具体sql的形式选取策略，例如正常sql就用statement，涉及到计算的就用row\n\n**刷盘方式：**\n\n与redolog类似，也有一个缓冲区，但是binlog的缓冲区线程之间是独立的，最终都要写进磁盘。\n- 0：不同线程的binlog buffer写到page cache里面，由操作系统写回\n- 1：直接写到磁盘\n- n：n个线程的binlog buffer写到page cache后触发一次写到磁盘。掉电就丢失n个线程的数据\n\n## binlog就够了为什么还需要redolog\nbinlog主要负责主从同步，redolog负责恢复数据库。一个是外部机制，一个是内部机制，两者独立但是互补，服务于不同的需求。\n\n## 两阶段提交\n如果在binlog和redolog之间出现问题就会导致数据不一致。所以需要两阶段提交\n\n<div style=\"text-align: center;\">\n  <img src=\"../img/Mysql/img_17.png\" alt=\"\" />\n</div>\n\n写redo的时候设置为prepare，等到binlog写完了才设置为commit状态。mysql认为binlog写完了就是一个事务结束，即便没有再设置redolog的状态。\n\n例如在写完redolog时宕机，binlog没有写，这个时候就会触发回滚；但是写完了binlog宕机了，redolog没有被改变为commit，还是会被认为事务提交了\n\n**存在什么问题：**\n- 多线程下需要原子性\n- 如果是“双1配置”，一个事务io两次\n\n## Mysql的锁\n1. 数据库锁：只有在进行数据库备份或者迁移的时候才会用\n2. 表级锁：表级读锁写锁，读写意向锁，自增锁，元数据锁\n\n|    | X   | IX  | S   | IS  |\n|----|-----|-----|-----|-----|\n| X  | 不兼容 | 不兼容 | 不兼容 | 不兼容 |\n| IX | 不兼容 | 兼容  | 不兼容 | 兼容  |\n| S  | 不兼容 | 不兼容 | 兼容  | 兼容  |\n| IS | 不兼容 | 兼容  | 兼容  | 兼容  |\n> 解释一下：IX表示表内已经有行在进行X操作了，此时不允许全局上锁，所以全局X和S都不允许；全局X的条件是里面没有任何一个在进行读写操作，所以与所有都不兼容；IS和IX能兼容是因为都只是作为指示作用，IS和IX可能锁住的列不一样，他们的存在只能影响全局锁。\n3. 行级锁：记录锁（锁住id=1这条记录），间隔锁（锁住（2，5）区间的），临键锁（锁住（2,5]）\n\n## Mysql什么时候上锁\n执行select for update或者update的时候，在事务执行范围内会上一个对当前事务有效的锁，一般锁的单位是**临键锁**，在某些情况下会退化成记录锁或者间隔锁。这一部分的提出正是为了解决。\n\n**唯一索引等值查询**\n\n1. select * from user where id = 1 for update;如果有id=1这条记录，临键锁就会退化为记录锁，只锁住当前这个id=1，因为这样就不会有别的事务来进行更改避免不可重复读\n2. select * from user where id = 1 for update；如果没有id=1这条记录，就会退化为间隔锁，找到最近的两个区间例如（0，3）锁住这段，不允许添加数据避免幻读\n\n**唯一索引范围查询**\n\n1. select * from user where id > 15 for update;如果有id=20的记录，那就会加锁(15,20]，(20,INf]\n2. select * from user where id >= 15 for update;如果有id=15，加锁15，(15,20]，(20,INf]\n3. select * from user where id < 6 for update;如果有id=1,5,10的记录，加锁(-INF,1],(1,5],(5,10)\n4. select * from user where id <= 5 for update;(-INF,1],(1,5]\n\n**非唯一索引等值查找**\n\n只用记住一点，非唯一索引是按照大小排序的，索引值相同再按照id升序排序。{(15,14),(25,20)}，锁住非唯一索引（15，25），如果新增的值在其中就不允许插入，如果=15且id>14也是不允许的，右边同理。\n\n\n## Mysql死锁\n\n<div style=\"text-align: center;\">\n  <img src=\"../img/Mysql/img_18.png\" alt=\"\" />\n</div>\n\n由于间隙锁是可以重复的，两边都锁住了不允许添加。\n\n## 慢sql\n**产生慢sql的原因**\n\n1. 索引未生效或者根本没有索引，导致全表扫描\n2. 数据量太大了，加索引都不行。这个时候就要考虑分库分表了\n3. sql书写不当，过多的join操作或者子查询过多，子查询会产生一张新表；或者是深度分页；用了比较长的排序\n4. 数据量比较大的时候，redolog满了，在更新checkpoint的时候会阻塞\n5. 等待行级锁\n\n**如何发现慢sql**\n\n1. mysql自己可以开启慢sql查询日志，将查询时间大于1s的记录下来\n2. 预防慢sql，做全局sql扫描。阿里的[这篇文章](https://mp.weixin.qq.com/s/LZRSQJufGRpRw6u4h_Uyww)提到了可以使用JVM的沙箱模型。相当于一个大的aop模型（值得一提的是gclib也是跟这个原理类似），通过在实例前后进行监听就可以无侵入式地录制HTTP/Java/Dubbo入参/返回值，业务系统无感知。基于这个能力，我们可以方便的采集和SQL执行相关的Java方法参数以及返回值。通过配置采集点，来采集执行sql的java代码的相关方法、参数和返回值，辅助实现sql采集功能。\n\n**如何识别高危sql**\n\n- 不符合规范的sql：不允许使用外键，深度分页优化，子查找过多，有*\n- 用explain观察，row字段过多需要优化，type字段是否使用索引，extra是否有排序操作，以及是否索引下推，索引覆盖。\n\n## 常见的sql优化手段\n1. 避免使用*：因为在查询的过程中还有一个预处理过程，可以自己写的就不要交给sql去查询，而且无法使用覆盖索引，也就是不用回表\n2. 深度分页优化：比如有1000000页，查询这个页的时候就是深度分页了。可以用子表查询这个页的第一条记录，然后再去查比这个大的10条记录作为范围。有两个缺点，首先是id必须自增，其次是子表查询不推荐\n3. 避免多join，避免使用外键\n4. 优化慢sql\n5. 正确使用索引：选择合适的字段创建索引（不为NULL的字段，频繁查找但是不是频繁增删的字段）；避免索引失效；考虑在字符串上建立前缀索引；避免冗余索引","tags":["Mysql"]},{"title":"一句话刷题","url":"/2024/09/19/一句话刷题/","content":"- **无重复字符的最长字串**：滑动窗口+heshset，右窗口需要在for外，每轮循环移动右窗口\n- LRU缓存机制：偷懒直接linkedHashMap，hashmap长度并不为cachesize，重写替换条件为size>cachesize，构造函数要传一个允许get后重排序的boolean\n- 反转链表：pre,p,q三指针\n- 数组第k个最大元素：快排思想，根据每次划分的结果二分查找\n- 最大子数组和：动态规划，看前面的是否为负数，为负数就抛弃自己作为开头，为正数就自己加上\n- 合并两个有序链表：模拟题\n- **最长回文字串**：二维动态规划，撇去前后剩下的也是回文子串\n- 两数之和：hashmap存余数\n- 二叉树层次遍历：queue秒杀，记得最牢的一集\n- 搜索旋转排序数组：随便划分总有一段是顺序的\n- 岛屿数量：dfs上下左右\n- 全排列：dfs回溯，visited数组\n- 有效的括号：stack秒了\n- 二叉树的最近公共祖先：回溯，找两个列表的最末尾相同元素\n- 环形链表：hashmap看是否包括\n- 反转链表2：模拟\n- **最长递增子序列**：动态规划，找前面每一个比自己小的，取最大值+1\n- 字符串相加：太简单不说\n- **接雨水**：单调栈，左->右和右->左，取最小值\n- **编辑距离**：二维动态规划，需要考虑左右对角线三个，对角线（相同），左右对角线取最大+1（不相同）\n- **最长公共子序列**：二维动态规划。比上面的好理解，对角线的+1（相同），左右取最大（不相同）\n- 栈实现队列：两个栈来回倒，一个入一个出，在入之前检查出是否为空，反之亦然\n- 括号生成：左右括号剩余数量判别，右边的数量一定得大于等于左边剩余\n- 反转字符串中的单词：split和stringbuilder\n- 二叉树的锯齿状层次遍历： 基本上就是普通的层次遍历，用reversed就可以倒转。\n- 环形链表2：哈希表存节点，遍历的时候同时检车哈希表是否含有\n- 二叉树右视图：层次遍历\n- 子集：回溯，以1234为例，依次增加子集的元素个数\n- 组合总和：回溯，每次dfs都是传剩余，递归终止条件是剩余<0，=0就添加一条，最后会有重复，需要一个排序+hashset去重\n- **分割回文串：** 两步走：首先二维动态规则找到所有的回文字串；再类似prim算法遍历dp数组回溯\n- 单词搜索：回溯，上下左右依次dfs，条件全为或，有一个为真就会是真\n- 找到字符串中所有字母异位词：哈希表（偷懒方法，滑动窗口每一个与目标字符串数组进行比较，用toCharArray和Arrays.sort和equals）\n- 电话号码的字母组合：哈希表写一个键盘集合，然后回溯\n- 重排链表：快慢指针找中点（奇数得在正常中点的后面一个）+后半部分倒转链表+双指针连接\n- 旋转图像：沿着左下右上对角线对调，然后按行按照中线对调\n- 搜索二维矩阵2：四个指针上下左右，逐步缩小范围，当循环内不再发生改变就为真\n- 腐烂的橘子：多源bfs，跟二叉树的层次遍历差不多\n- 课程表：拓扑排序，先记录所有点的入度，每次都找到入度为0的点，如果最后还有入度不为0的说明有环\n- 从前序与中序遍历序列构造二叉树：经典题目，要时不时写一下\n- 实现前缀树：Trim[26]数组，要用一个isend区分是前缀还是单词\n- 有序数组转化为二叉搜索树：二分+递归\n- **单词拆分：** 动态规划，当dp[j] == 1 && hashset.contains(s.subString(i,j))时dp[i]=1，字符串在字典里当且仅当划分的两部分应该也在字典里\n- 打家劫舍：经典dp题目，有两种策略，不抢的时候选择上一个两种情况中最大的，抢的时候前一个只能不抢。\n- **最大正方形：** 二维dp，dp[i][j]表示以ij为左下角的最大正方形，取左，上，对角线三个里面最小的。\n- 最小路径和：二维dp入门题目，看一眼肯定会\n- **零钱兑换：** dp数组长度为钱的多少，dp表示当前钱兑换的最少次数。每次往后移动都需要首尾指针一起动\n- **排序链表：** 菜就多练\n- 两数相加：两个链表做加法，太简单了不说，还有个2，就是反转之后相加\n- k个一组翻转链表：虽然是hard但是很简单，思路一定要清晰\n- **二叉树的最大路径和**：树形dp，用一个全局变量保存全局最大路径和，返回的时候需要判断子树是否为负数，如果都为负数就返回自己，只要有一个是正数就返回自己+正数\n- 二叉树的直径：跟上面思路是一样的，但是dp返回的是当前树的高度，也是用全局变量保存最大值\n- 打家劫舍3：树形dp，根节点抢了子节点都不能抢了，取其中dp[0]的和再加自己的，不抢就找dp[0]和dp[1]中较大的那个\n- 求根到叶子节点数字之和：深度和广度+回溯\n- 路径总和：回溯\n- **字符串解码：** 双栈，存数和字符串，遇到]后进行循环拼接，再将前面的依次出栈直到[\n- 最长连续序列：首先set去重，然后遍历，如果有比当前元素小的就跳过，也就是说每次都找一段里面最小的元素。然后用一个全局变量保存最大的。\n- 最长公共前缀：按列遍历，注意长度是否超出\n- 螺旋矩阵：上下左右四个指针，需要注意只有一列或者一行的情况，在循环的过程中要保持上下界左右界不越界\n- 合并K个升序链表：两两合并即可，或者每次都找最小节点\n- 删除链表的倒数第N个节点：栈先进后出\n- 复制ip地址：回溯，但是还有一点边界没搞明白\n- **二叉树的最大宽度**：新建一个内部类，保存编号，左节点是*2，右节点是*2+1，按层次遍历，找出一层内编号差最大的。\n- 对称二叉树：递归比较左子树和右字数\n- **每种字符至少取k个**：反向思维，滑动窗口。先哈希表存所有元素的个数，然后再-k，r随着循环移动，移动一次对应元素个数-1，当哈希表内出现负数元素，左边指针移动并且释放元素直到哈希表内都不为负数。最后取出最大的那次\n- **最小覆盖字串**：滑动窗口，首先哈希表保存目标字符的个数。r随着循环移动，每次都从哈希表减去对应字符的个数，如果不在哈希表就跳过。如果哈希表所有元素都小于等于0（也就是说集齐了所有，这里有可能比原来多），那么就记录这个长度，左边指针开始动直到再一次没有集齐元素\n- 二叉树的最大深度：左右子树求最大，dp\n- 平衡二叉树判断，返回一个数组，[0]是高度，[1]是是否平衡\n- 岛屿的最大面积：dfs没什么好说的\n- **N皇后**：终极回溯题，上下和两侧的对角线都不能有元素重复。设置四个visited数组，然后每一行开始尝试放q，如果能进行到最后一行那就保存，其余就是回溯操作","tags":["刷题笔记"]},{"title":"hot100天天刷-第一期","url":"/2024/09/19/hot100天天刷-第一期/","content":"> 记住大概的方法就行了\n\n## 1.无重复字符的最短字串（滑动窗口）\n\n[原题：](https://leetcode.cn/problems/longest-substring-without-repeating-characters/description/)给定一个字符串，找出不含有重复字符的最长**字串**（字串是连续的）。\n\n滑动窗口：主要思想是随着左边窗口的移动，右边窗口也一定会向右边移动。每一次循环找的都是以charAt(i)开头的最长字串。由于i在增加，在每次移动的同时保存最长字串的长度就是结果。\n\n判断是否重复：当右指针遍历到hashset含有的元素就停止右移。为了保证i移动的同时，后续字符都是以i这个元素开头的，当前循环结束以后需要把charAt(i)从hashset剔除。\n\n```java\npublic static int lengthOfLongestSubstring(String s) {\n    //检查重复性\n    HashSet<Character> hashSet = new HashSet<>();\n    int max = 0;\n    //p是最右边的指针\n    int p = 0;\n    for (int i = 0; i < s.length(); i++) {\n        //p一直向右边移动，直到碰到set里面有的元素，就进行下一次大循环\n        while (p < s.length()){\n            if (!hashSet.contains(s.charAt(p))){\n                hashSet.add(s.charAt(p));\n                p++;\n            }\n            else {\n                break;\n            }\n        }\n        //留一个max来返回最大值\n        max = Math.max(max,hashSet.size());\n        //在下一次大循环前保证set只有当前元素以后的，所以要把上一个元素从set删除\n        hashSet.remove(s.charAt(i));\n    }\n    return max;\n}\n```\n## 2.最长回文字串（二维动态规划）\n[原题：](https://leetcode.cn/problems/longest-palindromic-substring/description/)给你一个字符串，找到最长的回文字串。\n\n动态规划：回文子串的性质是，把最开始和最末尾的元素删除还是一个回文字串。状态转移方程为\n$$\ndp[i][i] = 1\n$$\n$$\ndp[i][j] = dp[i+1][j-1] \\land (S_{i} == S_{j})\n$$\n\n\n```java\npublic String longestPalindrome(String s) {\n    boolean[][] dp = new boolean[s.length()][s.length()];\n    int length = 1;\n    int head = 0,tail=0;\n    //对角线上的元素都为1\n    for (int i = 0; i < s.length(); i++) {\n        dp[i][i] = true;\n    }\n    //沿着对角线逐渐增大，而不是根据表格行列遍历\n    for (int k = 1; k < s.length(); k++) {\n        for (int i = 0; i < s.length() - 1; i++) {\n            //字串长度为2的时候单独考虑\n            if (k == 1){\n                //aa，bb这种字串\n                if (s.charAt(i) == s.charAt(i+k)){\n                    dp[i][i+k] = true;\n                    //循环外部变量保存最大值和起始点和重点，因为返回的是具体的字串，需要调用substring\n                    if (length < k+1){\n                        length = k+1;\n                        head = i;tail = i+k;\n                    }\n                }\n            }\n            //其余长度\n            //只有dp[i+1][i+k-1]为回文字串且[i]和[i+k]相同的时候才是一个回文字串\n            if (i+k < s.length() && s.charAt(i) == s.charAt(i+k) && dp[i+1][i+k-1]){\n                dp[i][i+k] = true;\n                if (k+1 > length){\n                    length = k+1;\n                    head = i;tail = i+k;\n                }\n            }\n\n        }\n    }\n    //System.out.println(Arrays.deepToString(dp));\n    return s.substring(head,tail+1);\n}\n```\n\n## 3.最大子数组和（动态规划）\n\n[原题：](https://leetcode.cn/problems/maximum-subarray/description/)给你一个整数数组 nums ，请你找出一个具有最大和的连续子数组（子数组最少包含一个元素），返回其最大和。\n\n动态规划：pre表示的是i之前的最大子数组和的最大值.如果pre是负数，那就相当于拖nums[i]后腿了，与其加上前面的不如自己作为开头。状态转移方程：\n\n$$f(i) = max ( f(i-1)+nums[i],nums[i] ) $$\n\n```java\npublic int maxSubArray(int[] nums) {\n    int pre = 0;\n    int max = Integer.MIN_VALUE;\n    for (int i = 0; i < nums.length; i++) {\n        pre = Math.max(pre+nums[i],nums[i]);\n        max = Math.max(max,pre);\n    }\n    return max;\n}\n```\n\n## 4.最长公共子序列（二维动态规划，与dp[i-1][j-1],dp[i-1][j],dp[i][j-1]有关）\n> 这道题和编辑长度是一个类型的\n\n[原题：](https://leetcode.cn/problems/longest-common-subsequence/description/)给定两个字符串 text1 和 text2，返回这两个字符串的最长 公共子序列 的长度。如果不存在 公共子序列 ，返回 0 。\n\n需要考虑dp[i-1][j-1],dp[i-1][j],dp[i][j-1]，所以dp[][]要多一行一列，状态转移方程：\n- text1.charAt(i) == text2.charAt(j):$dp[i][j] = dp[i-1][j-1]+1$\n- text1.charAt(i) != text2.charAt(j):$dp[i][j] = max(dp[i-1][j],dp[i][j-1])$\n\n```java\npublic int longestCommonSubsequence(String text1, String text2) {\n    //二维动态规划\n    int[][] dp = new int[text1.length()+1][text2.length()+1];\n    for (int i = 1; i < dp.length; i++) {\n        for (int j = 1; j < dp[0].length; j++) {\n            if (text1.charAt(i-1) == text2.charAt(j-1)){\n                dp[i][j] = dp[i-1][j-1]+1;\n            }\n            else {\n                dp[i][j] = Math.max(dp[i-1][j],dp[i][j-1]);\n            }\n        }\n    }\n    return dp[dp.length-1][dp[0].length-1];\n}\n```\n## 5.编辑距离（二维动态规划，与dp[i-1][j-1],dp[i-1][j],dp[i][j-1]有关）\n\n[原题：](https://leetcode.cn/problems/edit-distance/)给你两个单词 word1 和 word2， 请返回将 word1 转换成 word2 所使用的最少操作数  。\n\n与上面那个很相似，如果两个字符是相同的话就看dp[i-1][j-1]，但是不同的话也需要看dp[i-1][j-1]，因为即便字符不同也可以通过替换解决。状态转移方程为：\n- text1.charAt(i) == text2.charAt(j):$dp[i][j] = dp[i-1][j-1]$\n- text1.charAt(i) != text2.charAt(j):$dp[i][j] = min(dp[i-1][j],dp[i][j-1],dp[i-1][j-1])+1$\n\n还有不同的就是初始化边缘不再都是0了，根据插入字符的数量决定。\n\n```java\npublic int minDistance(String word1, String word2) {\n    int[][] dp = new int[word1.length()+1][word2.length()+1];\n    for (int i = 0; i < dp.length; i++) {\n        dp[i][0] = i;\n    }\n    for (int i = 0; i < dp[0].length; i++) {\n        dp[0][i] = i;\n    }\n    for (int i = 1; i < dp.length; i++) {\n        for (int j = 1; j < dp[0].length; j++) {\n            if (word1.charAt(i-1) == word2.charAt(j-1)){\n                dp[i][j] = dp[i-1][j-1];\n            }\n            else {\n                dp[i][j] = Math.min(dp[i-1][j-1],Math.min(dp[i-1][j],dp[i][j-1]))+1;\n            }\n        }\n    }\n    //System.out.println(Arrays.deepToString(dp));\n    return dp[dp.length-1][dp[0].length-1];\n}\n```","tags":["刷题笔记"]},{"title":"Redis面试篇","url":"/2024/09/17/Redis面试篇/","content":"## redis的数据结构\n- 基本数据结构：string，hash，list，set，sorted set\n- 高级数据结构：geo，hyperloglog，bitmap\n\n## String的SDS\nredis是基于c的，但是其并没有采用c语言的字符串形式。c语言的字符串是给首数据的指针，遍历字符串的时候一直往后找直到出现\"\\0\"，这对于需要快速处理字符串的redis是不可接受的。\n\n于是redis发明了一个SDS（简单动态字符串），相对于原先的c语言字符串，这里最大的不同是使用一个数据结构里面保存字符串的数组和长度。优点如下：\n1. 统计字符串的时间复杂度为o(1)：由于c语言的字符串是需要遍历的，类似链表的结构，但是sds直接就有一个字段保存长度\n2. 可以保存二进制信息：由于c语言是要找到\"\\0\"为末尾信息，不允许有空格，使得c语言字符串的存储形式受限；而sds没有这方面的特性，直接保存在一个数组里面，因此redis可以用字符串存储图片等二进制信息。\n3. 不会发生溢出：由于保存了数组的长度和数组剩余长度，当进行字符串拼接的时候会校验新的长度是否会导致数组溢出，c语言是没有这个特性的。\n\n## 压缩队列ziplist\n在讲其他数据结构之前，需要讲一下压缩。在redis中list，set和zset都有两种数据结构，其中一种就是ziplist，初始为ziplist而当元素数量达到一定阈值就会进化为其他的。\n\n一般来说存储数据每一个格子长度都是固定的，但redis为了节约这部分空白数据就使用了动态长度，也就是说每一个格子只会正好包括数据而不会有空白。但是这样不是固定长度也给遍历带来的问题，不能够按照数组那种长度*个数来找地址，于是redis也做了一些方便搜索的数据结构。\n\n**连锁更新问题**\n\n除了搜索有缺点，还存在着更新问题，也能想到，如果在中间的某个数据需要更新，那么后面所有的数据都需要向后移动，造成很大的开销，所以redis只在数据量很小的情况下使用这个数据结构。\n\n## hash\n类似jdk1.7实现的hashmap，就是拉链法解决哈希冲突。相较于jdk的hashmap，这里的扩容机制有很大不同。\n\n**渐进式扩容**\n对于redis来说，由于是单线程，hashmap扩容会导致线程阻塞，影响性能，所以采用了渐进式hash。每当需要进行扩容的时候不会立刻一次性全部扩容，而是同时创建一个两倍大小的hashmap，在以后的每一次请求中，都会移动一部分数据从旧的hashmap到新的hashmap中，新增的数据就直接写在新表上。\n\n这样将一次扩容平坦到每次请求中，完成扩容的同时也没有严重影响性能。但是此时get一个数据就需要在两个表上都要找。\n\n## sorted set和跳表\nzset是按照分数score有序排列的集合，底层使用了ziplist+skiplist，当元素数量大于128且元素长度大于64字节的时候自动扩展为跳表。\n\n**跳表**\n\n单向链表查找的时间复杂度平均为o(n)，跳表用多层链表实现在单向链表上查询的时间复杂度降低为logn\n\n<div style=\"text-align: center;\">\n  <img src=\"../img/Java/img_10.png\" alt=\"\" />\n</div>\n\n查询就像坐了快车，只有大站到达，节约了不少遍历的时间，一般来说上一层的节点数量是下一层的一半，当需要添加数据的时候如何确定这个节点的层级呢。如果说加入一个节点那么它在最底层的概率为100%，由于上面一层的数量是一半，那么他是2层节点的概率就为50%，3层的就为25%。\n```java\n// 随机生成节点的层数\nprivate int randomLevel() {\n    int lvl = 0;\n    while (random.nextFloat() < P && lvl < MAX_LEVEL) {\n        lvl++;\n    }\n    return lvl;\n}\n```\n**为什么用跳表而不是红黑树或者b树**\n- hashmap的红黑树结构不适合范围查找，zset有个命令就是查询score在某个范围之内的，而且指针数量也较少，实现难度较低\n- b树数据结构较大，内存不友好。节点分裂等实现起来较为复杂。\n\n## redis为什么这么快\n1. 基于单线程的io阻塞模型，能够高效的进行网络io\n2. 基于内存，且有自己的vm，不会导致虚拟内存频繁换入换出\n3. 高效的数据结构\n\n## redis的单线程io多路复用模型\n> select/poll/epoll：\n> 这三种是操作系统提供给程序员的系统调用api，epoll只有linux内核可以用，这三个系统调用封装了io多路复用机制：\n> - select：对于到来的socket链接，操作系统会把它按照特定的数据结构组织成一个文件（这个文件中元素的数量是有上限的，为1024），select操作将这个文件给内核态进行遍历，根据这些数据结构的状态找到需要进行处理的socket然后标记，再返回给用户态。这样的优点是简单直接，缺点是找到需要处理的socket时间复杂度大，而且能够处理的socket有限，所以仅仅支持小规模的io多路复用\n> - poll：是select的升级版，主要是将1024这个上线取消了，使用了链表就没有上限了，其余从用户态到内核态处理，从内核态到用户态都是一样的，都需要遍历两次，效率较低。\n> - epoll：首先是对于数据结构的处理，epoll在内核态维护了一个红黑树，使得处理socket不用每次都在两个态之间每次都复制，仅仅是将新增的加入红黑树，而且查询效率也很高；其次是采用了事件驱动的方法，并不会像select/poll那样轮询，这里是直接阻塞，等有消息再通知，是一种阻塞的方式。\n> - 水平触发和边缘触发：水平触发是有事件来了就一直通知，直到事件被处理；边缘触发是只通知一次，但是消息可能会丢失。\n\nredis基于reactor模式建立了一套高效的单线程io模型（这里的单线程指的是从socket到epoll发现到执行器处理的流程是单线程），主要组件有：\n- 多个socket（多个客户端请求，四元组即为一个socket）\n- io多路复用：redis是基于epoll的，实现事件驱动的监听机制\n- 事件分派器：根据epoll_wait监听到的socket事件来决定接下来的执行逻辑\n- 事件执行逻辑：有accept，read，write。accept逻辑是根据新来的socket加入红黑树维护并监听\n\n<div style=\"text-align: center;\">\n  <img src=\"../img/Java/img_11.png\" alt=\"\" />\n</div>\n\n## redis持久化\n- rdb：将当前的redis数据压缩成一个rdb文件，在文件压缩的过程中（save）会阻塞主线程，但是可以采用bgsave的方式，开辟一个新的线程执行。优点是数据形成的文件dump格式紧凑，redis恢复速度快；缺点是在两次save中间可能会导致丢数据，需要fork一个线程来进行save操作\n- aof：类似记录日志，每次操作都需要保存。优点是实时性比rdb要好，不容易丢数据；缺点是文件占的地方很大，可能偶尔还需要一个压缩线程来压缩，恢复速度也比dump.rdb慢\n\n**aof的刷盘方式**：\n- always：每次操作完就刷盘一条aof\n- everysec：每一秒钟刷盘一次\n- no：不主动刷盘，靠操作系统来刷盘\n\n**aof为什么是一条命令执行完了才触发**：\n1. 有可能命令是错误的，只有正确的命令会aof\n2. 不会阻塞当前的任务\n3. 缺点是反着来的，不阻塞当前任务但是会阻塞下条任务，有可能执行校验完了执行完了，但是出现异常使得数据丢失没有存储\n\n**rdb的机制**：\nbgsave：copyonwirte\nsave：save 50 1000，50s有1000条变化就触发\n\n## 过期删除\n- 定时清除：在设置key的过期时间的同时给一个定时任务，过期了定时任务删除。这种策略对于cpu不友好，还需要监控每一个key的剩余时间\n- 惰性删除：过期了不会立即删除，直到有访问的时候查询过期时间表，过期了就删除，返回空值。缺点是对于内存不友好\n- 定期删除：根据某个时间进行定期删除，算是上面两种的折中方案\n\nredis是惰性删除和定期删除相结合的过期删除策略，根据cpu运载情况选择策略，对于定期删除是随机挑选一些key检查过期情况，其中过期的删除，但是当这个比例大于某个阈值的时候redis就认为有大量过期key需要处理，因此会立即做一次删除，以此类推。\n\n## 内存淘汰策略\n1. volatile-lru\n2. volatile-lfu\n3. volatile-random\n4. volatile-ttl\n5. all-lru\n6. all-lfu\n7. all-random\n8. no-eviction：默认的策略，满了会报错，驱逐新请求\n\n## 主从复制\n### 全量复制\n1. 从服务器向主服务器发送replicaof命令，参数是自己的ip地址，请求主机的唯一id\n2. 主机收到了之后返回自己的runid和当前复制的offset\n3. 接着主机开始bgsave自己的rdb文件，与从机同步，每次复制都会有一个offset\n4. 在复制的过程中可能也会有新的数据，此时主机会用一个缓冲区缓存这部分数据\n5. 从机接受完成后，主机再把刚刚缓冲区的传给从机器\n\n### 增量复制\n主机会维护一个环形缓冲区，储存最近的操作。如果这个环形缓冲区满了就说明有的请求恢复不了了，需要重新进行增量复制。如果没有满，就会用到上面说的那个offset进行增量复制。增量复制的时间很短，毕竟只用传掉线时间丢失的一部分数据。\n\n这个缓冲区的大小应该设置为比掉线恢复时间*平均输入数据稍大，这样就可以避免丢失数据而全量复制\n\n## 哨兵机制\n> 哨兵也是集群，集群管理集群\n\n**主官下线和客观下线**\n\nredis集群通过心跳机制（ping-pong）来检测服务器是否存活，哨兵定期与服务器实例进行检测，当某一个哨兵发现主服务器没有在规定时间内返回pong，就会认为这个服务器下线了，这就是主观下线：\n\n但是这也有可能是由于网络信道丢失消息，未必是主服务器真的宕机了，所以还需要其他哨兵对其进行判断，当超过参数quorum的哨兵认为是下线了，那么就会被认为是客观下线，这个时候就需要进行故障转移。\n\n**选举和故障转移**\n\n既然主节点发生了故障，那么由谁来进行故障转移呢。那当然是最先发现故障的哨兵，但也可以是两个哨兵同时发现了故障，这个时候就要选举了，每个哨兵只有一个选票，自己发现的需要我选我，当哨兵获得的选票大于半数且也要大于quorum的时候就被选举为故障转移的节点。\n\n接下来就由这个节点进行故障转移：\n1. 主节点周围的从节点选一个作为下个主节点，进行转移：首先过滤网络状态不好的；再按照优先级，复制进度，id号进行择优。（网络状态有一个主从复制的断连次数，根据这个可以排除网络状态）\n2. 其他的从节点全部修改为replicaof这个节点\n3. 订阅者通知频道告诉客户端进行了改变\n4. 继续监视被替换的这个主机，如果上线了就让他变成从机\n\n## 大key问题\n什么是大key，一般来说String占用大于1MB，或者list，Hash，zset，set长度大于5000个会被认为是大key。String这种情况很可能是因为存储了二进制文件数据，其他顺序结构有可能是没有设计好表导致存储了过多数据。\n\n原因：\n- 程序设计不当，使用String存储了二进制文件（SDS是可以存Base64的）\n- 业务规模突然变大：导致list缓存了较多数据\n- 没有清除，例如哈希表缓存了大量没有用的数据\n\n危害：\n- 太大了话会阻塞redis线程，redis脆弱的单线程\n- 网络拥塞，一个大key就是1MB，1000个用户请求就是1GB\n\n如何发现：\n- redis自带的--bigkeys参数\n- scan命令自己手动看，有点像redis自带的任务管理器，结合别的一些指令可以查看各种value的大小。这个命令效率比较低\n- 第三方开源工具\n- 阿里云内嵌的redis有工具能管理大key\n\n如何解决：\n- 大的hash表可以拆分为多个key，使用二次哈希拆分为多个hash\n- 手动清理大的string（其实在redis桌面客户端就可以手动清理了，估计背后也是这一条命令）\n- 采用合适的数据结构：不要拿string来存照片或者文本\n\n\n> redis里有一个非常大的key，比如一个set，set里面可能有几亿条数据，我现在需要把这个key删掉，你会怎么做？\n> 一般情况下del是同步删除，手动清理会阻塞。在4.0版本以上提供了一个unlink命令，可以异步删除。非得用del的话可以先把其二次哈希到不同set，大事化小。\n\n\n## redis的阻塞原因\n1. 使用了o(n)的命令，例如keys，这个会在全表找key匹配，还有一些查询范围的数据（zset那些，这个时候又要吟唱跳表的好处了）\n2. 大key\n3. 持久化：rdb的save会阻塞，bgsave不会；aof会阻塞下一条，重写也会阻塞（就是压缩aof大小的那个操作）\n4. 集群库容等各种硬件问题\n5. swap：就是虚拟内存那一套，磁盘换内存，这对于redis也是很致命的。redis快的其中一条原因就是在内存中。因此需要禁止大量swap。\n\n## 热key问题\n突如其来的集中访问某几个key，处理不好会导致缓存击穿。\n\n如何发现：\n- redis自带的参数--hotkeys，但是用这个的前提是内存替换策略要选lfu相关的（可能是因为有了lfu才开启了频率计算吧）\n- 开源工具\n- monitor命令，可以监控redis当前的一些命令，自己观察某些key的频率手动解决。\n\n如何解决：\n- 读写分离主从架构\n- redis cluster：哈希槽，多个热点数据分散在多个库\n- 二级缓存\n\n","tags":["Redis"]},{"title":"分布式面试篇","url":"/2024/09/14/分布式面试篇/","content":"## 一致性哈希\n传统的哈希取模操作是基于当前服务器数量的，例如有三台服务器，每次算哈希值就对这三个取模，由于模只可能是0，1，2。\n\n上述HASH算法时，会出现一些缺陷：如果服务器已经不能满足缓存需求，就需要增加服务器数量，假设我们增加了一台缓存服务器，此时如果仍然使用上述方法对同一张图片进行缓存，那么这张图片所在的服务器编号必定与原来3台服务器时所在的服务器编号不同，因为除数由3变为了4，最终导致所有缓存的位置都要发生改变，也就是说，当服务器数量发生改变时，所有缓存在一定时间内是失效的，当应用无法从缓存中获取数据时，则会向后端服务器请求数据；同理，假设突然有一台缓存服务器出现了故障，那么我们则需要将故障机器移除，那么缓存服务器数量从3台变为2台，同样会导致大量缓存在同一时间失效，造成了缓存的雪崩，后端服务器将会承受巨大的压力，整个系统很有可能被压垮。为了解决这种情况，就有了一致性哈希算法。\n\n一致性哈希算法也是使用取模的方法，但是取模算法是对服务器的数量进行取模，而一致性哈希算法是对 2^32 取模，具体步骤如下：\n\n步骤一：一致性哈希算法将整个哈希值空间按照顺时针方向组织成一个虚拟的圆环，称为 Hash 环；\n步骤二：接着将各个服务器使用 Hash 函数进行哈希，具体可以选择服务器的IP或主机名作为关键字进行哈希，从而确定每台机器在哈希环上的位置\n步骤三：最后使用算法定位数据访问到相应服务器：将数据key使用相同的函数Hash计算出哈希值，并确定此数据在环上的位置，从此位置沿环顺时针寻找，第一台遇到的服务器就是其应该定位到的服务器\n下面我们使用具体案例说明一下一致性哈希算法的具体流程：\n\n（1）步骤一：哈希环的组织：\n\n<div style=\"text-align: center;\">\n  <img src=\"../img/Distributio/img.png\" alt=\"\" />\n</div>\n\n圆环的正上方的点代表0，0点右侧的第一个点代表1，以此类推，2、3、4、5、6……直到2^32-1,也就是说0点左侧的第一个点代表2^32-1，我们把这个由 2^32 个点组成的圆环称为hash环。\n\n（2）步骤二：确定服务器在哈希环的位置：\n\n哈希算法：hash（服务器的IP） % 2^32\n\n上述公式的计算结果一定是 0 到 2^32-1 之间的整数，那么上图中的 hash 环上必定有一个点与这个整数对应，所以我们可以使用这个整数代表服务器，也就是服务器就可以映射到这个环上，假设我们有 ABC 三台服务器，那么它们在哈希环上的示意图如下：\n\n<div style=\"text-align: center;\">\n  <img src=\"../img/Distributio/img_1.png\" alt=\"\" />\n</div>\n\n3）步骤三：将数据映射到哈希环上：\n\n我们还是使用图片的名称作为 key，所以我们使用下面算法将图片映射在哈希环上：hash（图片名称） % 2^32，假设我们有4张图片，映射后的示意图如下，其中橘黄色的点表示图片：\n\n<div style=\"text-align: center;\">\n  <img src=\"../img/Distributio/img_2.png\" alt=\"\" />\n</div>\n\n那么，怎么算出上图中的图片应该被缓存到哪一台服务上面呢？我们只要从图片的位置开始，沿顺时针方向遇到的第一个服务器就是图片存放的服务器了。最终，1号、2号图片将会被缓存到服务器A上，3号图片将会被缓存到服务器B上，4号图片将会被缓存到服务器C上。\n\n**一致性 hash 算法的优点**\n\n前面提到，如果简单对服务器数量进行取模，那么当服务器数量发生变化时，会产生缓存的雪崩，从而很有可能导致系统崩溃，而使用一致性哈希算法就可以很好的解决这个问题，因为一致性Hash算法对于节点的增减都只需重定位环空间中的一小部分数据，只有部分缓存会失效，不至于将所有压力都在同一时间集中到后端服务器上，具有较好的容错性和可扩展性。\n\n假设服务器B出现了故障，需要将服务器B移除，那么移除前后的示意图如下图所示：\n\n<div style=\"text-align: center;\">\n  <img src=\"../img/Distributio/img_3.png\" alt=\"\" />\n</div>\n\n**一致性 hash 算法的缺点**\n\n可能会出现倾斜，当服务器的哈希值都很接近就会出现在一段区间里分布着大量的服务器，分布不够均匀。因此也有了虚拟节点这个概念。也就是对服务器用多个哈希函数进行多次运算，使其分布更加均匀。、\n\n> 扩展：如果服务器之间有权重，如何设计负载均衡算法：\n> 1. 轮盘赌：多个服务器的权重按百分比进行分配，每次有访问请求就生成随机数，这样权重大的就有更大的概率被选中。缺点是如果服务器进行扩展，这个规则需要更改\n> 2. 一致性哈希的虚拟节点：可以根据权重分配更多的虚拟节点，但是效果有待验证，因为毕竟也是随机的，增加节点并不一定就能获得更大的访问范围。\n\n## 分布式限流算法\n- 固定窗口计数器算法：在一分钟内设定n次访问量，如果超过了n次就拒绝访问。缺点是限流不够平滑，如果在1分钟内前30秒就处理了n个，后面半分钟就等于空转；也无法保证限流速率和攻击，如果服务器的承载力就是1分钟n个请求，在这分钟的后30s给出n个请求，后面一分钟的前半部分给出n个，也能将服务器击垮\n- 滑动窗口计数器算法：划分的更细了，例如划分一秒多少个请求，这样可以应对突发的请求增大，但是也有不平滑的问题，也不好控制速率\n- 漏桶算法：请求进入桶，慢慢漏下来，满了就丢弃请求。这样优点是可以控制速率，但是同时也会导致漏请求\n- 令牌桶算法：桶里面是令牌，可以通过控制令牌的流速改变处理速度\n\n<div style=\"text-align: center;\">\n  <img src=\"../img/Distributio/img_4.png\" alt=\"\" />\n</div>\n\n## 分布式ID生成算法\n- UUID：jdk自带就有这个算法，有五个版本。优点是生成很简单，本地就可以生成；缺点是要用到mac地址，安全性不好，而且不是连续的，对于InnoDB非顺序的id插入可能造成页分裂的问题，影响性能，而且长度也很长，mysql不建议这么长的id\n- 雪花算法：时间戳+机房id+机器id+序列号。正常情况下一毫秒一个机器可以生成4096个。大致是有序的，只不过时间戳是在高位；缺点是高度依赖时间系统，如果服务器出现时间回退就会导致出错\n- 数据库自增：用一个专门的表来插入自增，然后需要id的时候找这个表插入一条，返回的id即为自己的id。优点是简单；缺点是自增的太明显了，而且不适合分布式\n- 美团开源生成算法leaf\n\n## 分布式事务\n- 2pc：两阶段提交。分布式事务有两个角色，一个是事务管理者，一个是事务执行者（比如mysql或者redis）。两个阶段指的是准备阶段和提交阶段。事务管理者会询问当前事务下所有的执行器是否已经准备就绪，执行器接到这个询问后开始执行自己的逻辑，如果没有出错就返回yes，出错了返回no；第二阶段是提交，如果所有都返回yes，管理者就会让其提交事务，如果有一个是no就会回滚。缺点是会有单点问题，极度依赖事务管理者，如果宕机就会一直卡住，其次是会有阻塞，在事务执行期间会阻塞其他事务\n- 3pc：三阶段提交，比上面的多了一个开始的询问阶段，首先是准备阶段，事务管理者只会问他们准备好了吗，而不会实际的执行自己的任务；如果此时有人返回no就会回滚，但是由于没有执行任务所以回滚的代价小很多；二阶段是预提交阶段，当所有人都准备好了就会执行自己的任务，此时跟上面的一样，如果有no就回滚，这是最后的回滚机会了；三阶段是提交，如果所有都返回为yes，事务管理器就会让他们提交事务。优点是这里都有超时机制，避免了阻塞；但是还是有不一致的现象，3pc用的比较少\n- TCC：需要代码实现的两阶段提交，分为try，confirm，cancel。首先try是预留业务资源；confirm确认执行业务逻辑，并且消耗try的业务资源，如果出现错误就会cancel，取消这个操作并且释放try的资源\n- AT：seata的一种模式，可以视为2pc的一种实现模式。原理一阶段是通过代理sql，先解析sql保存执行前后的数据快照和undolog；二阶段如果有出错就按照这个undolog进行回滚。\n- saga：将事务变成线性链表，然后一个一个做，中间某个事务出现了问题就会给前面已经提交的事务一个补偿措施。","tags":["Java"]},{"title":"Spring面试篇","url":"/2024/09/14/Spring面试篇/","content":"## Spring的循环依赖问题怎么解决\n循环依赖问题指的是在依赖注入过程中两个类互相依赖，这会导致实例化的过程混乱，因为一个类实例化需要另一个类已经实例化了，一般有三种循环依赖问题：\n1. 构造函数依赖循环，这个解决不了\n2. setter的依赖循环，其中bean是多例（prototype）\n3. setter的依赖循环，其中bean是单例。\n\nSpring仅仅对第三种进行了解决。第一种在new的时候就进行不下去了，无法生成对象；第二种由于无穷无尽的循环会生成很多的bean，最后导致oom。\n\n**如何解决**\n\nspring用一个三级缓存解决了，总的来说就是在一个类没有初始化的时候提前暴露bean存到三级缓存中，然后进行这个类初始化完成之后就放回第一层缓存中。\n\n创建 BeanA： Spring 首先将 BeanA 的实例存储在三级缓存中。 在创建 BeanA 的过程中，发现需要注入 BeanB。\n\n创建 BeanB： Spring 将 BeanB 的实例存储在三级缓存中（因为 BeanB 也需要 BeanA）。 在创建 BeanB 的过程中，Spring 发现 BeanA 还未完全初始化，所以从三级缓存中获取 BeanA 的代理对象。\n\n注入依赖： 当 BeanA 创建完成后，Spring 会将 BeanB 注入到 BeanA 的字段中。 同样地，BeanA 会注入到 BeanB 的字段中，完成依赖注入。\n\n>在三级缓存这一块，主要记一下 Spring 是如何支持循环依赖的即可，也就是如果发生循环依赖的话，就去 三级缓存 singletonFactories 中拿到三级缓存中存储的 ObjectFactory 并调用它的 getObject() 方法来获取这个循环依赖对象的前期暴露对象（虽然还没初始化完成，但是可以拿到该对象在堆中的存储地址了），并且将这个前期暴露对象放到二级缓存中，这样在循环依赖时，就不会重复初始化了！\n\n## Bean的生命周期\n\n大的可以分为四个部分：\n1. bean创建阶段：通过调用反射api完成bean的实例化\n2. 依赖注入：为bean设置相关属性和依赖\n3. 初始化bean：首先检查是否实现了Aware包下面的各种aware接口，例如beanname，beanclassloader，beanfactory，其次再检查是否实现了Postbeanprocess接口，执行postbeanbeforeinitialization，再看有没有实现bean初始化方法InitializationBean和xml的init-method，最后再用postBeanAfterInitialization\n4. 销毁，disposableBean和destory-bean\n\n\n<div style=\"text-align: center;\">\n  <img src=\"../img/Spring/img.png\" alt=\"\" />\n</div>\n\n## 依赖注入的三种方式\n- 字段注入：上面加@Autowired\n- 构造器注入：官方推荐\n- setter注入：通过setter方法注入\n\n## Bean会有线程安全问题吗\n首先来看看bean的作用域，常见的有：\n- singleton：默认的bean是单例的\n- prototype：每次获取都是不一样的bean\n- request：一次http请求就一个bean\n\n如果作用范围是prototype，那不会有线程安全问题，一次获取就有一个bean。但是如果是单例的bean会有线程安全问题，可以通过以下几个方式解决：\n- 尽量使用无状态bean\n- 使用ThreadLocal\n- 加锁synchronized\n\n## SpringMVC执行流程\n1. 客户端发送http请求，首先由DispatcherServlet接收\n2. DispatcherServlet去HandlerMapping里面找（这是一个哈希表，存储了请求路径和controller的映射关系）\n3. 找到了对应的Controller，先去通过适配器HandlerAdapter，再去请求Controller\n4. 调用完成后返回一个视图和模型ModelAndView。\n5. DispatcherServlet找视图解析器ViewResolver进行解析。\n6. 解析得到的Model结合View进行渲染返回请求者\n\n<div style=\"text-align: center;\">\n  <img src=\"../img/Spring/img_1.png\" alt=\"\" />\n</div>\n\n## spring的事务传播逻辑\n> [详情看这篇文章](https://mp.weixin.qq.com/s?__biz=Mzg2OTA0Njk0OA==&mid=2247486668&idx=2&sn=0381e8c836442f46bdc5367170234abb&chksm=cea24307f9d5ca11c96943b3ccfa1fc70dc97dd87d9c540388581f8fe6d805ff548dff5f6b5b&token=1776990505&lang=zh_CN#rd)\n1. TransactionDefinition.PROPAGATION_REQUIRED：外部方法没有开启事务，那就自己开启事务，且开启的事务之间相互独立不打扰；如果外部方法有事务，那就与外部同生共死，如果外部事务回滚自己也回滚。外部事务中的一个子事务回滚那所有都回滚。且如果内部事务异常被捕获，外部事物还是会回滚\n2. TransactionDefinition.PROPAGATION_REQUIRES_NEW：外部事务回滚，内部的new不回滚。内部事务回滚，如果被catch了就不会造成外部事务回滚，如果没有catch就会回滚。（上一个是有无catch都会造成外部事物回滚）\n3. TransactionDefinition.PROPAGATION_NESTED：与上一个是反的，外部事务回滚会造成内部回滚，自身回滚也是看catch，如果catch了外部事物就不会滚\n\n\n## @Transactional注解使用\n可以使用在方法和类上，使用在类上就是给所有方法都加上事务。常用参数有5个，事务隔离级别（读未提交，读提交，可重复读，串行化），传播行为，超时时间（在指定的时间内未完成回滚），是否只读，触发回滚的异常类型\n\n\n## spring自调用导致失效问题\n由于Transaction这个注释用到了aop，进行事务的对象都是代理对象。如果是自己调用就无法拦截到这个内部调用，所以事务会失效。这个时候可以获取其代理对象。\n```java\n@Service\npublic class MyService {\n\nprivate void method1() {\n     ((MyService)AopContext.currentProxy()).method2(); // 先获取该类的代理对象，然后通过代理对象调用method2。\n     //......\n}\n@Transactional\n public void method2() {\n     //......\n  }\n}\n\n```","tags":["Java"]},{"title":"Java基础篇","url":"/2024/09/14/Java基础篇/","content":"## Java的异常体系\n分为异常Exception和错误Error，异常是可以用try/catch来解决的，但是error就很严重了，会直接导致线程终止。他们都继承自ThrowAble接口。\n\n其中exception分为可以检查到的异常（checked exception），表示能够在编译阶段就发现并且抛出错误的异常，比如sqlexception或者ioexception；编译时无法检测出的异常（unchecked exception），指的就是运行时异常（runtimeException）例如数组越界，空指针这种。 runtimeException都是无法检测出的异常，其余的都是可以在编译阶段检测出来的。\n\nerror是比较严重的错误，例如oom或者stackoverflow，还有虚拟机内部的一些问题，这些error会导致线程直接终止，~~并不能用try/catch~~\n\n> gpt了一下，其实error也可以用catch，因为都是继承自throwable接口，throwable满足这个语法糖。但是出现了error就算catch了程序还是出现了严重错误，捕获这些问题并不能修复，继续运行可能导致更加严重的问题。\n\n## finally方法块\n- finally并不一定会执行：如果出现了电源掉电或者cpu卡死就不会（~~这有点幽默了~~）\n- 不建议在finally里面写返回：try/catch/finally方法块的逻辑是，即使在try中已经返回了，也会执行finally里面的语句，try的返回值保存起来，同时如果finally里面也有return，就会覆盖try的返回值。\n\n## 序列化和反序列化\nJava数据的传输是通过字节流的，也就是一个byte[]数组，如何将Java转化为这个数组就是序列化要干的事情，原生提供的序列化可以提供实现serialization接口并且指定serialVersionUID完成序列化，这个serialVersionUID会为反序列化提供一个校验，如果id与当前要强制类型转换的类是一样的，那就可以转换，否则就会抛出异常。\n\n比如下面这个代码，文件模拟的是传输的数据流。\n\n```java\nimport java.io.*;\n\nclass Person implements Serializable {\n    private static final long serialVersionUID = 1L; // 显式定义版本号\n    String name;\n    int age;\n\n    public Person(String name, int age) {\n        this.name = name;\n        this.age = age;\n    }\n}\n\npublic class Main {\n    public static void main(String[] args) {\n        try {\n            // 序列化\n            ObjectOutputStream out = new ObjectOutputStream(new FileOutputStream(\"person.ser\"));\n            Person person = new Person(\"Alice\", 30);\n            out.writeObject(person);\n            out.close();\n            \n            // 模拟反序列化后类发生变化\n            // 修改 serialVersionUID 为 2L，模拟版本不匹配\n            // private static final long serialVersionUID = 2L;\n\n            // 反序列化\n            ObjectInputStream in = new ObjectInputStream(new FileInputStream(\"person.ser\"));\n            Person deserializedPerson = (Person) in.readObject();  // 读取对象\n            in.close();\n\n            System.out.println(\"Deserialized Person: \" + deserializedPerson.name + \", \" + deserializedPerson.age);\n        } catch (Exception e) {\n            e.printStackTrace();  // 如果 serialVersionUID 不匹配，这里会抛出 InvalidClassException\n        }\n    }\n}\n\n```\n> 注意几点：\n> 1. 序列化转化的是对象，也就是说static修饰的字段是不会被序列化的，反正最终反序列化变成了一个对象之后类的静态属性又不会有变化，\n> 2. 不想序列化的变量用transient修饰，例如下面这个代码\n\n```java\nimport java.io.*;\n\nclass Example implements Serializable {\n    private static final long serialVersionUID = 1L;\n    \n    static int staticField = 10; // static 字段\n    transient int transientField = 20; // transient 字段\n    int instanceField = 30; // 实例字段\n    \n    public static void main(String[] args) {\n        Example example = new Example();\n        \n        try {\n            // 序列化对象\n            ObjectOutputStream out = new ObjectOutputStream(new FileOutputStream(\"example.ser\"));\n            out.writeObject(example);\n            out.close();\n            \n            // 修改 staticField 值以查看序列化和反序列化的影响\n            Example.staticField = 100;\n            \n            // 反序列化对象\n            ObjectInputStream in = new ObjectInputStream(new FileInputStream(\"example.ser\"));\n            Example deserializedExample = (Example) in.readObject();\n            in.close();\n            \n            // 打印结果\n            System.out.println(\"Deserialized Example:\");\n            System.out.println(\"Static Field: \" + Example.staticField); // staticField 仍然是 100\n            System.out.println(\"Instance Field: \" + deserializedExample.instanceField); // instanceField 是 30\n            System.out.println(\"Transient Field: \" + deserializedExample.transientField); // transientField 是 0\n        } catch (Exception e) {\n            e.printStackTrace();\n        }\n    }\n}\n\n```\n\n\n但是Java默认的这个序列化并不好，至少我在开发的时候都用的是Json，为什么呢？\n1. 不支持跨平台，java序列化得到的字节流只能java反序列化出来，json不一样，java转化为的json可以用python再反序列化回来\n2. 性能较差，转化后的字节数组大\n3. 安全性\n\n## String,StringBuffer,StringBuilder\n- string是被final修饰的，同时内部的实现数组（char[]或者byte[]）是private，无法通过外界修改。因此字符串的拼接是需要新生成对象的。\n- stringbuffer：方法由synchronized修饰，是线程安全的，其拼接字符串不会创建一个新的stringbuffer类，效率比string不知道高到哪里去了\n- stringbuilder：在大部分情况下string的操作都不太需要线程同步，这个类就取消了同步操作，效率比上一个高。\n\n**字符串的拼接**\n\njava是没有运算符重载的，但是唯一为了string的拼接修改了+和+=，使得string可以通过+和+=操作\n\n查看字节码我们可以知道，其实string的+和+=是基于stringbuilder调用append方法，最后再使用tostring赋值给String，但是每次进行这样一个拼接操作就需要new一个stringbuilder，对于循环来说，这样的开销就非常大了。\n\n```java\nString[] arr = {\"he\", \"llo\", \"world\"};\nString s = \"\";\nfor (int i = 0; i < arr.length; i++) {\n    s += arr[i];\n}\nSystem.out.println(s);\n```\n\n如果直接使用append来拼接，就不会有这个问题了。所以在大量字符串操作的时候用buffer或者builder比较好。\n\n## 字符串常量池和String对象\n**字符串常量池**\n首先做个区分，java程序在编译成了class文件之后会有一个常量池，也叫做静态常量池。当在被类加载器加载进内存时会将其分为字符串常量池和运行时常量池。\n- 运行时常量池是一直跟着方法区的，方法区在哪里它在哪里。在jdk1.7的时候方法区叫做永久代，放在运行时数据区，也就是和堆栈在一起的区域，为了减少oom的可能和内存的压力，在jdk1.8以后被搬到了直接内存中，由操作系统管理不由jvm虚拟机管理，从而减少了jvm的内存压力。但是虚拟机参数可以配置元空间的大小，并不是可以无限扩大的，也会有oom：metaspace的报错。说回运行时常量池，包含了class文件里静态常量池里的字面量和符号引用，但唯独没有字符串。**也就是说除字符串以外的字面量比如整形和浮点，符号引用的类与接口全限定名，方法和字段的名称和描述符，都还在运行时常量池；维度字符串排除出去了**\n- 字符串常量池：在jdk1.6的时候是没有与运行时常量池分开来的，也是跟着方法区的，到了1.7被分到堆内存去了。原因是本来方法区进行垃圾回收的机会就很少，除了fullgc基本到不了这里，字符串又是一个很容易产生大量对象的地方。所以就放到heap中了，可以进行垃圾回收，这里创建出的也被称之为字符串对象。实际上String的引用都是在字符串常量池里的地址。\n\n**new String(\"abc\")的问题**\n\n```java\nString s1 = new String(\"abc\");\n```\n我们假设前面都没有关于abc这个字符串的创建，这个操作会创建两个对象：\n- 首先会在字符串常量池上创建abc这个对象\n- 由于new了一个String对象，会在堆上创建一个新的对象\n\n但是如果前面有String s2=\"abc\"，也就是前面已经有在字符串常量池里面创建对象了，前面这个new的代码只会创建一个对象，即堆内存上的。\n\n```java\nString s1 = \"abc\";\nString s2 = \"abc\";\n\nSystem.out.println(s1 == s2);  // true，因为 s1 和 s2 都引用常量池中的同一个对象\n```\n\n\n```java\nString s1 = \"abc\";              // 常量池中的 \"abc\" 对象\nString s2 = new String(\"abc\");  // 堆中的新对象\n\nSystem.out.println(s1 == s2);   // false，s1 和 s2 是不同的对象，s1 是常量池中的，s2 是堆中的\n\n```\n> 那我在new了一个String以后，我怎么找到字符串常量池里面的对象？\n> intern可以直接找，返回的是在字符串常量池里的地址，这样就保证了字符串内容是一致的情况下，地址也完全一样，在某些要用锁的场合下，使用intern可以保证锁的对象是唯一的。\n> 例如在黑马点评这个锁代码块里面：\n```java\n@Transactional\npublic  Result createVoucherOrder(Long voucherId) {\n\tLong userId = UserHolder.getUser().getId();\n\tsynchronized(userId.toString().intern()){\n        ...\n    }\n}\n```\n> 这里面每一个id的toString都会创建一个新的对象，为了保证锁对象的唯一性，需要用intern直接取出常量池的地址\n\n为什么要有这种设计？字符串常量池本身就是为了可重复才扩展的，有的时候不想要与字符串的常量池共享，就需要创建一个新的字符串。\n\n## Double和BigDecimal的区别\n- double是基本数据类型，bigDecimal是类\n- double双精度浮点型可能会有数据溢出的情况，bigdecimal是用字符串做运算的，精度有保障，由于是字符串，会频繁创建对象，开销较大\n- 精度要求特别高比如在金融领域需要用big\n\n## 反射\n\n类加载器加载字节码文件分为三部分：通过类的全限定名获取字节码文件，将字节码文件生成一个InstanceKlass文件在方法区，保存了类的元数据包括字段，方法，虚方法表登；同时会在堆中生成一个Class对象，作为访问这个InstanceKlass对象的接口。 正是这个Class对象为Java反射提供了基础。\n\n获取Class对象有以下四种方式：\n1. Class.forName传入路径\n2. 通过对象获取class（instance.getClass()）\n3. 通过类加载器传入loadClass路径获取（走的就是双亲委派机制那一套了，估计findClass是根据路径改造成全限定名然后找，但是这里仅仅是加载类，没有连接过程，所有静态代码块不会执行）\n4. 可以直接.class获取\n\n```java\nimport java.lang.reflect.Constructor;\nimport java.lang.reflect.Field;\nimport java.lang.reflect.Method;\n\npublic class ReflectionExample {\n    public static void main(String[] args) {\n        try {\n            // 1. 获取Class对象\n            Class<?> clazz = Class.forName(\"com.example.Person\");\n            \n            // 2. 获取构造器并创建对象\n            Constructor<?> constructor = clazz.getConstructor(String.class, int.class);\n            Object person = constructor.newInstance(\"张三\", 25);\n            \n            // 3. 获取并调用方法\n            Method sayHello = clazz.getMethod(\"sayHello\");\n            sayHello.invoke(person);\n            \n            // 4. 获取并修改字段\n            Field ageField = clazz.getDeclaredField(\"age\");\n            ageField.setAccessible(true); // 如果字段是private的，需要设置可访问\n            int age = (int) ageField.get(person);\n            System.out.println(\"年龄: \" + age);\n            ageField.set(person, 30);\n            System.out.println(\"修改后的年龄: \" + ageField.get(person));\n            \n        } catch (Exception e) {\n            e.printStackTrace();\n        }\n    }\n}\n\n// 假设存在如下Person类\npackage com.example;\n\npublic class Person {\n    private String name;\n    private int age;\n    \n    public Person(String name, int age) {\n        this.name = name;\n        this.age = age;\n    }\n    \n    public void sayHello() {\n        System.out.println(\"你好，我是\" + name + \"，今年\" + age + \"岁。\");\n    }\n}\n\n```\n> 如何获得私有变量/方法：setAccessible\n\n**反射的优缺点**\n\n- 优点：灵活度高，多种框架都是基于反射的，能够在运行时动态操作类和对象\n- 缺点：安全性，private字段可以访问，性能开销\n\n> 总结：为什么反射的效率低？ \n> 反射效率低的原因可以归结为以下几点：\n> - 需要进行动态的类型解析，耗费额外时间。 \n> - 反射的调用绕过了编译时的优化。 \n> - 反射需要执行额外的安全性检查。 \n> - 方法调用的间接性导致了额外的步骤和开销。 \n> - 缺乏JIT对反射的优化支持。 \n> - 频繁使用时，开销会明显累积。 \n> - 反射可能会引发对象包装和拆箱操作，进一步降低效率。\n\n## switch能否用String当作case\n在jdk1.7以后有了这个语法糖。在底层原理中switch仅支持int，char等基本数据类型。switch中string是通过case哈希值，然后在equals内容得到的。哈希值是一个int类型的，由于也可能产生哈希碰撞，所以还得再检查字符串内容是否相同。\n\n## 泛型\n虚拟机中没有泛型，只有普通类和普通方法，所有泛型类的类型参数在编译时都会被擦除，泛型类并没有自己独有的Class类对象。比如并不存在List<String>.class或是List<Integer>.class，而只有List.class。\n\n也就是说泛型仅仅是在编译的时候做检查，而不会真正落到jvm里\n\n## Object有哪些方法\n- wait和notify以及notifyAll：wait方法是放弃当前监视器锁，进入waitSet，状态变为WAIT。notify和notifyAll是唤醒当前监视器锁下面wait的线程，在hotspot中是分别是随机唤醒一个和全部唤醒。这个方法仅能在synchronized方法块中使用\n- hashCode和equals：下面讲\n- getClass：获取Class对象，与类.class方法是一样的，一共有四种方法获取Class对象（getClass，.class，类加载器loadClass,Class.forName）\n\n## hashCode和equals\n说这两个就离不开集合，例如hashMap存储是按照hashCode散列找Entry，如果hashCode相同则在一个Entry下，但是有可能是哈希碰撞，所以还要比较equals，当且仅当hashCode相等并且equals返回为真的时候才会被认为是一个。在Object中默认hashCode是两个对象的内存散列，equals是比较内存地址是否相同。\n\n**String重写的hashCode和equals**\n\n```java\nString s = \"abc\";\nString s1 = new String(\"abc\");\nSystem.out.println(s.hashCode()+\"  \"+s1.hashCode());\nSystem.out.println(s.equals(s1));\n```\n\n按道理来说s和s1是两个不同的对象，由于之前说的s应该是在字符串常量池，而s1是在堆中，hashCode按照Object默认的比较地址肯定是不相同的。但是String重写了hashCode，通过字符累加的方式计算哈希码，在保证了分散的同时使得相同字符串的哈希码相等。也重写了equals，比较字符串内容而不是比较地址。\n\n**为什么重写了equals就必须重写hashCode**\n\n如果两个对象内容相同就是一个对象的话，在set里面不能重复。需要重写equals，但是不重写hashCode可能会分配到不同的Entry，这样两个对象同时会被存储。所以保证equals为真的同时hashCode也要相同。\n\n","tags":["Java"]},{"title":"多线程总结","url":"/2024/09/10/多线程总结/","content":"## 进程和线程的区别\n1. 资源分配的角度来看：进程是资源分配的最小单位，线程的切换仅仅保存了较少的寄存器，线程不是资源分配的最小单位\n2. 调度的角度：没有引入线程的时候，进程是调度的最小单位，有了线程之后操作系统调度的就是线程了。\n3. 切换开销来看：线程的切换开销较小，只用保存较少的寄存器，进程切换的开销较大\n4. 并发的角度来看：同一个进程的线程可以并发，不同进程的线程之间也可以并发\n\n## Java的线程和操作系统的线程有什么区别\n在jdk1.2以前，java的线程是用户级线程，也就是一对多模型，向下操作系统申请一个线程，这个线程来轮询java的操作，在用户层面看上去是有多线程，但这样效率并没有提升\n\n在此之后java的线程可以直接用内核级线程，也就是java创建的线程就是操作系统认可的一个线程。\n\n## 创建线程的方法\n网上都说有四五种方法，但是其实只用一种就是thread.start()方法，但是为了完整性还是说一下这四种方法：\n1. 继承Thread\n2. 实现Runnable接口:run方法不允许有返回值，也不能抛出异常\n3. 实现Callable接口：call可以有返回值，可以抛出异常\n4. 线程池\n\n> 什么时候实现runnable接口什么时候继承thread：由于java是单继承的，如果有继承别的类就不能继承Thread了，此时就要使用Runnable接口\n> runnable和callable只是两个接口而已，直接运行对应的run和call是不能成为一个线程的，start是Thread类才有的方法，所以要真正运行起来线程需要new Thread(),这里面传入的是一个Runnable参数，Callable不行，需要用futureTask包装\n \n\n> callable怎么用？\n> 1. callable和futureTask:由于new Thread要传一个Runnable参数，Callable不是；所以要用futureTask包装\n> 2. callable和线程池：submit给线程池返回一个future对象\n\n## Future和FutureTask\nfuture是异步编程的一个体现，当前线程正在执行，我需要取到程序运行结束的结果，也就是未来的结果。具有取消任务，查看任务执行情况，获取任务执行结果等功能。而futureTask是其的一个实现类，同时实现了future和runnable接口，由于new Thread只能传Runnable接口，futureTask正式为了搭配Callable接口，使用的时候new Thread(new FutureTask(new Callable))\n\n## 线程池的submit和execute什么区别\n1. submit返回值为Future，可以执行Callable和Runnable任务，但是如果是runnable那future.get返回就是null，也可以抛出异常\n2. execute返回值为空，只能执行Runnable任务\n\n## 线程的状态\n类似原来408学的线程状态，但是在java中这些状态略有不同，相比于底层操作系统，这些状态更加高层次，因为不涉及到io操作阻塞进程：\n- NEW:初始状态，一般是new Thread()，创建出来但是还没有执行start方法\n- RUNNABLE:运行状态，执行了start方法\n- BLOCKED:被synchronized方法阻塞了，没有抢到锁就进入这个状态，等到锁释放就变为RUNNABLE\n- WAITING:一般是线程之间通讯，调用了wait方法变成waiting状态，而等到notify或者notifyAll变为运行态\n- TIME_WAITING:具有具体时间的等待状态\n- TERMINATED:线程终止\n\n**BLOCKED和WAITING的区别**\n- 触发条件：线程没有获取到监视器锁（synchronized）就BLOCKING，WAITING是获取到了synchronized锁但是锁对象wait了，也就是释放了锁，主要用于线程通讯\n- 唤醒条件：获取到锁了之后会解除BLOCKING，而别的线程对锁对象.notify或者notifyAll之后会唤醒waiting线程，但是要继续执行还是要获取锁\n\n**notify和notifyAll的区别**\n- notify随机唤醒一个（这个是hotspot的机制，可能别的虚拟机是指定的）\n- notifyAll，当前锁对象下所有的WAITING线程，共同争抢。唤醒的线程在被 notify() 或 notifyAll() 唤醒后，仍然需要重新争抢对象的锁。它会从 WAITING 状态转换到 BLOCKED 状态，等待对象的锁释放。\n\n**wait和sleep方法的区别**\n- wait是Object的方法，sleep是Thread的方法\n- wait释放锁，sleep不释放锁\n- wait需要notify唤醒而sleep到时间就唤醒了\n- wait主要用于进程同步，sleep延时\n\n> 为什么wait定义在object里面而sleep定义在Thread里面：\n> 与他们使用的场合有关，wait需要在synchronized方法中运用，而代码块的锁正是object，操作的并不是线程\n> 而sleep的作用是让线程睡眠一段时间，这显然跟锁没什么关系，就定义在线程中\n\n## 死锁的四个条件，通过什么方法解决\n- 互斥条件：一个资源同时刻只能有一个线程访问\n- 不剥夺条件：一个线程的资源不可以被别的线程剥夺\n- 占有且等待条件：一个线程获取不到更多的资源，原来持有的资源也不会释放\n- 循环等待条件：资源的请求变成了一个环，大家都拿不到对应的资源\n\n> 破坏这四个条件是**死锁预防**要做的事情。此外还有死锁避免（银行家算法），死锁检测和恢复（有向图检测方法）。这三个大的方法性能由低到高，对死锁的控制由严格到松。\n\n- 破坏互斥条件：多增加临界资源的数量（虚拟化）\n- 破坏不剥夺条件：抢占式调度（还有一种是协同式调度，是等到当前线程结束才进行线程调度），得不到资源就释放\n- 破坏占有等待条件：可以先申请，检查当前系统剩余资源是否能够让线程运行，一次性全拿而不是一次拿一点\n- 循环等待条件：编号，类似TCP的同步号\n\n## volatile关键字\n1. 在多线程环境中，每个线程都有自己的 CPU 缓存，线程在读写变量时可能会先将变量的值缓存在自己的工作内存中，而不是直接从主内存中读取或写入。使用volatile关键字就告诉程序这个变量是不稳定的，禁用cpu缓存，每次找这个变量都需要去主存取，\n2. 在jvm的执行优化中可能出现指令重排序，也就是123条指令，可能实际变成132，volatile关键字能够在其中加上内存屏障，禁止指令重排序（主要用在双校验单例模式）\n```java\npublic class Singleton{\n    private static volatile Singleton singleton;\n    public Singleton(){}\n    public Singleton getSingleton(){\n        if (singleton == null){\n            synchronized (this){\n                if (singleton == null){\n                    singleton = new Singleton();\n                }\n            }\n        }\n        return singleton;\n    }\n}\n```\n3. 能保证可见性，但不能保证原子性，例如再出现i++这种操作的时候，即使volatile i，还是会有线程安全问题。因为i++其实是三个操作，从内存取出i，对这个副本+1，再把其写回内存。此时两个线程来操作就会少一次+1。（可以加锁synchronized,reentrantlock或者用AtomicInteger的cas操作）\n\n> 内存屏障：\n> - 写 volatile 变量时： 当一个线程写入 volatile 变量时，JVM 会在写操作后插入一个写内存屏障（Store Memory Barrier），确保所有在 volatile 变量写操作之前的内存操作都已经完成，并且这些操作的结果对其他线程可见。 在 Java 内存模型中，写 volatile 变量时会确保该写入操作之前的所有变量修改都被刷新到主内存中，然后将 volatile 变量的值写入主内存。 \n> - 读 volatile 变量时：当一个线程读取 volatile 变量时，JVM 会在读操作前插入一个读内存屏障（Load Memory Barrier），确保所有在 volatile 变量读操作之后的内存操作都不会被重排序到它之前。 在 Java 内存模型中，读 volatile 变量时，会确保在读取这个 volatile 变量之后，线程能够看到所有其他线程在写入该 volatile 变量之前对其他变量的修改。\n\n## synchronized\n1. 可以修饰实例方法，锁住当前对象\n2. 修饰静态方法，锁住当前类。这里就会有个有意思的事情，修饰在静态方法上的synchronized和实例方法上的**不互斥**，因为静态方法锁住的是当前类，实例方法锁住的是对象\n3. 修饰代码块，这个比较灵活\n> 注意这里不能修饰构造函数，构造函数本身就是线程安全的\n\n**原理**\n基于jvm的monitor，在代码块进入前加上monitorenter，退出的时候加上monitorexit，也是可重入的，底层是监视器维护了一个state字段，有锁进来就+1，重入多次就+n，退出-1，同时也会有维护一个队列存放未获取锁的对象，等到释放了就会唤醒（BLOCKED）\n\n两个队列entryList和waitList:\n- entryList是进入锁代码块的所有线程，当锁释放了之后其中的一个线程会给monitor上锁，同时+1（队列里的状态为BLOCKED），注意这里是队列里所有线程都进行竞争，所以synchronized是非公平锁\n- waitList是调用了wait方法的线程，等待notify或者notifyAll进行唤醒，唤醒之后还需要重新获得锁，也就是要进入entryList里（状态为WAITING）\n\n![](../img/Java/img_9.png)\n\n**锁升级**\n- 不加锁：\n- 偏向锁：当第一个线程进入之后会给其加上偏向锁，使得这个资源偏向。\n- 轻度锁：使用cas操作\n- 重度锁：\n\n> 为什么会有偏向锁这个过程：\n> 对于竞争不激烈但是还有加了synchronized关键字的，如果是单线程就没有必要进行锁和上下文切换，偏向锁的初衷正是为了减小上下文切换的开销。如果一个线程反复进入和退出同步块，而没有其他线程争用这把锁，那么这些加锁解锁的开销是完全多余的。\n\n## AQS\n抽象队列同步器（AbstractQueuedSynchronizer）提供了一种同步机制，被应用于Reentrantlock，semaphore，cyclicBarrier等排他和共享锁结构中。\n\n在说AQS之前先要讨论一下CHL锁，因为AQS是基于CHL的。CHL锁是一个队列结构，头节点是一个空节点，当线程进入后会对前驱节点的资源释放状态做cas，例如第一个线程进入之后就对表头节点的锁资源cas，获取到了就改变自己的值为false，后续另外一个线程进入之后就会轮询上一个节点的状态，拉成一个队列。\n\n这种结构的好处是资源状态分布在每个节点上，但同时也增大了cpu的开销，因为每一个节点都需要轮询。\n\nAQS正式基于这种思想，首先他把CHL的虚拟队列改造成了显式队列，保存了前驱和后继（为了满足一些对于公平锁和非公平锁的要求，使用了双向链表），全局维护一个state，当获取到锁之后state+1，可以实现锁重入。\n\n## reentrantLock\n\nreentrantLock基于aqs，实现可重入，可中断，可以设置公平锁，可以有多个条件设置，可以设置超时时间。\n\n","tags":["Java"]},{"title":"集合总结","url":"/2024/09/10/集合总结/","content":"## 数组与集合区别，用过哪些？\n1. 数组是定长的，集合不是定长的，例如ArrayList是动态数组\n2. 数组存储的是基本数据类型和对象，而集合只能存储对象（这里也可以引申出为什么要有包装类，为了统一集合的操作需要把基本数据类型包装成对象，int变成Integer，但是这样也会有自动拆箱导致的性能问题）\n3. 数组可以随机访问，集合需要通过迭代器或者其他方法访问（其实还是由于包装了）\n\n用过哪些：ArrayList,LinkedList,Vector,HashSet,TreeSet,BlockingQueue,HashMap.TreeMap,LinkedHashMap\n## 说说Java中的集合？\nJava的集合类分为两个大类，Collection接口实现的List，Set和Queue，以及Map接口。Collection存储的是**单一元素**，而Map存储的是**键值对**。\n\n在Collection接口下：\n1. List是顺序的，主要实现类是ArrayList，底层基于动态数组，LinkedList，底层基于双向链表，Vector，是加了重度锁的有线程安全的顺序数据结构，由于性能一般已经很少用了\n2. Set是集合，存储的元素不能重复：HashSet基于HashMap（有待考究），TreeSet是基于红黑树，按照某种顺序有序的集合。\n3. QUeue是队列，了解较少\n\n在Map接口下：\n1. HashMap：jkd1.7基于Entry+链表的形式，在1.8之后改成Node+链表+红黑树。线程不安全\n2. TreeMap：基于红黑树，可以根据key的大小排序\n3. LinkedHashMap：基于HashMap，但是内部维护了一个双向链表来指定顺序，1.7之前是循环链表，1.8以后是双向链表\n## Java中的线程安全的集合是什么？\n在java.util包下的数据结构，也就是本身设计就是为了线程安全的：\n1. Vector：操作都用了synchronized方法，用copyOnWriteArrayList替代\n2. HashTable：也是加了重度锁的哈希表，性能不如concurrentHashMap\n\n在1.8以后有Concurrent的包出现，带来了很多新的线程安全的集合\n1. ConcurrentHashMap：1.7之前是分段锁，把每个Segment都锁上，默认可以实现16个线程并发（前提是他们访问的这些数据都分布在16个不同的Segment上）。1.8以后是用了Node+链表+红黑树，锁的粒度变小了，只用synchronized锁住头节点，然后如果要添加到话用cas操作，大大减轻了锁的粒度，性能较好\n2. CopyONWriteArrayList：写时复制COW，指的是在写操作的时候赋值一份副本在副本上进行操作，最后再用新数组替换旧的数组，如果期间原数组有操作，就记录下来最后进行变更\n## Collections和Collection的区别\nCollection是一个接口，如上面所述；Collections是一个工具类，提供了一系列静态方法来帮助完成集合的操作，比如sort\n\n> Collections还有一个方法可以让HashMap变成线程安全的集合，这个方法是给所有HashMap的方法加上synchronized关键字，这样效率是很低的，所以能用还是用ConcurrentHashMap吧\n\n## ArrayList和LinkedList的区别\n1. 底层数据结构不同：ArrayList基于的是动态数组自动扩容，LinkedList基于的是双向链表，都不是线程安全的\n2. 增删改查的时间复杂度不一样，ArrayList可以支持随机查询，但是增删都需要平均一半的时间o(n/2)，双向链表在头尾增删有比较高的效率，但是在中间添加也是有查询的时间复杂度（注意，这里ArrayList实现了RandomAccess接口，表示可以支持随机访问，但是这个接口是空的，仅仅作为一个标记）\n3. 空间占用，ArrayList是连续的数据结构，数据密度大，而LinkedList可以是离散的，由于要存储下一个数据的指针，需要额外的空间，数据密度不大\n\n## ArrayList的初始化\nArrayList有两种初始化方式（还有一种是输入集合的，考察的比较少）：\n1. 无参构造方法，初始化为一个默认的DEFAULTCAPACITY_EMPTY_ELEMENTDATA的空数组，并没有分配具体对象，到后面grow操作才会有对象\n2. 有参构造方法，如果输入的长度不为0，直接就new一个Object数组，如果输入的长度为0，初始化为另外一个EMPTY_ELEMENTDATA的空数组\n\n> 为什么要有两个这个空数组呢，是因为需要区分是否为有参构造函数，在后续的扩容操作中只有DEFAULTCAPACITY_EMPTY_ELEMENTDATA才会分配默认值。这个在后续扩容操作有用处\n\n## ArrayList的扩容机制\n> 注意区分数组长度elementData.length和数据量size\n\n1. 再添加一个元素之前，先检查下一个位置是否存在，进行检查，传入参数是当前数据量+1；检查的目的是为了确定数组长度能否支持下一个数据的添加\n2. 在检查前针对无参构造方法有一个当前需要的最小长度的计算，由于初始化的时候长度没有确定，这个时候需要的最短长度就是默认的10；其余的方法容量计算的结果就是传入参数直接输出（即数据量+1）\n3. 进行检查，如果这个需要的最短长度大于了数组长度，就需要扩容了\n4. 扩容：非0情况下是1.5倍的原长度，如果原长度为0，则新长度为刚刚计算得到的最小长度。\n\n> 引申问题1:new ArrayList()和new ArrayList(0)第一次扩容的长度是多少，由刚刚的分析可知，有参在最小长度计算得到的长度为0+1=1，无参为10，所以说第一次扩容分别为10和1\n> 引申问题2:new ArrayList(10)在添加一个元素的时候扩容了几次，没有扩容，因为在构造函数里面已经有分配空间，而且1<10，不会触发grow\n\n## ArrayList是线程安全的吗？如果想用线程安全的List怎么操作\nArrayList不是线程安全的，主要是因为在加入数据的时候++操作不是原子性的，首先先说明加入数据的逻辑，总结为如下：\n1. 先进行上面所述的校验和扩容\n2. 再在当前size的位置添加元素（引发线程安全）\n3. 最后将当前的size+1\n\n有以下三种情况会引发线程安全问题：\n1. 出现null值，线程a添加2位置的元素，此时线程b也同时进来想要添加2位置的元素，线程a添加完成后没有立刻size++，线程b也存到当前位置，最后两个都size++，此时size为4，2的位置由于重复冲掉了一个数据，3的位置直接就是null。\n2. 出现溢出：当前长度为9，容量是10，两个线程同时在9的时候添加，就会出现溢出\n3. size和put的次数不一样：这个出现的最多，也是因为size++\n\n如何使用线程安全的list：\n1. Collections.synchronizedList(arrayList)，相当于在方法上全部加上synchronized，效率低\n2. CopyOnWriteArrayList：写操作用reentrantlock来保证不会出现**上述1和3**的线程安全问题，临界区内执行复制，不会影响读操作，存入完成后进行地址替换。虽然这种方法确实是线程安全的，但是每次写操作都要进行上锁和复制数组，在写多读少的情况下肯定是不好的。只适用于读多写少\n3. Vector：效率更低，已经淘汰\n\n## LinkedList的机制\n双向链表，查询数据的复杂度为o(n)，在首尾添加数据的复杂度为o(1)\n\n## HashMap的数据结构\nHashMap的结构经历了大的改版，在jkd1.7HashMap是基于Entry数组的，哈希冲突用拉链法解决，就形成了Entry数组+链表的结构；在jkd1.8以后，拉链超过一定的数量转化成查询效率更高的红黑树，形成了Entry数组+链表+红黑树的结构\n\n解决哈希冲突的方法（复习）：\n1. 拉链法：把冲突的都还是放在原来的位置\n2. 开放选址法：线性探测法（顺序往后面找），二次探测（1，4，9这样找）\n3. 再散列法：把冲突的再进行散列\n4. 哈希桶扩容：减小碰撞的可能性\n\n## HashMap的put过程\n1. 首先会判断是否初始化，如果没有初始化（比如空参构造方法）就resize\n2. 按照hash方法计算哈希值，这个哈希值的计算是通过hashcode和高位的异或运算得出，融合了高位和低位信息的hash函数，效果会更好\n3. 与当前Entry数组进行取余操作，这里的取余是位运算，hash&(n-1)，hashmap规定n必须是2的整数倍，也就是说n-1为最高位为0，其余都是1的二进制数，与hash进行与运算就能得到余数，位运算比%的效率高\n4. 得到了应该存储的Entry号，判断这个Entry上有没有别的元素，如果没有就直接写上去（这里还要判断是不是一个key，如果是一个key的话需要替换值）\n5. 如果有别的元素，首先判断是不是红黑树的节点，如果是红黑树的节点就走红黑树的添加逻辑\n6. 如果不是红黑树，是链表，在查找的过程中还需要看key是否重复，如果是重复的还需要覆盖v，如果没有重复的key就添加到表尾（尾插法，这也是1.8与1.7的一个区别，1.7的尾插会出现一些并发安全问题）\n7. 如果加入之后大于红黑树的阈值，还需要转换成红黑树\n8. 最后检查size，如果大了就需要resize\n\n![](../img/Java/img_6.png)\n\n## HashMap如何检查重复\n主要是靠这一段代码：\n```\np.hash == hash && ((k = p.key) == key || (key != null && key.equals(k)))\n```\n1. 首先判断两个的hash值，hash值相同说明碰撞了\n2. 再判断key的地址是不是一个，如果是基本数据类型那就是比较本身，如果是字符串或者是对象就比较地址\n3. 再比较内容，如果内容是一个那就是相同的，比如两个不同地址的字符串但是内容都是“abc”，再map这里也是一个\n\n## HashMap的resize机制\n1. 首先进行新数组的长度和阈值的计算，如果当前数组有长度和阈值，那就都乘2，如果有阈值但是没有长度（或长度为0，这种情况可能是有参构造函数造成的，在构造函数中只有这个阈值的初始化）就把要最小数组长度设置为阈值（此阈值不是那个乘负载因子的阈值），最后就是无参构造函数的，直接设置最小数组长度为默认的16\n\n```java\npublic HashMap(int initialCapacity, float loadFactor) {\n     if (initialCapacity < 0)\n         throw new IllegalArgumentException(\"Illegal initial capacity: \" + initialCapacity);\n     if (initialCapacity > MAXIMUM_CAPACITY)\n         initialCapacity = MAXIMUM_CAPACITY;\n     if (loadFactor <= 0 || Float.isNaN(loadFactor))\n         throw new IllegalArgumentException(\"Illegal load factor: \" + loadFactor);\n     this.loadFactor = loadFactor;\n     // 初始容量暂时存放到 threshold ，在resize中再赋值给 newCap 进行table初始化\n     this.threshold = tableSizeFor(initialCapacity);\n }\n```\n\n2. 根据最小数组长度和阈值进行扩容，进行新的哈希值计算\n3. 链表上的元素根据e.hash&oldCap，得到高位是0或者1，如果是1就放在新加的数组后面：\n\n![](../img/Java/img_7.png)\n\n![](../img/Java/img_8.png)\n\n4. 红黑树也根据哈希值按照上述方法进行分裂，小于6个点的退化为链表’\n\n## 为什么HashMap要用2的整数倍\n1. 位运算的取余操作\n2. 扩容机制巧妙将0，1分开\n\n## HashMap怎么样才能不扩容\n只要满足大于扩容阈值就行了，如果预计有100个数据要进入hashmap，那就做一个（100/负载因子）+1这么大的数组，这样可以保证不会触发扩容，当然初始化的resize还是有的\n\n## HashMap的线程安全问题\n并不是安全的，主要会有以下几个问题：\n1. 在jkd1.7，由于是头插法，在数据迁移的过程中可能会出现死循环的问题，有两个数据A->B，线程1和2同时操作当前快照，线程1已经操作了这两个节点变为B->A，线程2的当前节点为A，next为B，此时线程2进行操作，插入A节点，把当前节点换为B，B的next也是A，这样就循环了\n2. 此外还是会有跟ArrayList那种出现索引位置重复，比如当前某个Entry上没有元素，两个hash冲突的就可能会覆盖\n3. 还有size与put操作次数不一致的问题\n\n什么方法解决？\n1. concurrentHashMap：一种基于多段锁的Map\n2. HashTable：直接锁整个表\n\n## concurrentHashMap和HashTable的区别\nconcurrentHashMap在jdk1.7以前是segment+hashEntry+链表构成，给每个segment上锁，默认支持16的并发（前提是每一个线程访问的数据都不在一个segment上）；在jdk1.8以后通过synchronized+cas+node完成了并发控制，直接在hashmap进行锁操作，用synchronized锁住头节点，cas锁住添加操作。HashTable是方法全加synchronized的哈希表，效果肯定是不如cas的\n\n> concurrentHashMap什么时候用synchronized什么时候用cas，当Entry是空节点的时候就用自旋锁，当不为空就不用\n## HashSet\nHashSet是基于HashMap的，由于HashMap的特性会替换重复的key，HashSet直接把当前元素作为key，这样就可以达到去重的目的\n\n# 未完待续。。。。。","tags":["Java"]},{"title":"jvm-类加载器和双亲委派机制","url":"/2024/08/16/jvm-类加载器和双亲委派机制/","content":"\n# 类加载器\n\n## 类加载过程\n首先回顾一下类加载的过程。类的生命周期分为以下七个阶段：加载->验证->准备->解析->初始化->使用->卸载。其中加载是生命周期的第一个阶段，类加载器用不同的方式加载jar包，可以从classpath，动态代理和网络获取要加载的类。\n\n类加载主要做了以下三件事：\n1. 用类的全限定名找到类的class文件，也就是字节流\n2. 将字节流文件转化为方法区中运行时数据结构InstanceKlass，保存常量池，字段，方法等内容\n3. 在堆中创建相应Class对象，映射到方法区的数据结构，这部分是允许程序员操纵的\n\n> 在 Java 的类加载阶段，`InstanceKlass` 和 `Class` 对象各自扮演了不同的角色。理解它们的作用有助于更好地理解 Java 类加载和运行时的内部机制。\n\n### 1. **`InstanceKlass`**\n\n`InstanceKlass` 是在某些 Java 虚拟机（JVM）实现中用于表示类的内部数据结构的一个术语（例如在 HotSpot JVM 中）。它是 JVM 内部的一个概念，主要用于存储和管理类的各种元数据。`InstanceKlass` 包含了类的实例字段、方法、常量池等信息，主要用于：\n\n- **表示和管理类的元数据**：包括类的字段、方法、常量池等。\n- **支持类的实例化和动态操作**：在 JVM 内部用于实现类实例化、反射等操作。\n- **管理类的加载和初始化**：确保类在加载、链接和初始化过程中能够正确地管理其结构和状态。\n\n`InstanceKlass` 是 JVM 内部对类的表示，主要用于实现 JVM 的各种内部操作，通常不会被应用程序直接访问或操作。\n\n### 2. **`Class` 对象**\n\n`Class` 对象是 Java 语言层面提供的，用于表示和操作 Java 类的一个公共 API。每个 Java 类在运行时都有一个 `Class` 对象，用于提供对该类的元数据的访问。`Class` 对象的作用包括：\n\n- **反射**：提供了用于访问类的信息和操作类的方法（如获取类的字段、方法、构造函数等）。\n- **类型检查**：在运行时进行类型检查和类型转换。\n- **动态实例化**：允许在运行时创建类的实例（如 `Class.forName` 和 `newInstance`）。\n\n### 类的加载阶段的角色和作用\n\n1. **加载（Loading）**：\n    - 在这个阶段，JVM 会根据类的全限定名找到类文件（`.class` 文件），并将其字节码读入内存。此时，`InstanceKlass` 对象可能在内部被创建来管理这个类的字节码和元数据。\n\n2. **链接（Linking）**：\n    - **验证（Verification）**：确保类文件的字节码符合 Java 虚拟机规范。\n    - **准备（Preparation）**：为类的静态变量分配内存并初始化。\n    - **解析（Resolution）**：将类中的符号引用解析为直接引用。\n\n3. **初始化（Initialization）**：\n    - 执行类的静态初始化代码（如静态块）。\n\n### `InstanceKlass` 和 `Class` 对象的关系\n\n- 在内部，`InstanceKlass` 是 JVM 实现中的一个结构，用于存储和管理类的内部数据和状态。\n- `Class` 对象是 Java 语言层面的一个公共 API，提供了对类信息的访问。\n\n**在类加载阶段，`InstanceKlass` 负责 JVM 内部的类数据结构管理和维护**，而 `Class` 对象则提供了给应用程序使用的接口来访问和操作这些数据。它们各自作用于不同的层面，但在类加载和运行时的操作中是相辅相成的。\n\n### 关系总结\n\n- **`InstanceKlass`**：JVM 内部用于表示和管理类的元数据，是实现细节。\n- **`Class` 对象**：Java API 提供的类元数据的公共接口，让应用程序能够通过反射等机制与类进行交互。\n\n**`InstanceKlass` 和 `Class` 对象** 在类加载过程中相互协作，使得类的内部管理与外部访问得以实现。\n\n> 补充知识点：反射\n> \n> 反射是通过操作Class文件，在类的运行过程中动态的获取一些信息，比如当前时刻的字段和方法。首先在某个类的加载阶段，会在jvm的方法区生成一个InstanceKlass文件，这是由c++实现的，同时也会在堆中生成一个Class对象，该对象有反射的功能。\n> 在需要使用反射的时候，利用Class.forName(全限定名)，可以在堆中取出该Class对象，完成反射调用。\n\n## 类加载器分类\n\n一般来说，自上而下可以分为四个类加载器：\n### 启动类加载器(BootStrapClassLoader)\n由jvm提供c++编写，对于java程序员不可见，当打印类加载器名为null的时候即为启动类加载器。一般用于加载java核心类（rt）\n### 扩展类加载器(ExtensionClassLoader)\n加载一些拓展jar包，比如applet等一系列非java核心功能，但是有时候有需要用到的官方包\n### 应用程序类加载器(ApplicationClassLoader)\n加载在classpath下用户编写的所有java类和jar包\n### 自定义类加载器\n用户为了实现某些特定功能编写的，首先说明在java中的ClassLoader继承结构，ClassLoader是一个抽象类，URLClassLoader实现了在对应路径下的二进制文件写入内存的操作。其中由几个重要方法：\n- `protected Class<?> findClass(String name)`：传入的参数为全限定名。默认实现是空方法，在URLClassLoader中将全限定名转换成了路径（.改成了/，后缀添上了.class），自定义类加载器需要重写该方法，自定义逻辑去实际查找并加载类的字节码。例如，从网络、数据库或非标准路径加载类。\n- `public Class<?> loadClass(String name)`：入口方法，遵循双亲委派机制。**双亲委派机制**：当要加载一个类的时候，如果当前类加载器发现已经加载过该类（调用findLoadedClass，这个方法会使用native方法完成判断是否加载），就直接返回该Class对象，如果没有加载就委派父加载器进行加载，直到启动类加载器。启动类加载器会自顶向下判断是否能够加载当前类，比如启动类加载器加载不了的就给扩展类加载器，如果全部都加载不了，则报错ClassNotFoundException\n```java\nprotected Class<?> loadClass(String name, boolean resolve) throws ClassNotFoundException {\n    synchronized (getClassLoadingLock(name)) {\n        // 检查类是否已经被加载\n        Class<?> c = findLoadedClass(name);\n        if (c == null) {\n            try {\n                if (parent != null) {\n                    // 委派给父类加载器\n                    c = parent.loadClass(name, false);\n                } else {\n                    // 使用引导类加载器加载\n                    c = findBootstrapClassOrNull(name);\n                }\n            } catch (ClassNotFoundException e) {\n                // 父加载器找不到类\n            }\n\n            if (c == null) {\n                // 如果父加载器也找不到类，则调用 findClass 方法加载\n                c = findClass(name);\n            }\n        }\n\n        if (resolve) {\n            resolveClass(c);\n        }\n\n        return c;\n    }\n}\n```\n- `protected final Class<?> defineClass()`：做一些类名的校验，然后调用虚拟机的native方法将字节码信息加载到虚拟机缓存中\n\n> defineClass() 与 loadClass() 和 findClass() 的关系:\n> \n> - loadClass()：loadClass() 是类加载的入口方法，负责按照双亲委派模型加载类。如果父类加载器无法加载该类，loadClass() 最后会调用当前类加载器的 findClass()。 \n> - findClass()：findClass() 是自定义类加载器用于查找类字节码的地方。一般自定义类加载器会重写 findClass()，在找到类的字节码后，通过调用 defineClass() 将其定义为 Class 对象。 \n> - defineClass()：defineClass() 是类加载过程中的核心步骤，负责将字节码转化为 JVM 识别的 Class 对象。通常是 findClass() 方法中的最后一步。\n> - 调用关系：loadClass双亲委派机制，用findClass实现文件到字节数组的转换，defineCLass完成字节数组到内存的转化\n\n**还有一个要注意的点：**\n\nloadClass()有一个resolve参数，这个参数默认为false，代表只进行加载阶段，不进行连接阶段。比如要加载的类中有静态代码块，只调用loadclass是不会初始化这个代码块的，原因是只有加载阶段，不会进行连接更不会初始化了。\n\n# 双亲委派机制\n1. 自底向上委托父类加载器完成加载\n2. 自顶向下加载类\n3. 好处：可以避免重复加载类；保证安全性\n\n## 打破双亲委派机制——Tomcat\n\n在Tomcat中，往往包含了很多的web应用，这些应用的全限定名很可能相同（例如启动了两个相同的服务）。对于双亲委派机制而言，每一个web应用都会委派父类加载器进行加载来保证当前classpath下的全限定名唯一性，比如demo1加载了com/yyf/service.class而demo2也有这个类，此时根据机制由于存在一个全限定名相同的类，demo2下的新类不会被加载，这样就会出现问题，因此我们要打破双亲委派机制。\n\n### Tomcat的类加载器\n\nTomcat定义了一系列类加载器：\n![](../img/jvm/img_2.png)\n\n- common：通用类，Tomcat和web应用都要使用的类\n- catalina:Tomcat要使用的类（注意：在tomcat中默认没有配置这个的路径，此时catalina类加载器即为common加载器，如果需要使用catalina加载器，需要在catalina.properties中进行路径的配置）\n- shared：web应用要使用的可以共享的类，比如spring和mybatis（注意：与上面的类似，tomcat默认没有配置，此时也为common类加载器）\n- 并行web应用类加载器：每个应用一个\n\n> 注：共享类加载器\n> 如果所有的web应用都是基于SSM的，可以配置共享类加载器实现只加载一次Spring容器，但是可能由于版本不同可能导致兼容问题，以及由于存在的大量反射会导致类污染，所以需要谨慎选择shared加载的类\n\n### 如何打破双亲委派机制\n有了前面web应用隔离的需求，tomcat通过为每一个web应用创建一个并行web类加载器，只有当全限定名和加载器都是同一个时，才会被认为是重复加载，\n![](../img/jvm/img_3.png)\n\n- 左边是双亲委派机制：当需要加载一些内部类的时候还是要走双亲委派机制到启动类加载器\n- 右边是打破双亲委派机制：比如com/yyf/myclass.class这个类，并行web应用类加载器就会直接自己加载，但是如果没有加载成功还是会委派父类加载器进行加载\n\n## 打破双亲委派机制？——spi机制\n\nspi机制是一种服务发现机制，分为服务提供者和调用者，spi机制规定了需要在classpath下的META-INF/service下以服务调用者的全限定名为文件名创建一个文件，内容为服务提供者的全限定名。再使用ServiceLoader加载服务调用者，就可以调用多个服务提供者。\n\n在jdbc中，可能有多个数据库源。java.sql.Driver需要能够调用多个数据库的驱动比如mysql和Oracle，Driver位于rt中需要用启动类加载器进行加载，而mysql驱动是第三方jar包，需要用应用程序类加载器。**但是在默认情况下，一个类及其依赖类由同一个类加载器加载。**由于spi的存在，mysql的实现类也会在Driver加载的时候通过启动类加载器进行加载，这显然是错误的。\n\n### 线程上下文类加载器\n问题重述，对于Driver这个类，是需要用启动类加载器进行加载，但由于spi让其发现了mysql的实现类，需要对其进行加载，此时需要打破双亲委派机制，**启动类加载器委派应用程序加载器加载mysql驱动**。\n\n其实在每个线程都会默认保存一个应用程序类加载器，一般来说子线程会继承父线程的应用程序类加载器。在spi问题中，我们只需要取出线程保存的应用程序类加载器实现mysql驱动的加载即可。\n\n同样的，在前文所述的tomcat的spring加载中，我们的代码会使用到spring的一系列注解，在spring容器初始化的过程中也会产生依赖。也就是说，本来是shared（也有可能是common）类加载器加载的spring类，但需要用到classpath下的自己写的应该由并行web应用处理的类。这与spi机制所产生的问题是一致的。\n\n此时也是通过线程上下文类加载器进行解决，只不过这里在开辟一个线程的时候重新set这个变量为当前的并行web应用类加载器，原理与上面的类似。\n![](../img/jvm/img_4.png)\n\n### 真的打破了双亲委派机制吗？\n\n其实没有，从宏观上看，这里的Driver还是启动类加载器进行加载，mysql还是应用程序类加载器进行加载，也没有重新写loadclass方法。但也确实是由启动类加载器委派应用程序类加载器进行了加载。只不过是角度不同的问题，从宏观上没有打破，微观上打破了。","tags":["Java"]},{"title":"jvm-class文件和类的加载过程","url":"/2024/08/14/jvm-class文件和类的加载过程/","content":"\n# class文件\n\njava所推出的口号是：一次编译，处处执行。这种跨平台性主要就是由屏蔽操作系统底层的jvm所实现的，java程序从编译到执行有几个阶段，首先是java程序被javac编译器编译成.class的字节码文件，jvm加载并运行这一个中间文件，再通过jvm自身的接近底层的方法，实现在window，macos和linux上的跨平台。\n\n我的个人理解是，java为了保证跨平台性牺牲了一部分效率，这部分牺牲主要体现在要多一个通用的字节码文件的编译，而不是直接就从java变成机器能够执行的汇编了。\n\n## class文件的结构\n\n字节码文件主要有几个部分构成：\n1. 魔数和大小版本号：“cafebabe”，大版本号码就是-44，比如jdk8的版本号是52\n2. 常量池：最核心的部分，主要是字面量和符号引用，注意：在class文件中的都是符号引用，可以想象成逻辑地址，而在类加载的**解析**过程中，才会变为物理地址的引用。\n3. 字段表：当前类的变量\n4. 方法表：当前类的方法\n5. 属性：很复杂的一个表示\n\n案例：\n````java\npublic class ClassFile {\n    public static final String J = \"2222222\";\n\n    private int k;\n\n    public int getK() {\n        return k;\n    }\n\n    public void setK(int k) throws Exception {\n\n        try {\n            this.k = k;\n        } catch (IllegalStateException e) {\n            e.printStackTrace();\n        } finally {\n        }\n    }\n\n    public static void main(String[] args) {\n\n    }\n}\n\n````\n\n### 魔数和大小版本号\n首先，改变一个文件的后缀名是不会改变其文件本身的编码结构的，jvm不能仅仅是校验后缀是不是.class来判断是否为一个字节码文件，所以在文件内容的开头有一个cafebabe的校验，只有以这个为开头的才是jvm所需要的字节码文件，当然也不仅仅是要满足这一点，在后续类加载过程的**校验过程**中会详细说明。\n其次大版本号码就是当前jdk版本+44，还有小版本，这一部分是为了jvm在校验的过程中判断当前的class是否能够兼容版本，一般当然是只能向下兼容。、\n\n### 常量池\n> 在常量池之前，需要补充一下各种名以及描述符\n> - 全限定名和非限定名：**Class文件中的类和接口，都是使用全限定名，又被称作Class的二进制名称**。例如“com/ikang/JVM/classfile”是这个类的全限定名，仅仅是把类全名中的“.”替换成了“/”而已，为了使连续的多个全限定名之间不产生混淆，在使用时最后一般会加入一个“;”表示全限定名结束。 \n**非限定名又被称作简单名称**，Class文件中的方法、字段、局部变量、形参名称，都是使用简单名称，没有类型和参数修饰，例如这个类中的getK()方法和k字段的简单名称分别是“getK”和“m”。\n非限定名不得包含ASCII字符. ; [ / ，此外方法名称除了特殊方法名称< init >和< clinit >方法之外，它们不能包含ASCII字符<或>，字段名称或接口方法名称可以是< init >或< clinit >，但是没有方法调用指令可以引用< clinit >，只有invokespecial指令可以引用< init >。\n> - 描述符：在class文件里的方法和基本数据类型也有特定的写法，一般来说基本数据类型都是首字母大写作为描述，比如int就为I，**其中long特殊，描述为J**，void为V，引用类型为L+类的全限定名，对于数组类型，每一维度将使用一个前置的“[”字符来描述，如一个定义为“java.lang.String[][]”类型的二维数组，将被记录为：“[[Ljava/lang/String；”，一个整型数组“int[]”将被记录为“[I”\n方法描述符的写法看一眼就会，比如Object m(int i, double d, Thread t) {… }则描述符为(IDLjava/lang/Thread;)Ljava/lang/Object;\n\n言归正传，常量池有两部分内容：字面量和符号引用。字面量就是一个变量的对应值，符号引用就是名称，主要有三部分名称：类与接口的全限定名，字段的名称和描述符（NameAndType），方法的名称和描述符（NameAndType）。\n每一个字段都有自己的结构，具体见[这篇博客](https://blog.csdn.net/weixin_43767015/article/details/105310047?ops_request_misc=&request_id=&biz_id=102&utm_term=class%E6%96%87%E4%BB%B6&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-0-105310047.142^v100^pc_search_result_base4&spm=1018.2226.3001.4187)。\n说几个比较重要的：\n1. CONSTANT_Utf8_info和CONSTANT_String_info：前者是真正存储字符串本体的，而后者的结构只包含引用utf8的索引位置，为什么这样设计？当多个String名字不同但是值相同的时候，就可以只创建一条CONSTANT_Utf8_info常量，多个String可以引用，从而节省空间；其次是有可能符号引用也是当前值，例如声明了String a = \"a\"，这样多少也能节约空间。\n2. CONSTANT_NameAndType_info：name就是字段或者方法的非限定名，type是其描述符，这个同样是引用类型，放的是CONSTANT_Utf8_info的索引。例如println:(Ljava/lang/String;)V\n3. CONSTANT_Fieldref_info和CONSTANT_Methodref_info：两个字段，前者是这个字段或者方法的名字引用，后者是CONSTANT_Methodref_info的引用\n    <div style=\"text-align: center;\">\n      <img src=\"../img/jvm/img.png\" alt=\"\" />\n    </div>\n4. CONSTANT_Class_info：类的全限定名，注意所有的类都会有java/lang/Object这个类的class常量，因为除了Object本身，其余都是继承自它。\n### 访问标志\n用2字节作为访问标志，其中16位的某些位置可能代表了某些修饰词，例如public，final，interface等\n### 当前类，父类和接口索引\n类索引用于确定这个类的全限定名，父类索引用于确定这个类的父类的全限定名，由于 Java 语言的单继承，所以父类索引只有一个，除了 java.lang.Object 之外，所有的 Java 类都有父类，因此除了 java.lang.Object 外，所有 Java 类的父类索引都不为 0。接口索引集合用来描述这个类实现了那些接口，这些被实现的接口将按 implements (如果这个类本身是接口的话则是extends) 后的接口顺序从左到右排列在接口索引集合中。\n### 字段表\n- access_flags: 字段的作用域（public ,private,protected修饰符），是实例变量还是类变量（static修饰符）,可否被序列化（transient 修饰符）,可变性（final）,可见性（volatile 修饰符，是否强制从主内存读写）。\n- name_index: 对常量池的引用，表示的字段的名称；\n- descriptor_index: 对常量池的引用，表示字段和方法的描述符；\n- attributes_count: 一个字段还会拥有一些额外的属性，attributes_count 存放属性的个数；\n- attributes[attributes_count]: 存放具体属性具体内容。\n\n> 这里说一下字段里面常见的属性：\n> ConstantValue：一个字段有了这个说明是被static修饰了的。目前Sun Javac编译器的选择是：如果同时使用final和static来修饰一个变量（按照习惯，这里称“常量”更贴切），并且这个变量的数据类型是基本类型或者java.lang.String的话，就生成ConstantValue属性来进行初始化，即编译的时候；如果这个变量没有被final修饰，或者并非基本类型及字符串，则将会选择在＜clinit＞方法中进行初始化，即类加载的时候。\n\n### 方法表\n内容基本上与上面一致，也是有访问标志，name，descriptor和属性，这里的属性只要方法内不为空都会有code这个属性\n\n方法里常见的属性：\n- Code：Java方法体里面的代码经过Javac编译之后，最终变为字节码指令存储在Code属性内，Code属性出现在在method_info结构的attributes表中，但在接口或抽象类中就不存在Code属性（JDK1.8可以出现了）。一个方法中的Code属性值有一个。在整个Class文件中，Code属性用于描述代码，所有的其他数据项目都用于描述元数据。在字节码指令之后的是这个方法的显式异常处理表（下文简称异常表）集合，异常表对于Code属性来说并不是必须存在的。\n- LineNumberTable：位于Code属性中，描述Java源码行号与字节码行号（字节码的偏移量）之间的对应关系。主要是如果抛出异常时，编译器会显示行号，比如调试程序时展示的行号，就是这个属性的作用。Code属性表中，LineNumberTable可以属性可以按照任意顺序出现。在Code属性 attributes表中，可以有不止一个LineNumberTable属性对应于源文件中的同一行。也就是说，多个LineNumberTable属性可以合起来表示源文件中的某行代码，属性与源文件的代码行之间不必有一一对应的关系。\n- exception_table： **在Java虚拟机中，处理异常（catch语句）不是由字节码指令来实现的，而是采用异常表来完成的。** 异常表中每一个数据由4部分组成，分别是start_pc、end_pc、handler_pc和catch_type。这4项表示从方法字节码的start_pc偏移量开始（包括）到end_pc 偏移量为止（不包括）的这段代码中，如果遇到了catch_type所指定的异常， 那么代码就跳转到handler_pc的位置执行，handler_pc即一个异常处理器的起点。在这4项中， start_pc、end_pc和handlerpc 都是字节码的编译量， 也就是在code[code_length]中的位置， 而catch_type为指向常量池的索引，它指向一个CONSTANT_Class_info 类，表示需要处理的异常类型。如果catch_type值为0,那么将会在所有异常抛出时都调用这个异常处理器，这被用于实现finally语句\n","tags":["Java"]},{"title":"代驾微服务项目-乘客下单","url":"/2024/07/21/代驾微服务项目-乘客下单/","content":"# 乘客下单\n主要业务是从乘客选择起始地点和终止地点之后，也就是初步计算了估计价格然后点击呼叫司机之后的过程。等到附近的司机抢单成功，该业务执行完毕。\n\n流程大致是：\n1. 在乘客点击下单之后，向数据库增加了一个order，前端每5s询问一次order的当前状态是否为接单，在这期间就没有客户端的事情了。\n2. 这个下单的动作会激活xxl-job的一个任务调度，给附近已经开启接单的司机的队列中加上这个order的id\n3. 等到司机端前端检查队列发现了这个订单，就会弹窗问是否需要接取，随后就是一个加锁然后抢票的逻辑了，需要有高并发的支持。\n4. 这个司机抢到了之后就会update这个订单，把司机的id变为自己的，状态改变。\n5. 等到乘客5s轮询发现改变了状态，此时接单成功。\n\n其中，最重要的过程是xxl-job的这个任务如何找到周围的司机。平时使用xxl-job都是在web新建的，这里要求每一个订单自动给xxl-job，也就是要对xxl-job的接口做调整，使其能够继承springboot新建任务。\n那么周围的司机如何找，每一个司机在接单的时候都会把自己的地理纬度上传到redis，用redis的数据结构geo快速完成计算。\n\n## 新建订单\n\n### POST\"/order/submitOrder\"\n\n由前端调用百度的api得到的初始位置和结束位置，以及乘客的id来在order数据库中新建一个订单。\n\n**对外接口**\n```java\n@Operation(summary = \"乘客下单\")\n@GuiguLogin\n@PostMapping(\"/submitOrder\")\npublic Result<Long> submitOrder(@RequestBody SubmitOrderForm submitOrderForm) {\n   submitOrderForm.setCustomerId(AuthContextHolder.getUserId());\n   return Result.ok(orderService.submitOrder(submitOrderForm));\n}\n```\n\n但是在数据库中这个表是需要有估计里程和估计价格的，也就是说还要再去调用一次map微服务和rule微服务。\n```java\n@Autowired\nprivate OrderInfoFeignClient orderInfoFeignClient;\n\n@Override\npublic Long submitOrder(SubmitOrderForm submitOrderForm) {\n    //1.重新计算驾驶线路\n    CalculateDrivingLineForm calculateDrivingLineForm = new CalculateDrivingLineForm();\n    BeanUtils.copyProperties(submitOrderForm, calculateDrivingLineForm);\n    DrivingLineVo drivingLineVo = mapFeignClient.calculateDrivingLine(calculateDrivingLineForm).getData();\n\n    //2.重新计算订单费用\n    FeeRuleRequestForm calculateOrderFeeForm = new FeeRuleRequestForm();\n    calculateOrderFeeForm.setDistance(drivingLineVo.getDistance());\n    calculateOrderFeeForm.setStartTime(new Date());\n    calculateOrderFeeForm.setWaitMinute(0);\n    FeeRuleResponseVo feeRuleResponseVo = feeRuleFeignClient.calculateOrderFee(calculateOrderFeeForm).getData();\n\n    //3.封装订单信息对象\n    OrderInfoForm orderInfoForm = new OrderInfoForm();\n    //订单位置信息\n    BeanUtils.copyProperties(submitOrderForm, orderInfoForm);\n    //预估里程\n    orderInfoForm.setExpectDistance(drivingLineVo.getDistance());\n    orderInfoForm.setExpectAmount(feeRuleResponseVo.getTotalAmount());\n\n    //4.保存订单信息\n    Long orderId = orderInfoFeignClient.saveOrderInfo(orderInfoForm).getData();\n    \n    //TODO启动任务调度\n    \n    return orderId;\n}\n```\n**订单微服务**\n\n```java\n@Autowired\nprivate OrderInfoService orderInfoService;\n\n@Operation(summary = \"保存订单信息\")\n@PostMapping(\"/saveOrderInfo\")\npublic Result<Long> saveOrderInfo(@RequestBody OrderInfoForm orderInfoForm) {\n   return Result.ok(orderInfoService.saveOrderInfo(orderInfoForm));\n}\n```\n设置订单的信息，状态为正在等待接单，给redis设置一个标识，说明正在接单。\n```java\n@Autowired\nprivate OrderInfoMapper orderInfoMapper;\n\n@Autowired\nprivate OrderStatusLogMapper orderStatusLogMapper;\n\n@Autowired\nprivate RedisTemplate redisTemplate;\n\n@Transactional(rollbackFor = {Exception.class})\n@Override\npublic Long saveOrderInfo(OrderInfoForm orderInfoForm) {\n   OrderInfo orderInfo = new OrderInfo();\n   BeanUtils.copyProperties(orderInfoForm, orderInfo);\n   String orderNo = UUID.randomUUID().toString().replaceAll(\"-\",\"\");\n   orderInfo.setStatus(OrderStatus.WAITING_ACCEPT.getStatus());\n   orderInfo.setOrderNo(orderNo);\n   orderInfoMapper.insert(orderInfo);\n\n   //记录日志\n   this.log(orderInfo.getId(), orderInfo.getStatus());\n\n   //接单标识，标识不存在了说明不在等待接单状态了\n   redisTemplate.opsForValue().set(RedisConstant.ORDER_ACCEPT_MARK, \"0\", RedisConstant.ORDER_ACCEPT_MARK_EXPIRES_TIME, TimeUnit.MINUTES);\n   return orderInfo.getId();\n}\n\npublic void log(Long orderId, Integer status) {\n   OrderStatusLog orderStatusLog = new OrderStatusLog();\n   orderStatusLog.setOrderId(orderId);\n   orderStatusLog.setOrderStatus(status);\n   orderStatusLog.setOperateTime(new Date());\n   orderStatusLogMapper.insert(orderStatusLog);\n}\n```\n### 司机和乘客查询订单状态GET\"/getOrderStatus/{orderId}\"\n\n乘客下完单后，订单状态为1，乘客端小程序会轮询订单状态，当订单状态为2时，说明已经有司机接单了，那么页面进行跳转，进行下一步操作\n\n```java\n@Override\npublic Integer getOrderStatus(Long orderId) {\n   LambdaQueryWrapper<OrderInfo> queryWrapper = new LambdaQueryWrapper<>();\n   queryWrapper.eq(OrderInfo::getId, orderId);\n   queryWrapper.select(OrderInfo::getStatus);\n   OrderInfo orderInfo = orderInfoMapper.selectOne(queryWrapper);\n   if(null == orderInfo) {\n      //返回null，feign解析会抛出异常，给默认值，后续会用\n      return OrderStatus.NULL_ORDER.getStatus();\n   }\n   return orderInfo.getStatus();\n}\n```\n> 乘客端是一下单就一直询问这个接口有没有改变；司机端得是抢到单了然后才能有订单的id，再去用这个id请求方法\n\n## xxl-job和redis完成司机搜索调度\n\nxxl-job定时任务查询附近司机，开启接单的司机会把地理坐标传到redis\n\n```java\n@Autowired\nprivate RedisTemplate redisTemplate;\n\n@Override\npublic Boolean updateDriverLocation(UpdateDriverLocationForm updateDriverLocationForm) {\n    /**\n     *  Redis GEO 主要用于存储地理位置信息，并对存储的信息进行相关操作，该功能在 Redis 3.2 版本新增。\n     *  后续用在，乘客下单后寻找5公里范围内开启接单服务的司机，通过Redis GEO进行计算\n     */\n    Point point = new Point(updateDriverLocationForm.getLongitude().doubleValue(), updateDriverLocationForm.getLatitude().doubleValue());\n    redisTemplate.opsForGeo().add(RedisConstant.DRIVER_GEO_LOCATION, point, updateDriverLocationForm.getDriverId().toString());\n    return true;\n}\n\n @Override\npublic Boolean removeDriverLocation(Long driverId) {\n    redisTemplate.opsForGeo().remove(RedisConstant.DRIVER_GEO_LOCATION, driverId.toString());\n    return true;\n}\n```\n### POST\"/searchNearByDriver\"\n\n司机端的小程序开启接单服务后，开始实时上传司机的定位信息到redis的GEO缓存，前面乘客已经下单，现在我们就要查找附近适合接单的司机，如果有对应的司机，那就给司机发送新订单消息。\n\n首先是配置经纬度点和距离，作为中心和半径传入redis的arg，得到了一个升序排序的位置表，然后根据这些待选司机的一些配置（比如超过多少距离不予配送）筛选出符合的一队司机，作为结果返回。\n```java\n@Autowired\nprivate DriverInfoFeignClient driverInfoFeignClient;\n\n@Override\npublic List<NearByDriverVo> searchNearByDriver(SearchNearByDriverForm searchNearByDriverForm) {\n    // 搜索经纬度位置5公里以内的司机\n    //定义经纬度点\n    Point point = new Point(searchNearByDriverForm.getLongitude().doubleValue(), searchNearByDriverForm.getLatitude().doubleValue());\n    //定义距离：5公里(系统配置)\n    Distance distance = new Distance(SystemConstant.NEARBY_DRIVER_RADIUS, RedisGeoCommands.DistanceUnit.KILOMETERS);\n    //定义以point点为中心，distance为距离这么一个范围\n    Circle circle = new Circle(point, distance);\n\n    //定义GEO参数\n    RedisGeoCommands.GeoRadiusCommandArgs args = RedisGeoCommands.GeoRadiusCommandArgs.newGeoRadiusArgs()\n            .includeDistance() //包含距离\n            .includeCoordinates() //包含坐标\n            .sortAscending(); //排序：升序\n\n    // 1.GEORADIUS获取附近范围内的信息\n    GeoResults<RedisGeoCommands.GeoLocation<String>> result = this.redisTemplate.opsForGeo().radius(RedisConstant.DRIVER_GEO_LOCATION, circle, args);\n\n    //2.收集信息，存入list\n    List<GeoResult<RedisGeoCommands.GeoLocation<String>>> content = result.getContent();\n\n    //3.返回计算后的信息\n    List<NearByDriverVo> list = new ArrayList();\n    if(!CollectionUtils.isEmpty(content)) {\n        Iterator<GeoResult<RedisGeoCommands.GeoLocation<String>>> iterator = content.iterator();\n        while (iterator.hasNext()) {\n            GeoResult<RedisGeoCommands.GeoLocation<String>> item = iterator.next();\n\n            //司机id\n            Long driverId = Long.parseLong(item.getContent().getName());\n            //当前距离\n            BigDecimal currentDistance = new BigDecimal(item.getDistance().getValue()).setScale(2, RoundingMode.HALF_UP);\n            log.info(\"司机：{}，距离：{}\",driverId, item.getDistance().getValue());\n\n            //获取司机接单设置参数\n            DriverSet driverSet = driverInfoFeignClient.getDriverSet(driverId).getData();\n            //接单里程判断，acceptDistance==0：不限制，\n            if(driverSet.getAcceptDistance().doubleValue() != 0 && driverSet.getAcceptDistance().subtract(currentDistance).doubleValue() < 0) {\n                continue;\n            }\n            //订单里程判断，orderDistance==0：不限制\n            if(driverSet.getOrderDistance().doubleValue() != 0 && driverSet.getOrderDistance().subtract(searchNearByDriverForm.getMileageDistance()).doubleValue() < 0) {\n                continue;\n            }\n\n            //满足条件的附近司机信息\n            NearByDriverVo nearByDriverVo = new NearByDriverVo();\n            nearByDriverVo.setDriverId(driverId);\n            nearByDriverVo.setDistance(currentDistance);\n            list.add(nearByDriverVo);\n        }\n    }\n    return list;\n}\n```\n\n> xxl-job这里配置就不做介绍了，主要梳理业务\n\n准备一个添加任务调度的业务，一分钟执行一次\n```java\n@Transactional(rollbackFor = Exception.class)\n@Override\npublic Long addAndStartTask(NewOrderTaskVo newOrderTaskVo) {\n    OrderJob orderJob = orderJobMapper.selectOne(new LambdaQueryWrapper<OrderJob>().eq(OrderJob::getOrderId, newOrderTaskVo.getOrderId()));\n    if(null == orderJob) {\n        //新增一个名为newOrderTaskHandler的任务调度，一分钟执行一次\n        Long jobId = xxlJobClient.addAndStart(\"newOrderTaskHandler\", \"\", \"0 0/1 * * * ?\", \"新订单任务,订单id：\"+newOrderTaskVo.getOrderId());\n\n        //记录订单与任务的关联信息\n        orderJob = new OrderJob();\n        orderJob.setOrderId(newOrderTaskVo.getOrderId());\n        orderJob.setJobId(jobId);\n        orderJob.setParameter(JSONObject.toJSONString(newOrderTaskVo));\n        orderJobMapper.insert(orderJob);\n    }\n    return orderJob.getJobId();\n}\n```\n**newOrderTaskHandler**\n```java\n@XxlJob(\"newOrderTaskHandler\")\npublic void newOrderTaskHandler() {\n    log.info(\"新订单调度任务：{}\", XxlJobHelper.getJobId());\n\n    //记录定时任务相关的日志信息\n    //封装日志对象\n    XxlJobLog xxlJobLog = new XxlJobLog();\n    xxlJobLog.setJobId(XxlJobHelper.getJobId());\n    long startTime = System.currentTimeMillis();\n    try {\n        //执行任务\n        newOrderService.executeTask(XxlJobHelper.getJobId());\n\n        xxlJobLog.setStatus(1);//成功\n    } catch (Exception e) {\n        xxlJobLog.setStatus(0);//失败\n        xxlJobLog.setError(ExceptionUtil.getAllExceptionMsg(e));\n        log.error(\"定时任务执行失败，任务id为：{}\", XxlJobHelper.getJobId());\n        e.printStackTrace();\n    } finally {\n        //耗时\n        int times = (int) (System.currentTimeMillis() - startTime);\n        xxlJobLog.setTimes(times);\n        xxlJobLogMapper.insert(xxlJobLog);\n    }\n}\n\n@Override\npublic Boolean executeTask(Long jobId) {\n    //获取任务参数\n    OrderJob orderJob = orderJobMapper.selectOne(new LambdaQueryWrapper<OrderJob>().eq(OrderJob::getJobId, jobId));\n    if(null == orderJob) {\n        return true;\n    }\n    NewOrderTaskVo newOrderTaskVo = JSONObject.parseObject(orderJob.getParameter(), NewOrderTaskVo.class);\n\n    //查询订单状态，如果该订单还在接单状态，继续执行；如果不在接单状态，则停止定时调度\n    Integer orderStatus = orderInfoFeignClient.getOrderStatus(newOrderTaskVo.getOrderId()).getData();\n    if(orderStatus.intValue() != OrderStatus.WAITING_ACCEPT.getStatus().intValue()) {\n        xxlJobClient.stopJob(jobId);\n        log.info(\"停止任务调度: {}\", JSON.toJSONString(newOrderTaskVo));\n        return true;\n    }\n\n    //搜索附近满足条件的司机\n    SearchNearByDriverForm searchNearByDriverForm = new SearchNearByDriverForm();\n    searchNearByDriverForm.setLongitude(newOrderTaskVo.getStartPointLongitude());\n    searchNearByDriverForm.setLatitude(newOrderTaskVo.getStartPointLatitude());\n    searchNearByDriverForm.setMileageDistance(newOrderTaskVo.getExpectDistance());\n    List<NearByDriverVo> nearByDriverVoList = locationFeignClient.searchNearByDriver(searchNearByDriverForm).getData();\n    //给司机派发订单信息\n    nearByDriverVoList.forEach(driver -> {\n        //记录司机id，防止重复推送订单信息\n        String repeatKey = RedisConstant.DRIVER_ORDER_REPEAT_LIST+newOrderTaskVo.getOrderId();\n        boolean isMember = redisTemplate.opsForSet().isMember(repeatKey, driver.getDriverId());\n        if(!isMember) {\n            //记录该订单已放入司机临时容器\n            redisTemplate.opsForSet().add(repeatKey, driver.getDriverId());\n            //过期时间：15分钟，新订单15分钟没人接单自动取消\n            redisTemplate.expire(repeatKey, RedisConstant.DRIVER_ORDER_REPEAT_LIST_EXPIRES_TIME, TimeUnit.MINUTES);\n\n            NewOrderDataVo newOrderDataVo = new NewOrderDataVo();\n            newOrderDataVo.setOrderId(newOrderTaskVo.getOrderId());\n            newOrderDataVo.setStartLocation(newOrderTaskVo.getStartLocation());\n            newOrderDataVo.setEndLocation(newOrderTaskVo.getEndLocation());\n            newOrderDataVo.setExpectAmount(newOrderTaskVo.getExpectAmount());\n            newOrderDataVo.setExpectDistance(newOrderTaskVo.getExpectDistance());\n            newOrderDataVo.setExpectTime(newOrderTaskVo.getExpectTime());\n            newOrderDataVo.setFavourFee(newOrderTaskVo.getFavourFee());\n            newOrderDataVo.setDistance(driver.getDistance());\n            newOrderDataVo.setCreateTime(newOrderTaskVo.getCreateTime());\n\n            //将消息保存到司机的临时队列里面，司机接单了会定时轮询到他的临时队列获取订单消息\n            String key = RedisConstant.DRIVER_ORDER_TEMP_LIST+driver.getDriverId();\n            redisTemplate.opsForList().leftPush(key, JSONObject.toJSONString(newOrderDataVo));\n            //过期时间：1分钟，1分钟未消费，自动过期\n            //注：司机端开启接单，前端每5秒（远小于1分钟）拉取1次“司机临时队列”里面的新订单消息\n            redisTemplate.expire(key, RedisConstant.DRIVER_ORDER_TEMP_LIST_EXPIRES_TIME, TimeUnit.MINUTES);\n            log.info(\"该新订单信息已放入司机临时队列: {}\", JSON.toJSONString(newOrderDataVo));\n        }\n    });\n    return true;\n}\n```\n最后再在下单的那个任务中执行添加xxl-job任务调度。","tags":["Redis"]},{"title":"代驾微服务项目-乘客端核心业务逻辑","url":"/2024/07/19/代驾微服务项目-乘客端核心业务逻辑/","content":"# 登录\n## 微信的登录逻辑\n<div style=\"text-align: center;\">\n  <img src=\"../img/daijia/img.png\" alt=\"\" />\n</div>\n说明：\n\n1. 调用 [wx.login()](https://developers.weixin.qq.com/miniprogram/dev/api/open-api/login/wx.login.html) 获取 **临时登录凭证code** ，并回传到开发者服务器。\n2. 调用 [auth.code2Session](https://developers.weixin.qq.com/miniprogram/dev/OpenApiDoc/user-login/code2Session.html) 接口，换取 **用户唯一标识 OpenID** 、 用户在微信开放平台账号下的**唯一标识UnionID**（若当前小程序已绑定到微信开放平台账号） 和 **会话密钥 session_key**。\n\n之后开发者服务器可以根据用户标识来生成自定义登录态，用于后续业务逻辑中前后端交互时识别用户身份\n\n对于业务而言，最首先返回的就是这个code，后端要做的就是把这个code结合自己小程序的appid和appsecret去请求微信后台，给出当前用户的openid。\n\n### 接口1：GET\"/customer/login/{code}\"\n\n整体的流程是：\n1. 对外service拿着这个前端发来的code去远程调用乘客微服务\n2. 乘客微服务请求微信的后台得到openid，然后去数据库查是否存在这个openid，也就是是否注册过了\n3. 如果没有注册就进行注册，最后写入操作日志，返回这个openid对应的数据库id，我们的业务是基于自家数据库的。\n4. 对外service拿到了这个id，将其缓存进redis中，key是UUID，value是在**自家数据库**中的id，也就是说在redis中有这个键值对那就说明是登录状态。\n5. 最后redis中的这个UUID作为token给前端，前端每次发请求都携带这个token。**优化：** 可以使用jwt？\n\n**对外接口：**\n```java\n@Operation(summary = \"小程序授权登录\")\n@GetMapping(\"/login/{code}\")\npublic Result<String> wxLogin(@PathVariable String code) {\n   return Result.ok(customerInfoService.login(code));\n}\n```\n\n```java\n@Autowired\nprivate CustomerInfoFeignClient customerInfoFeignClient;\n\n@Autowired\nprivate RedisTemplate redisTemplate;\n\n@Override\npublic String login(String code) {\n   //获取openId\n   Result<Long> result = customerInfoFeignClient.login(code);\n   if(result.getCode().intValue() != 200) {\n      throw new GuiguException(result.getCode(), result.getMessage());\n   }\n   Long customerId = result.getData();\n   if(null == customerId) {\n      throw new GuiguException(ResultCodeEnum.DATA_ERROR);\n   }\n\n   String token = UUID.randomUUID().toString().replaceAll(\"-\", \"\");\n   redisTemplate.opsForValue().set(RedisConstant.USER_LOGIN_KEY_PREFIX+token, customerId.toString(), RedisConstant.USER_LOGIN_KEY_TIMEOUT, TimeUnit.SECONDS);\n   return token;\n}\n```\n**乘客微服务接口：**\n```java\n@Operation(summary = \"小程序授权登录\")\n@GetMapping(\"/login/{code}\")\npublic Result<Long> login(@PathVariable String code) {\n   return Result.ok(customerInfoService.login(code));\n}\n```\n\n```java\n@Autowired\nprivate WxMaService wxMaService;\n\n@Autowired\nprivate CustomerLoginLogMapper customerLoginLogMapper;\n\n/**\n * 条件：\n *      1、前端开发者appid与服务器端appid一致\n *      2、前端开发者必须加入开发者\n * @param code\n * @return\n */\n@Transactional(rollbackFor = {Exception.class})\n@Override\npublic Long login(String code) {\n   String openId = null;\n   try {\n      //获取openId\n      WxMaJscode2SessionResult sessionInfo = wxMaService.getUserService().getSessionInfo(code);\n      openId = sessionInfo.getOpenid();\n      log.info(\"【小程序授权】openId={}\", openId);\n   } catch (Exception e) {\n       e.printStackTrace();\n      throw new GuiguException(ResultCodeEnum.WX_CODE_ERROR);\n   }\n   //查本地数据库是否有这条\n   CustomerInfo customerInfo = this.getOne(new LambdaQueryWrapper<CustomerInfo>().eq(CustomerInfo::getWxOpenId, openId));\n   if(null == customerInfo) {\n      customerInfo = new CustomerInfo();\n      customerInfo.setNickname(String.valueOf(System.currentTimeMillis()));\n      customerInfo.setAvatarUrl(\"https://oss.aliyuncs.com/aliyun_id_photo_bucket/default_handsome.jpg\");\n      customerInfo.setWxOpenId(openId);\n      this.save(customerInfo);\n   }\n    \n   //登录日志\n   CustomerLoginLog customerLoginLog = new CustomerLoginLog();\n   customerLoginLog.setCustomerId(customerInfo.getId());\n   customerLoginLog.setMsg(\"小程序登录\");\n   customerLoginLogMapper.insert(customerLoginLog);\n   return customerInfo.getId();\n}\n```\n### 接口2：GET\"/customer/getCustomerLoginInfo\"\n\n当前一个方法执行成功后，返回了一个token，每次请求都会携带这个token。有了这个token后前端会发这个请求获得该用户的详细信息。那既然把UUID和value都放在redis里了，那每一次都从redis取就好了。\n\n**AOP+注解+ThreadLocal**\n这个注解的主要思路是，在每个controller方法调用之前，先去解析request请求的token，token里面是登录时所给的UUID，然后再去访问redis得到当前登录用户的本地数据库id，把它放在一个ThreadLocal保存，这样该方法执行的过程中只用在ThreadLocal中取这个就可以用了。\n\n***主要是用于取代网关的prehandle，其实本质是一样的，网关是直接拦截request得到token，进行处理后再放到ThreadLocal中***\n```java\n@Documented\n@Retention(RetentionPolicy.RUNTIME)\n@Target(ElementType.METHOD)\npublic @interface GuiguLogin {\n\n}\n```\n\n```java\n@Slf4j\n@Component\n@Aspect\n@Order(100)\npublic class GuiguLoginAspect {\n\n    @Autowired\n    private RedisTemplate redisTemplate;\n\n    /**\n     *\n     * @param joinPoint\n     * @param guiguLogin\n     * @return\n     * @throws Throwable\n     */\n    @Around(\"execution(* com.atguigu.daijia.*.controller.*.*(..)) && @annotation(guiguLogin)\")\n    public Object process(ProceedingJoinPoint joinPoint, GuiguLogin guiguLogin) throws Throwable {\n        RequestAttributes ra = RequestContextHolder.getRequestAttributes();\n        ServletRequestAttributes sra = (ServletRequestAttributes) ra;\n        HttpServletRequest request = sra.getRequest();\n        String token = request.getHeader(\"token\");\n\n        if(!StringUtils.hasText(token)) {\n            throw new GuiguException(ResultCodeEnum.LOGIN_AUTH);\n        }\n        String userId = (String)redisTemplate.opsForValue().get(RedisConstant.USER_LOGIN_KEY_PREFIX+token);\n        if(StringUtils.hasText(userId)) {\n            AuthContextHolder.setUserId(Long.parseLong(userId));\n        }\n        return joinPoint.proceed();\n    }\n\n}\n```\n\n有了这个注解再进行个人资料的获取，现在直接就在接口上用则个注解，然后再在方法里面取出id就可以了，大致流程是：\n1. 用注解得到的id请求客户微服务\n2. 客户微服务用id查数据库\n\n**对外接口：**\n```java\n@Operation(summary = \"获取客户登录信息\")\n@GuiguLogin\n@GetMapping(\"/getCustomerLoginInfo\")\npublic Result<CustomerLoginVo> getCustomerLoginInfo() {\n   Long customerId = AuthContextHolder.getUserId();\n   return Result.ok(customerInfoService.getCustomerLoginInfo(customerId));\n}\n```\n\n```java\n@Override\npublic CustomerLoginVo getCustomerLoginInfo(Long customerId) {\n   Result<CustomerLoginVo> result = customerInfoFeignClient.getCustomerLoginInfo(customerId);\n   if(result.getCode().intValue() != 200) {\n      throw new GuiguException(result.getCode(), result.getMessage());\n   }\n   CustomerLoginVo customerLoginVo = result.getData();\n   if(null == customerLoginVo) {\n      throw new GuiguException(ResultCodeEnum.DATA_ERROR);\n   }\n   return customerLoginVo;\n}\n```\n**客户微服务：**\n这里只写实现类，接口只是传导没有实际逻辑。\n```java\n@Override\npublic CustomerLoginVo getCustomerLoginInfo(Long customerId) {\n   CustomerInfo customerInfo = this.getById(customerId);\n   CustomerLoginVo customerInfoVo = new CustomerLoginVo();\n   BeanUtils.copyProperties(customerInfo, customerInfoVo);\n   //判断是否绑定手机号码，如果未绑定，小程序端发起绑定事件\n   Boolean isBindPhone = StringUtils.hasText(customerInfo.getPhone());\n   customerInfoVo.setIsBindPhone(isBindPhone);\n   return customerInfoVo;\n}\n```\n\n## 自定义feign全局处理\n在Feign调用的过程中，由于全局异常的处理，所有的Feign调用都会返回Result<T>对象，我们还必须判断它的code是否等于200，如果不等于200，那么说明调用结果抛出异常了，我们必须返回异常信息提示给接口，如果返回code等于200，我们又必须判断data是否等于null，处理方式都一致，处理起来很繁琐，有没有好的统一处理方式呢？\n\n答案是肯定的，我们可以通过全局自定义Feign结果解析来处理就可以了。\n\n说明：任何Feign调用Result<T>对象的data我们都必须默认给一个返回值，否则任务数据异常。\n\n**自定义解码器:**\n\n```java\n/**\n * OpenFeign 自定义结果解码器\n */\npublic class FeignCustomDataDecoder implements Decoder {\n    private final SpringDecoder decoder;\n\n    public FeignCustomDataDecoder(SpringDecoder decoder) {\n        this.decoder = decoder;\n    }\n\n    @Override\n    public Object decode(Response response, Type type) throws IOException {\n        Object object = this.decoder.decode(response, type);\n        if (null == object) {\n            throw new DecodeException(ResultCodeEnum.FEIGN_FAIL.getCode(), ResultCodeEnum.FEIGN_FAIL.getMessage(), response.request());//\"数据解析失败\"\n        }\n        if(object instanceof Result<?>) {\n            Result<?> result = ( Result<?>)object;\n            //返回状态!=200，直接抛出异常，全局异常捕获异常，接口提示\n            if (result.getCode().intValue() != ResultCodeEnum.SUCCESS.getCode().intValue()) {\n                throw new DecodeException(result.getCode(), result.getMessage(), response.request());//\"数据解析失败\"\n            }\n            //远程调用必须有返回值，具体调用中不用判断result.getData() == null，这里统一处理\n            if (null == result.getData()) {\n                throw new DecodeException(ResultCodeEnum.FEIGN_FAIL.getCode(), ResultCodeEnum.FEIGN_FAIL.getMessage(), response.request());//\"数据解析失败\"\n            }\n            return result;\n        }\n        return object;\n    }\n}\n```\n\n```java\n@Configuration\npublic class FeignConfig {\n\n    /**\n     * 自定义解析器\n     *\n     * @param msgConverters 信息转换\n     * @param customizers   自定义参数\n     * @return 解析器\n     */\n    @Bean\n    public Decoder decoder(ObjectFactory<HttpMessageConverters> msgConverters, ObjectProvider<HttpMessageConverterCustomizer> customizers) {\n        return new OptionalDecoder((new ResponseEntityDecoder(new FeignCustomDataDecoder(new SpringDecoder(msgConverters, customizers)))));\n    }\n\n}\n```\n//TODO: 这一块需要对了解openfeign的结构有一定的了解，后续将对这一块进行补充","tags":["Redis"]},{"title":"新坑-代驾微服务项目","url":"/2024/07/19/新坑-代驾微服务项目/","content":"> 前言：一直都想做一个有点高级但是又重复度低的项目，正好让我遇上了这个尚硅谷刚发的《乐尚代驾》，做了一下感觉还是有学到很多东西的，不管以后放不放在简历里面，算是先开了一个小头，弄懂再说。\n\n# 项目介绍\n该项目是模仿滴滴的一个代驾系统，前端是基于uniapp的微信小程序，分为三个端口：客户端，司机端和管理系统。业务很纯粹，就是乘客基于当前位置呼叫周围正在接单的司机，司机接单后对车辆的基本情况进行上传，然后就可以开始代驾了，到达终点后计算金额并用微信支付；此外还涉及到优惠券业务，也就是传统的那套秒杀逻辑，分布式锁+lua脚本那些。\n## 技术栈\n- SpringCloudAlibaba：包括nacos，openfeign，gateway这些，其实后续还可以做一些熔断和限流操作。\n- redis：项目中主要是用到了geo数据结构，在司机开启抢单的时候将地理坐标上传到redis；此外也用到了redisson实现分布式锁和延迟队列（这里的延迟队列就是超时15分钟自动取消，其实也可以用rabbitmq实现）\n- xxl-job：项目最核心的业务是通过xxl-job实现的，每次下单客户端都会创建一个新的任务调度，这个任务可以定时寻找周围司机。\n- mongodb：在代驾订单进行过程中的地理坐标不好放在mysql里面，用mongodb存储地理坐标，考虑了其在实时性方面的优化\n- rabbitmq：延迟队列，业务解耦和流量削峰\n- Drools：第一次听说，规则引擎，就是把一些业务计算逻辑抽取出来，不硬编码在程序中，项目中用规则引擎定义了路程和费用的计算，还有最后算金额也用到了规则引擎，它有自己的一套语法。\n- seata：分布式事务，在分布式系统中一个事务的回调是无法用@Transactional进行回调的，这里就用了阿里的这个分布式事务中间件完成，目前还没有太了解这个，只知道导入包只会加一个注释就能完成功能。\n- minio：存储上传照片和音频文件\n## 项目亮点\n- 使用**aop+注解+threadlocal**的方式完成登录状态保存，微信小程序登录，做到了对业务代码的零入侵。\n- 在下单后用**xxl-job**进行任务调度，定时搜索周围司机，并基于**redis的geo**数据结构，提高附近司机的搜索速度，完成司乘对接。\n- 为了减小mysql访问压力使用**mongodb**记录沿途地理坐标，并使用基于**rabbitmq**的延迟队列将多次写请求合并为一次写入，再次减小了数据库操作。\n- 使用**策略模式和规则引擎Drools**，定义了业务执行过程中的多种流程，例如代驾费用计算，司机积分计算以及优惠卷最大优惠策略计算。\n- 使用**Redisson分布式锁**解决了高并发场景下的司机抢单和优惠卷超发问题，使用Redisson提供的**延时队列**完成了订单超出时间限制后自动取消。\n- 使用**CompletableFuture**完成了订单结束提交过程的异步编排，提高了响应效率，并使用**seata**来保证分布式事务的执行。\n## 可以有增量的地方\n- 数据库分库分表，ShardingSphere可以尝试一下\n- 二级缓存，本地caffeine和redis二级缓存\n- 做限流和熔断，以及redis集群和哨兵\n- 点赞（set），评论，排行榜（zset），签到（bitmap）和UV统计（hyperloglog）都是可以用redis解决的\n- kafka-stream还是没能找到应用的场景，\n- binlog实现与mysql的持久化\n- 优惠卷兑换算法和优惠卷的最大优惠计算\n\n## 业务流程\n\n### 乘客端：\n> 登录--选择代驾地址--呼叫代驾--等待接单--15分钟没有司机接单自动取消--15内有司机接单，司乘同显--账单支付\n1. 登录：前端首先调用wx.login()，返回一串字符串，然后后端拿到这个去请求微信服务器得到当前登录的openid，也就是微信的唯一id（可以理解为wx.login()请求发出之后，wx的缓存中就保存了这个键值对，键是这个字符串，值是当前用户的结构，必须是连贯的动作，可能设置了失效时间，这也是为什么我后面拿postman同样请求得不到的原因）\n2. 选择代驾地址：百度地图的api返回距离和时间，这个参数给后端规则引擎计算出费用\n3. 等待接单：xxl-job新增一个订单任务，每隔一分钟根据redis的geo搜索周围的汽车，然后在附近开启接单的汽车的队列里面（通过redis的list实现）添加这个订单。\n4. 15分钟没有司机接单自动取消：通过redisson实现或者rabbitmq的延迟队列实现\n5. 15内有司机接单，司乘同显：当司机抢到了该订单后，订单状态改变，进入司乘同显模式，此后基本上没有乘客端什么事情了，前端会一直轮询订单当前状态，并根据司机端存在mongodb中的数据获取车的位置。\n6. 账单支付：微信支付\n### 司机端：\n> 登录--认证--开始接单--抢单--开始代驾--生成账单，发送乘客\n1. 登录：逻辑类似\n2. 认证：司机需要上传身份证驾驶证人脸等，通过腾讯云，我懒得注册这部分就跳过了\n3. 开始接单：开始接单后会把当前的坐标上传到redis，然后每隔5s询问一次当前队列状态，当xxl-job把订单放在了该司机的队列里面，前端就会有显示可以抢单\n4. 抢单：分布式锁，无需多言，抢单之后将订单状态改变，这样乘客端轮询这个订单的时候就会发现有司机已经接单了。\n5. 到达乘客指定地点，这里会有一个刷单的校验，也就是说在距离地点1km之内才能有效\n6. 上传车辆状况和车牌号：拍照上传minio\n7. 开始代驾：每隔几秒钟将当前的坐标上传到mongodb中，实现与乘客端同步显示。\n8. 生成账单：根据规则引擎分账，然后推送微信支付给乘客。\n","tags":["Redis"]},{"title":"论文精读-DOMINANT","url":"/2024/07/11/论文精读-DOMINANT/","content":"<!-- TOC -->\n* [Deep Anomaly Detection on Attributed Networks](#deep-anomaly-detection-on-attributed-networks)\n  * [图卷积神经网络](#图卷积神经网络)\n  * [模型架构](#模型架构)\n  * [带带锐评](#带带锐评)\n<!-- TOC -->\n> 开发项目虽好，但是还是要毕业的嘛，正好今天做完了组会汇报ppt，趁热打铁写一篇读后感。\n> \n> **2024-7-11，我研究生生涯的第一篇论文，我会记住你的。**\n\n# Deep Anomaly Detection on Attributed Networks\n\n> 在说明这篇文章之前，首先说一下图卷积神经网络（GSN）\n\n## 图卷积神经网络\n\n&nbsp;&nbsp;图是一种数据结构，相比于离散的点，更具有特征的是边带来的结构上的关系。普通的卷积神经网络处理的主要是结构性数据，类似图片或者nlp任务，这一类任务的输入数据是有迹可循的，结构不会发生变化，图片可以拆为像素进行卷积核操作，nlp可以将词进行词嵌入。\n\n&nbsp;&nbsp;而图不一样，图的拓扑结构是无迹可寻的，这也意味着普通的cnn无法满足在图上的搜索，但是我们可以参考cnn的公式$Y=XW+B$，结合图的规律探索出图的卷积神经网络。\n\n&nbsp;&nbsp;首先我们定义一个图，$\\mathcal{G}=(\\mathcal{V},\\epsilon,X)$，（1）节点集合$\\mathcal{V} = \\mathcal{v_1},\\mathcal{v_2},\\ldots,\\mathcal{v_n}$，其中$\\left|\\mathcal{V}\\right|=\\mathcal{n}$ （2）边集$\\epsilon$，其中$\\left|\\epsilon\\right|=\\mathcal{m}$（3）节点属性$\\mathrm{X}\\in\\Bbb{R}^{n \\times d}$,其中第i行向量$\\mathrm{x_i}\\in\\\\Bbb{R}^{d}\\(i=1,\\ldots,n\\)$表示的是第i个节点的属性表示，d是属性的数量。举例如下：\n\n<div style=\"text-align: center;\">\n  <img src=\"../img/oodPaper/img_1.png\" alt=\"\" />\n</div>\n\n&nbsp;&nbsp;这里举例是有权图，如果是无权图那就都为0或者1，这张图的邻接矩阵A为：\n\n|    | v1 | v2 | v3 | v4 |\n|----|----|----|----|----|\n| v1 | 0  | 2  | 5  | 6  |\n| v2 | 2  | 0  | 0  | 0  |\n| v3 | 5  | 0  | 0  | 3  |\n| v4 | 6  | 0  | 3  | 0  |\n\n&nbsp;&nbsp;节点属性H，比如这四个点代表一个人，属性就可能是身高体重这类，举例如下：\n\n|    | 姓名  | 性别 | 身高  | 体重  |\n|----|-----|----|-----|-----|\n| H1 | 239 | 2  | 175 | 120 |\n| H2 | 542 | 1  | 168 | 100 |\n| H3 | 937 | 2  | 188 | 150 |\n| H4 | 365 | 1  | 163 | 90  |\n\n&nbsp;&nbsp;如果我们取矩阵中的一行A1点乘H，即有：\n$$\nA_1 \\cdot H=\\(0 \\times H_1 +2 \\times H_2 +5 \\times H_3 + 6 \\times H_4\\)\n$$\n\n&nbsp;&nbsp;可以看出，这个结果是V1的邻居节点信号的加权求和，其中权重为关系强弱数值，由A提供，但是这个权重并没有进行归一化，也就是说，如果某个节点的邻居顶点越多，关系数值越强，这个结果就越大，为了避免这种情况，我们进行了归一化操作，即，让权重除以该节点所有边的关系数值的和，上这些边的关系数值成为真正意义上和为1的“权值”。那么我们所需要的数学过程即让A的每一行都除以该行的和。这是我们引入一个新的矩阵D，这个矩阵为对角矩阵，每行对角线上的元素为A的这行的元素和，也就是该顶点的度。\n\n&nbsp;&nbsp;D矩阵：\n\n| <!-- --> | <!-- --> | <!-- --> | <!-- --> |\n|----------|----------|----------|----------|\n| 13       | 0        | 0        | 0        |\n| 0        | 2        | 0        | 0        |\n| 0        | 0        | 8        | 0        |\n| 0        | 0        | 0        | 9        |\n\n&nbsp;&nbsp;接着执行$D^{-1}A$操作，即把A的每个关系数值都归一化了，变为权重，最后与H相乘，归一化后的表达式为：\n\n$$\nD_{1}^{-1}A_1 \\cdot H=\\(1/13\\) \\times \\(0 \\times H_1 +2 \\times H_2 +5 \\times H_3 + 6 \\times H_4\\)\n$$\n\n&nbsp;&nbsp;所有行都进行同样的操作，$D^{-1}A \\times\\ H$，则每一行的量纲都一样了，不会出现某一行计算结果特别夸张。此时我们来考虑这个结果是什么，它相当于是某一个顶点周围所有顶点的信号按关系（权重）相加（聚合），那么这个结果就能表征出周围节点对自己的影响了，同时由于是通过矩阵进行运算，数据结构变得很规整，可以使用计算机来运算。\n\n> 到这里为止有两个问题：\n> - 首先是自身的因素没有考虑，就如上面的例子，没有考虑到$H_1$的影响，也就是自己，这是很不合理的，在传播过程中需要有“我”的参与，而不是都是客体。\n> - 其次是没有考虑到邻居的影响，假如我的朋友只有一个大佬，那么我和大佬的关系网络如果用平均算法的话就等同了，所以直接把B的特征赋给A肯定是不合适的。\n\n<div style=\"text-align: center;\">\n  <img src=\"../img/oodPaper/img_2.png\" alt=\"\" />\n</div>\n\n**针对第一个问题：**\n把“我”的信息加进去\n$$\\tilde{A}=A+a \\cdot I$$\n这样A就不再是一个单位阵，在进行矩阵运算的时候就能考虑到自己的因素，因此D也要随之改变，D原本是表示出度，此时要变为出度+a\n$$\\tilde{D}=D+a \\cdot I$$\n\n**针对第二个问题：**\n关系数值我们一般是通过两顶点信号之间的欧氏距离得到的，这只跟这两个顶点有关系，与其二阶邻居的信号是无关的，但是显然，邻居的邻居对我影响应该也是有的，我们怎样才能把二阶邻居的影响考虑进来呢？\n\n比如，我叫V2，是个自闭症患者，班里一共四个人，我们自己的信号为社交能力值，我只认识V1，认识V1也不是因为我和他聊得来，纯纯是因为V1是个社牛， V1谁都认识。在这个例子中，如果考虑V2经过一次传播后形成的新V2，是通过$\\tilde{D_2^{-1}}\\tilde{A_2}H$计算的，结果为$\\tilde{D_2^{-1}}\\tilde{A_2}H$=(1/3)×(2×H1 + 1×H2 + 0×H3 + 0×H4) =2/3 H1 + 1/3H2 ,可以看出，此时新的V2的信号绝大部分来源于原来V1的信号，这当然不行，怎么经过一次传播后，把我一个社恐变成了社牛，这显然传播仍然存在问题，我希望能把二阶邻居和一阶邻居都考虑进去，二阶邻居和一阶邻居数量差别较大的时候，我希望能衰减这种影响，尽可能让自己的信号和别人的信号尽可能的分开来，那么就容易得到两种思路了，一是传播中我尽可能保留自己的权重，削减别人的权重，即$\\tilde{A}$对做处理，第二是让“我”的信号根据传播产生某种线性变化，即随着二阶邻居和一阶邻居数量差别越大我信号越小，这样也可以把我和别人区分出来，也可以有我自己单独的特征，就是信号特别小嘛。GCN中是按照第二个思路来的，在数学上新的传播过程表示为：\n\n$$\n\\tilde{D^{-\\frac{1}{2}}}\\tilde{A}\\tilde{D^{-\\frac{1}{2}}}H\n$$\n为什么是开根号，因为在衰减的时候还是要尽量满足归一化。可以看作对$\\tilde{A_{i,j}}$做了如下操作：\n\n$$\n\\tilde{A_{ij}} = \\frac{A_{ij}}{\\sqrt{D_{ii}} \\sqrt{D_{jj}}}\n$$\n\n这样，当一阶邻居顶点j的度（边数量，也就是它的一阶邻居数量）很大，那么传播一次后信号就会变得很小，比如对V2来讲：\n\n$$\n\\tilde{D_{22}^{-\\frac{1}{2}}}\\tilde{A_2}\\tilde{D_{11}^{-\\frac{1}{2}}}H=\\frac{1}{\\sqrt{3} \\sqrt{14}}\\(2H_1+H_2\\)\n$$\n\n最后再通过一个激活函数，以及一个阈值b，就可以类似cnn得到一个前向传播公式：\n\n$$\nH^{l+1}=\\sigma \\(\\tilde{D^{-\\frac{1}{2}}}\\tilde{A}\\tilde{D^{-\\frac{1}{2}}}H^{l}W^{l}+b^{l}\\)\n$$\n\n其中$\\tilde{D^{-\\frac{1}{2}}}\\tilde{A}\\tilde{D^{-\\frac{1}{2}}}$是固定不变的，其余的工作就是构建全连接层，损失函数，反向传播，更新参数。\n\n## 模型架构\n\n前面铺垫了这么多，那么这个模型是什么呢，说白了就是类似transformer的编码器解码器，基于重构的方法。\n\n<div style=\"text-align: center;\">\n  <img src=\"../img/oodPaper/img_3.png\" alt=\"\" />\n</div>\n\n- 编码器：将输入的属性矩阵通过前面所述的图卷积神经网络提取特征，得到了一个杂糅属性和结构关系的低维向量表示。\n- 解码器：根据这个得到的中间表示**重构**，结构通过结构解码器进行重构，属性通过属性编码器进行重构。然后计算重构结果与实际结果，完成损失函数最小化。原文这里的结构解码器非常简单暴力，就用中间低维特征提取的内积完成，$\\sigma\\(Z\\*Z^{T}\\)$，属性用另一层GSN进行映射。\n\n**那么是怎么判断出异常的？**\n\n通过以上步骤重建拓扑网络结构，将结构和属性的重构误差共同学习，可以表示为：\n$$\n\\mathcal{L}=(1-\\alpha)R_S+\\alpha R_A=(1-\\alpha){\\Vert A-\\hat{A} \\Vert}_F^2+\\alpha {\\Vert X-\\hat{X} \\Vert}_F^2\n$$\n式中，是用来平衡结构重建和属性重建影响的一个重要参数。\n\n通过最小化目标函数，自编码器可以基于编码的潜在表示迭代地近似输入的属性网络，直至收敛。最后，使用两项重构误差之和来评估节点的异常性。 也就是，得分越高的实例越是被认为异常。再由该分数来计算属性网络的异常排名\n\n## 带带锐评\n\n- 优点：结构新颖，基于图卷积神经网络，特征提取考虑了结构和点属性两方面特征，更能检测出异常，对稀疏图数据的处理有优势。\n- 缺点：重构本身具有一定的局限性，只适用于异常数据占比比较小的数据集进行训练，否则通过重构方法后的误差较大，还有前文所说的解码器构造有一点暴力了，因为Z不止有结构还有属性，直接内积会有干扰，改进措施应该可以剔除这部分因素进行单纯的结构重构。\n","tags":["科研论文"]},{"title":"redis项目-elasticsearch与mongodb","url":"/2024/06/11/redis项目-elasticsearch与mongodb/","content":"\n> 其实这个搜索的功能反倒跟redis没啥关系了，所以我也没归类在redis里。基于es的搜索也不无非是增删改查，先简单说一下项目里用到的这两个功能和api。后续我会详细学习\n\n## elasticsearch\n\nes是一个搜索器，具体怎么搜的我不知道（笑）。目前的理解是在新增blog的时候同时把这个对象给es服务器，它通过索引的方式进行缓存。后续查找的时候就可以根据field，也就是字段名来指定匹配位置。\n\n比如这里要搜索文章标题的关键字，就用title；如果是内容的，就用content。经过我的实验发现黑马给的分词器只能检索中文，我想要搜素字母“s”，都搜不到。\n\n项目里的应用也比较简单，就是上述的关键字搜索，然后返回前端的时候要标红，这个步骤是通过在前后加font标签完成的。\n\n首先是配置：\n\n这里用到的对象是RestHighLevelClient，我们只需要配置端口号和ip就可以了，这个客户端会使用es的restful端口进行增删改查。\n\n我们对其的增删改查操作就跟redistemplate一样的。\n\n```java\n@Configuration\npublic class ElasticSearchConfig {\n    @Value(\"${elasticsearch.host}\")\n    private String host;\n    @Value(\"${elasticsearch.port}\")\n    private Integer port;\n    /**\n     * 配置RestHighLevelClient对象\n     * 将该对象交给Spring容器去管理\n     *\n     * @return RestHighLevelClient对象\n     */\n    @Bean\n    public RestHighLevelClient restHighLevelClient() {\n        return new RestHighLevelClient(\n                RestClient.builder(\n                        //若有多个，可以传一个数组\n                        new HttpHost(host, port, \"http\")));\n    }\n}\n```\n\n用这个RestHighLevelClient，传输的是一个httprequest，返回的是response。思路是创建一个request，查询标题和内容两个方面的关键词。\n返回以后进行一个转换，返回的加上颜色标签的hits是一个数组，需要转换成字符串。最后向前端返回这个字符串。\n\n```java\n@Override\npublic Result searchBlog(QueryBlogDto dto) throws IOException {\n    //1.输入校验\n    if (dto == null){\n        return Result.fail(\"输入为空\");\n    }\n    if (dto.getKeyWord() == null || dto.getKeyWord().isEmpty()){\n        return Result.fail(\"输入关键词为空\");\n    }\n    //异步给mongo\n    insertSearchHistory(dto.getKeyWord());\n\n    //2.查询条件\n    SearchRequest searchRequest = new SearchRequest(\"hmdp_blogs\");\n    SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder();\n\n    //2.1布尔类型的查询\n    BoolQueryBuilder builder = QueryBuilders.boolQuery();\n\n    //2.2 查询标题和内容两方面的关键词\n    QueryStringQueryBuilder queryStringQueryBuilder = QueryBuilders.queryStringQuery(dto.getKeyWord()).field(\"content\").field(\"title\").defaultOperator(Operator.OR);\n    builder.must(queryStringQueryBuilder);\n\n    //2.3 文本高亮这个查找到的关键字\n    HighlightBuilder highlightBuilder = new HighlightBuilder();\n    highlightBuilder.field(\"title\");\n    highlightBuilder.field(\"content\");\n    //2.3.1 标红\n    highlightBuilder.preTags(\"<font style='color: red; font-size: inherit;'>\");\n    highlightBuilder.postTags(\"</font>\");\n    searchSourceBuilder.highlighter(highlightBuilder);\n\n    //2.4 传给searchRequest\n    searchSourceBuilder.query(builder);\n    searchRequest.source(searchSourceBuilder);\n\n    //2.5 包装好了的request给客户端返回\n    SearchResponse response = restHighLevelClient.search(searchRequest, RequestOptions.DEFAULT);\n\n    //3 处理消息回应，一个gethits是一个类，要获取这个数组需要在这个包装类里面再获取。\n    SearchHit[] hits = response.getHits().getHits();\n    List<Map> list = new ArrayList<>();\n    for (SearchHit hit : hits){\n        String json = hit.getSourceAsString();\n        Map map = JSONUtil.toBean(json, Map.class);\n        Map<String, HighlightField> highlightFields = hit.getHighlightFields();\n        if (highlightFields != null){\n            HighlightField title = highlightFields.get(\"title\");\n            HighlightField content = highlightFields.get(\"content\");\n            if (title != null){\n                //title高亮\n                Text[] titleFragments = title.getFragments();\n                String collect = Arrays.stream(titleFragments).map((value) -> value.toString()).collect(Collectors.joining());\n                map.put(\"title\",collect);\n            }\n            if (content != null){\n                //content高亮\n                Text[] contentFragments = content.getFragments();\n                String collect = Arrays.stream(contentFragments).map((value) -> value.toString()).collect(Collectors.joining());\n                map.put(\"content\",collect);\n            }\n        }\n\n        list.add(map);\n    }\n    return Result.ok(list);\n}\n```\n\n## mongodb\n\n登录用户的查询记录很多而且变化频繁。需要用非关系型数据库来存储。（其实我个人觉得没必要存在服务器上，这些缓存应该都是在客户端上保留的）\n\nmongodb的配置比较简单，甚至不用写配置类。直接看使用\n\n```java\n/**\n * 保存搜索记录\n * @param keyWord\n */\n@Async\npublic void insertSearchHistory(String keyWord){\n\n    Long userID = UserHolder.getUser().getId();\n    //1 先从db找这个数据\n    Query query = Query.query(Criteria.where(\"userId\").is(userID).and(\"keyWord\").is(keyWord));\n    QueryBlogDto searchHistory = mongoTemplate.findOne(query, QueryBlogDto.class);\n    //2 如果又就更新创建时间\n    if (searchHistory != null){\n        searchHistory.setCreateTime(new Date());\n    }\n    //3 没有就存进db\n    QueryBlogDto dto = new QueryBlogDto();\n    dto.setCreateTime(new Date());\n    dto.setUserId(userID);\n    dto.setKeyWord(keyWord);\n\n    Query query1 = Query.query(Criteria.where(\"userId\").is(userID))\n            .with(Sort.by(Sort.Direction.DESC,\"createTime\"));\n    List<QueryBlogDto> dtos = mongoTemplate.find(query1, QueryBlogDto.class);\n    //3.1 如果数量少于10条就直接存\n    if (dtos.size() < 10){\n        mongoTemplate.save(dto);\n    }\n    //3.2 如果多余10就要替换了\n    else {\n        QueryBlogDto last = dtos.get(dtos.size() - 1);\n        mongoTemplate.findAndReplace(Query.query(Criteria.where(\"keyWord\").is(last.getKeyWord())),dto);\n    }\n\n}\n\n```\n\n值得注意的是这里使用了@Async，gpt给我的答案是这个跟你新开一个线程没有区别，但是注意要在启动类上enable。这里query更像lambdaQueryWrapper那种。具体的增删改查语法以后再学。\n","tags":["Java"]},{"title":"redis项目-kafkaStream改造","url":"/2024/06/07/redis项目-kafkaStream改造/","content":"> 想到之前在黑马头条看到的流式实时更新排行，遂想用在这个项目里。\n\n目标是实时计算每一条博客的评论数，点赞数，还有浏览数收藏数来计算分数，更新在redis上，redis使用zset完成排序。\n\nTodo：其实可以分离，点赞评论这一系列数据可以放在mongodb里面，不用写在sql中，后续改造这部分内容。\n\n## HyperLogLog\n\n浏览量想用redis的这个hyperloglog实现，目前学的比较肤浅，这里只是把它当作一个有概率丢失但容量很大并且很高效的set。主要用stringRedisTemplate.opsForHyperLogLog()中的size和add，很简单前者是有多少个，后者是添加\n\n## KafkaStream实现单词统计\n\n先从配置说起，hosts与kafka本身是一致的，APPLICATION_ID_CONFIG表示的是stream的唯一标识符，Kafka Streams利用这个ID来创建一个内部的消费组，跟踪处理的偏移量，并生成应用的状态存储。\n\nCLIENT_ID_CONFIG是跟踪消费者的网络状态的客户端组\n\n多个 CLIENT_ID_CONFIG 可以对应一个 APPLICATION_ID_CONFIG，这种配置允许在一个Kafka Streams应用中，细粒度地控制和监控不同的客户端实例或任务。这对于大型、复杂的应用尤其有用，使得在监控和调试中可以精确定位问题和分析性能。\n- APPLICATION_ID_CONFIG 定义了Kafka Streams应用的全局标识，影响整个应用的行为、状态管理和数据分区处理。\n- CLIENT_ID_CONFIG 则用于标识具体的客户端实例或任务，便于监控、日志和调试。\n这种配置方式增强了应用的灵活性和可管理性，使得在复杂的流处理场景中，能够更有效地跟踪和优化每个组件的表现。\n\n\n这里我们只有很简单的应用，直接就是一对一\n```java\n@ConfigurationProperties(prefix = \"kafka\")\n@Configuration\n@EnableKafkaStreams\n@Data\npublic class KafkaStreamConfig {\n    private static final int MAX_MESSAGE_SIZE = 16*1024*1024;\n    private String hosts;\n    private String group;\n\n    @Bean(name = KafkaStreamsDefaultConfiguration.DEFAULT_STREAMS_CONFIG_BEAN_NAME)\n    public KafkaStreamsConfiguration defaultKafkaStreamsConfig(){\n        Map<String, Object> props = new HashMap<>();\n        //ip地址和端口号\n        props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, hosts);\n        //全局标识\n        props.put(StreamsConfig.APPLICATION_ID_CONFIG, this.getGroup()+\"_stream_aid\");\n        //客户端标识\n        props.put(StreamsConfig.CLIENT_ID_CONFIG, this.getGroup()+\"_stream_cid\");\n        props.put(StreamsConfig.RETRIES_CONFIG, 10);\n        //key的序列化和反序列化都为string\n        props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());\n        //value，也就是说键值对都得是String的，后面有坑点\n        props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());\n        return new KafkaStreamsConfiguration(props);\n    }\n}\n```\n\n测试用例：\n\n随机在这几种颜色里发送给kafka一个消息\n\n```java\n@Autowired\nprivate KafkaTemplate<String,String> kafkaTemplate;\n@Test\npublic void KafkaTest() throws InterruptedException {\n    String[] field = new String[]{\"red\",\"blue\",\"yellow\",\"green\",\"black\",\"white\"};\n    Random random = new Random();\n    while (true){\n        try {\n            Thread.sleep(1000);\n            String s = field[random.nextInt(5)];\n            System.out.println(s);\n            kafkaTemplate.send(\"itcast-topic-input\",s);\n        }\n        catch (Exception e){\n            e.printStackTrace();\n        }\n    }\n\n}\n```\n\n流式应用：\n\n由于流式是不具有缓存功能的，也就是说一段时间内得到的结果只能代表这一段时间。不能与历史记录累加，这个时候就要存redis了，把每一段时间的数据都累加。\n\n数据流向：测试线程一直发给itcast-topic-input主题，流式应用监听itcast-topic-input，通过处理得到键为单词，值为数量的键值对，传给itcast-topic-output主题。最后再有一个监听器把这些数据与redis进行处理和持久化。\n\n\n> 输入的数据是一个字符串，例如“apple apple tea tea”，对于这个构建的流来说，key为空，value为这个字符串，我们首先需要将这个字符串划分为四个键值对，要保留key的同时value变成各个单词（这里不能用map，map是一对一的，这里涉及拆分，需要一对多），得到了key为null，值为单词的很多键值对，\n> 对这些键值对分组，key为它的值，也就是说所有是一个单词的分在一个组里面，方便后续统计。\n> \n> 随后可以用count直接得出每个组的数量，但是我嫌太简单了，用aggregate，这个方法有三个参数。作为聚合，你需要给这个聚合器一个初始的数据类型和数据，后续再对其进行叠加，第一个函数是这个初始化的动作，第二个函数有三个参数，分别是k，v和这个叠加器的传递值。第三个函数指定序列化类型，这个我也没太搞懂\n>\n> 在这个应用中，迭代器的初始为0，由于aggregate是对每个组进行聚合，因此迭代器只用每次将初始的值+1即可。\n> \n> 最后收集这个数据进行转换，发送到对应主题给redis监听。\n```java\n@Bean\npublic KStream<String,String> kStream(StreamsBuilder streamsBuilder){\n    KStream<String, String> stream = streamsBuilder.stream(\"itcast-topic-input\");\n    stream.flatMapValues(((value)->Arrays.asList(value.split(\" \"))))\n            .groupBy((key,value)->value)\n            .windowedBy(TimeWindows.of(Duration.ofSeconds(10)))\n            .aggregate(()->\"0\",(key,value,aggValue)->{\n                Long i = Long.parseLong(aggValue)+1;\n                return i+\"\";\n            })\n            .toStream()\n            .map(new KeyValueMapper<Windowed<String>, String, KeyValue<?, ?>>() {\n                @Override\n                public KeyValue<String, String> apply(Windowed<String> key, String value) {\n                    return new KeyValue<>(key.key(),value);\n                }\n            })\n            .to(\"itcast-topic-output\");\n    return stream;\n}\n```\n\n监听器：\n\n先查找redis有没有这个key，后续就是简单的加上\n\n```java\n@KafkaListener(topics = \"itcast-topic-out\")\npublic void handleMessage(ConsumerRecord<String,String> message){\n    //先在redis里找有没有这个单词\n    Double score = stringRedisTemplate.opsForZSet().score(REDIS_WORD, message.key());\n    if (score == null){\n        stringRedisTemplate.opsForZSet().add(REDIS_WORD,message.key(), Double.parseDouble(message.value()));\n    }\n    else {\n        stringRedisTemplate.opsForZSet().add(REDIS_WORD,message.key(), score+Double.parseDouble(message.value()));\n    }\n    System.out.println(\"总计：\"+message.key()+\":\"\n            +stringRedisTemplate.opsForZSet().score(REDIS_WORD, message.key()));\n}\n```\n\n\n## KafkaStream改造项目\n\n定义两个实体类：\n- 一个是传给kafkastream的消息实体，也就是要告诉stream那一篇文章，有了什么动作，点赞了还是评论了。\n\n```java\n@Data\npublic class UpdateBlogMess {\n    /**\n     * 文章id\n     */\n    private Long id;\n\n    /**\n     * 更新类型\n     */\n    private UpdateType updateType;\n\n    /**\n     * 更新数值，点赞是1，取消点赞是-1\n     */\n    private int add;\n    public enum UpdateType{\n        VIEW,LIKED,COMMENT\n    }\n}\n```\n\n- 另一个是传出kafkastream的消息实体，输出处理后的当前片段内blog的一些统计结果\n\n```java\n@Data\npublic class BlogVisitedStreamMess {\n    /**\n     * 文章id\n     */\n    private Long id;\n    /**\n     * UV统计量\n     */\n    private Long view;\n    /**\n     * 点赞的\n     */\n    private Long liked;\n    /**\n     * 评论数\n     */\n    private Long comment;\n}\n\n```\n\n流式应用：\n\n如上所述，这里的输入是key为空，值为UpdateBlogMess的json字符串，\n1. 首先要把json反序列化，映射到key为id，值为类型和数量，例如liked:1，\n2. 再对key进行分组，很多操作的都是同一篇文章。\n3. 聚合：根据对象的操作类型在对应的未知+1，初始化是一个\"VIEW:0;COMMENT:0;LIKED:0\"，后续的聚合都在这上面分割和校验。这里只测试了点赞的，对于view这个字段我打算是直接使用HyperLogLog来统计。评论这个功能暂时没做。\n4. UV统计近一个礼拜的文章访问量。\n5. 最后返回一个key为文章id，值为BlogVisitedStreamMess的Json字符串\n\n```java\nprivate static final String KAFKA_STREAM_HOT_BLOG = \"hotBlog-produce\";\nprivate static final String KAFKA_STREAM_HOT_BLOG_CONSUMER = \"hotBlog-consumer\";\n@Resource\nprivate StringRedisTemplate stringRedisTemplate;\n\n@Bean\npublic KStream<String,String> kStream(StreamsBuilder streamsBuilder){\n    KStream<String, String> stream = streamsBuilder.stream(KAFKA_STREAM_HOT_BLOG);\n    stream.map(new KeyValueMapper<String, String, KeyValue<String , String>>() {\n        /**\n         *\n         * @param key 空\n         * @param value UpdateBlogMess的Json字符串\n         * @return key为文章id，value为类型和数量\n         */\n        @Override\n        public KeyValue<String, String> apply(String key, String value) {\n            UpdateBlogMess blogMess = JSONUtil.toBean(value, UpdateBlogMess.class);\n            //key:120312391 value:LIKED:-1\n\n            return new KeyValue<>(blogMess.getId()+\"\",blogMess.getUpdateType().name()+\":\"+blogMess.getAdd());\n        }\n    })\n            .groupBy((key,value)->key)\n            .windowedBy(TimeWindows.of(Duration.ofSeconds(10)))\n            .aggregate(new Initializer<String>() {\n                @Override\n                public String apply() {\n                    return \"VIEW:0;COMMENT:0;LIKED:0\";\n                }\n            }, new Aggregator<String, String, String>() {\n                @Override\n                public String apply(String key, String value, String aggValue) {\n                    if(StringUtils.isBlank(value)){\n                        return aggValue;\n                    }\n                    long viewNum = 0L, commentNum = 0L, likedNum = 0L;\n                    //处理aggValue\n                    String[] splits = aggValue.split(\";\");\n                    for (String split : splits) {\n                        String[] type = split.split(\":\");\n                        switch (UpdateBlogMess.UpdateType.valueOf(type[0])) {\n                            case VIEW:\n                                viewNum = Long.parseLong(type[1]);\n                                break;\n                            case LIKED:\n                                likedNum = Long.parseLong(type[1]);\n                                break;\n                            case COMMENT:\n                                commentNum = Long.parseLong(type[1]);\n                                break;\n                        }\n                    }\n                    //处理value\n                    String[] splitVal = value.split(\":\");\n                    switch (UpdateBlogMess.UpdateType.valueOf(splitVal[0])) {\n                        case VIEW:\n                            //查询一周以内的UV访问量\n                            viewNum = calculateUV(key);\n                            break;\n                        case LIKED:\n                            //如果是liked:-1就要-1\n                            likedNum += Long.parseLong(splitVal[1]);\n                            break;\n                        case COMMENT:\n                            commentNum += 1;\n                            break;\n                    }\n                    //最后返回agg\n                    return \"VIEW:\" + viewNum + \";COMMENT:\" + commentNum + \";LIKED:\" + likedNum;\n                }\n            })\n            .toStream().map((key,value)->{\n                BlogVisitedStreamMess blogVisitedStreamMess = new BlogVisitedStreamMess();\n                blogVisitedStreamMess.setId(Long.valueOf(key.key()));\n                String[] splits = value.split(\";\");\n                for (String split:splits){\n                    String[] strings = split.split(\":\");\n                    switch (UpdateBlogMess.UpdateType.valueOf(strings[0])){\n                        case LIKED:\n                            blogVisitedStreamMess.setLiked(Long.valueOf(strings[1]));\n                            break;\n                        case COMMENT:\n                            blogVisitedStreamMess.setComment(Long.parseLong(strings[1]));\n                            break;\n                        case VIEW:\n                            blogVisitedStreamMess.setView(Long.parseLong(strings[1]));\n                            break;\n                    }\n                }\n                return new KeyValue<>(key.key(),JSONUtil.toJsonStr(blogVisitedStreamMess));\n            })\n            .to(KAFKA_STREAM_HOT_BLOG_CONSUMER);\n\n    return stream;\n}\nprivate static final String HyperLogLogPrefix = \"HLL:blog:\";\n/**\n * 统计一周以内的UV访问量\n * @return\n */\npublic Long calculateUV(String key){\n    long UV4Week = 0L;\n    LocalDate now = LocalDate.now();\n    //不够优雅\n    /*String yesterday = now.minusDays(1).format(DateTimeFormatter.ofPattern(\"yyyy:MM:dd\"));\n    String theDayBeforeYesterday = now.minusDays(2).format(DateTimeFormatter.ofPattern(\"yyyy:MM:dd\"));*/\n    List<LocalDate> lastWeekDates = IntStream.rangeClosed(1, 7)\n            .mapToObj(now::minusDays)\n            .collect(Collectors.toList());\n\n    //blog的id\n    long id = Long.parseLong(key);\n    for (LocalDate localDate:lastWeekDates){\n        String date = localDate.format(DateTimeFormatter.ofPattern(\"yyyy:MM:dd\"));\n        UV4Week += stringRedisTemplate.opsForHyperLogLog().size(HyperLogLogPrefix + \":\" + id + date);\n    }\n    return UV4Week;\n}\n```\n后续监听器根据这个消息操作redis，但是由于黑马点评这个项目里查询热点文章是直接走数据库，根据likes的升序查询，所有要改很多部分。今天先介绍到这里。\n\n> 2026.6.11更新\n\n实现了用xxl-job定期统计UV然后发送给管道.\n\n这里还可以优化为scan命令，如果用key的话会在redis里全局找，开销很大。现在数据量小点还没关系，一旦大起来就效率很低了。\n\n```java\n/**\n * 使用 SCAN 命令获取与给定模式匹配的键\n *\n * @param pattern 模式，例如 \"*HLL_PREFIX*\"\n * @return 匹配的键集合\n */\npublic Set<String> scanKeys(String pattern) {\n    return stringRedisTemplate.keys(\"*\" + pattern + \"*\");\n}\n/**\n * 定时给kafka管道发view的消息，从HyperLogLog里找\n */\n@XxlJob(\"demoJobHandler\")\npublic void viewSchedule(){\n    //查找所有文章\n    Set<String> keys = scanKeys(HLL_PREFIX);\n    for (String key :keys){\n        Long size = stringRedisTemplate.opsForHyperLogLog().size(key);\n        UpdateBlogMess mess = new UpdateBlogMess();\n        mess.setUpdateType(UpdateBlogMess.UpdateType.VIEW);\n        mess.setAdd(Math.toIntExact(size));\n        mess.setId(Long.valueOf(key.split(\":\")[2]));\n        log.debug(mess.toString());\n        kafkaTemplate.send(KAFKA_STREAM_HOT_BLOG, JSONUtil.toJsonStr(mess));\n    }\n}\n\n```\n","tags":["Redis"]},{"title":"java-多线程-CompletableFuture","url":"/2024/06/04/java-多线程-CompletableFuture/","content":"> 平时多线程都是用runnable，callable，FutureTask，线程池这类完成任务。而completableFuture可以完成异步任务的编排，更具有灵活性。\n\n## 创建异步任务\n\n### supplyAsync\n\n两种方法，一种是用默认的线程池ForkJoinPool.commonPool()，另一种需要自己定义，推荐后者\n\n```java\n// 带返回值异步请求，默认线程池\npublic static <U> CompletableFuture<U> supplyAsync(Supplier<U> supplier);\n \n// 带返回值的异步请求，可以自定义线程池\npublic static <U> CompletableFuture<U> supplyAsync(Supplier<U> supplier, Executor executor);\n\n```\n\n使用方法：\n\n```java\npublic static void main(String[] args) throws ExecutionException, InterruptedException, TimeoutException {\n    ExecutorService myThreadPool = Executors.newFixedThreadPool(4);\n    //这是有线程池的，这里lambda表达式也可以携程()->1，如果没有别的逻辑的话\n    CompletableFuture<Integer> test1 = CompletableFuture.supplyAsync(()->{\n        System.out.println(Thread.currentThread().toString());\n        return 1;\n    },myThreadPool);\n    //默认方法\n    CompletableFuture<Integer> test2 = CompletableFuture.supplyAsync(()->{\n        System.out.println(Thread.currentThread().toString());\n        return 1;\n    });\n    System.out.println(test1.get(10, TimeUnit.SECONDS));\n    myThreadPool.shutdown();\n}\n```\n\n结果：\n\n```text\nThread[ForkJoinPool.commonPool-worker-25,5,main]\n1\n```\n\n### runAsync\n\n与前面的supplyAsync相比，runAsync没有返回值，就相当于是runnable，前面那个是callable。同样也有两个方法，一个是用默认线程池，另一个自己定义\n\n```java\npublic static void main(String[] args) throws ExecutionException, InterruptedException, TimeoutException {\n        ExecutorService myThreadPool = Executors.newFixedThreadPool(4);\n        CompletableFuture<Void> test1 = CompletableFuture.runAsync(()->{\n            System.out.println(Thread.currentThread());\n        });\n        System.out.println(test1.get());\n        myThreadPool.shutdown();\n    }\n```\n结果(这是默认线程)：\n\n```text\nThread[ForkJoinPool.commonPool-worker-25,5,main]\nnull\n\n```\n\n### 获取结果的方法\n\n```java\n// 如果完成则返回结果，否则就抛出具体的异常\npublic T get() throws InterruptedException, ExecutionException \n \n// 最大时间等待返回结果，否则就抛出具体异常\npublic T get(long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException\n \n// 完成时返回结果值，否则抛出unchecked异常。为了更好地符合通用函数形式的使用，如果完成此 CompletableFuture所涉及的计算引发异常，则此方法将引发unchecked异常并将底层异常作为其原因\npublic T join()\n \n// 如果完成则返回结果值（或抛出任何遇到的异常），否则返回给定的 valueIfAbsent。\npublic T getNow(T valueIfAbsent)\n \n// 如果任务没有完成，返回的值设置为给定值\npublic boolean complete(T value)\n \n// 如果任务没有完成，就抛出给定异常\npublic boolean completeExceptionally(Throwable ex) \n  \n\n```\n\n## 异步回调处理\n\n> apply就是有参数有返回值，accept就是有参数没有返回值，run就是没有参数没有返回值。async都是可以有异步线程池的\n\n### thenApply和thenApplyAsync\n\n```java\npublic static void main(String[] args) throws ExecutionException, InterruptedException, TimeoutException {\n    ExecutorService myThreadPool = Executors.newFixedThreadPool(4);\n\n    CompletableFuture<Integer> task1 = CompletableFuture.supplyAsync(()->{\n        System.out.println(\"task1\"+Thread.currentThread()+\"->\"+1);\n        return 1;\n    },myThreadPool);\n\n    CompletableFuture<Integer> task2 = task1.thenApply((result) -> {\n        System.out.println(\"task2\"+Thread.currentThread() + \"->\" + (1 + result));\n        return 1 + result;\n    });\n\n    CompletableFuture<Integer> task3 = task1.thenApplyAsync((result) -> {\n        System.out.println(\"task3\"+Thread.currentThread() + \"->\" + (1 + result));\n        return 1 + result;\n    },myThreadPool);\n\n    System.out.println(task2.get());\n\n    System.out.println(task3.get());\n\n    myThreadPool.shutdown();\n}\n```\n\n结果：\n```text\ntask1Thread[pool-1-thread-1,5,main]->1\ntask2Thread[pool-1-thread-1,5,main]->2\ntask3Thread[pool-1-thread-2,5,main]->2\n2\n2\n```\n\n这个结果挺有意思的，需要分析一下。首先是线程池里面提交的线程都是并发的，这里就体现了并发，task2和3先进行了标准输出再完成值的输出。说明并发乱序了。\n\n还有就是thenApply用的是跟前一个任务相同的线程，而带参数的thenApplyAsync用的是不一样的。\n\n**默认参数的supplyAsync和thenApply/默认thenApplyAsync**\n```java\nCompletableFuture<Integer> task1 = CompletableFuture.supplyAsync(()->{\n    System.out.println(\"task1\"+Thread.currentThread()+\"->\"+1);\n    return 1;\n});\n//这里用不加参数的thenApplyAsync也是一样的\nCompletableFuture<Integer> task2 = task1.thenApply((result) -> {\n    System.out.println(\"task2\"+Thread.currentThread() + \"->\" + (1 + result));\n    return 1 + result;\n});\n```\n\n```text\ntask1Thread[ForkJoinPool.commonPool-worker-25,5,main]->1\ntask2Thread[ForkJoinPool.commonPool-worker-25,5,main]->2\n2\n```\n\n**默认参数的supplyAsync和线程池thenApplyAsync**\n\n```java\nCompletableFuture<Integer> task1 = CompletableFuture.supplyAsync(()->{\n    System.out.println(\"task1\"+Thread.currentThread()+\"->\"+1);\n    return 1;\n});\n\nCompletableFuture<Integer> task2 = task1.thenApplyAsync((result) -> {\n    System.out.println(\"task2\"+Thread.currentThread() + \"->\" + (1 + result));\n    return 1 + result;\n},myThreadPool);\n```\n\n```text\ntask1Thread[ForkJoinPool.commonPool-worker-25,5,main]->1\ntask2Thread[pool-1-thread-1,5,main]->2\n2\n```\n\n**线程池的supplyAsync和thenApply**\n\n```java\nCompletableFuture<Integer> task1 = CompletableFuture.supplyAsync(()->{\n    System.out.println(\"task1\"+Thread.currentThread()+\"->\"+1);\n    return 1;\n},myThreadPool);\n\nCompletableFuture<Integer> task2 = task1.thenApply((result) -> {\n    System.out.println(\"task2\"+Thread.currentThread() + \"->\" + (1 + result));\n    return 1 + result;\n});\n```\n\n```text\ntask1Thread[pool-1-thread-1,5,main]->1\ntask2Thread[pool-1-thread-1,5,main]->2\n2\n```\n\n**线程池的supplyAsync和线程池thenApplyAsync**\n\n```java\nCompletableFuture<Integer> task1 = CompletableFuture.supplyAsync(()->{\n    System.out.println(\"task1\"+Thread.currentThread()+\"->\"+1);\n    return 1;\n},myThreadPool);\n\nCompletableFuture<Integer> task2 = task1.thenApplyAsync((result) -> {\n    System.out.println(\"task2\"+Thread.currentThread() + \"->\" + (1 + result));\n    return 1 + result;\n},myThreadPool);\n```\n\n```text\ntask1Thread[pool-1-thread-1,5,main]->1\ntask2Thread[pool-1-thread-2,5,main]->2\n2\n```\n\n**线程池supplyAsync和默认thenApplyAsync**\n\n```java\nCompletableFuture<Integer> task1 = CompletableFuture.supplyAsync(()->{\n    System.out.println(\"task1\"+Thread.currentThread()+\"->\"+1);\n    return 1;\n},myThreadPool);\n\nCompletableFuture<Integer> task2 = task1.thenApplyAsync((result) -> {\n    System.out.println(\"task2\"+Thread.currentThread() + \"->\" + (1 + result));\n    return 1 + result;\n});\n```\n\n```text\ntask1Thread[pool-1-thread-1,5,main]->1\ntask2Thread[ForkJoinPool.commonPool-worker-25,5,main]->2\n2\n```\n\n> **总结：**\n> \n> thenApply无论如何都会与前一个任务用相同的线程，而thenApplyAsync是重新起一个线程完成，如果没有参数，那么就会使用默认的ForkJoinPool.commonPool()\n> \n> 在后面的有async都是这个逻辑\n\n### thenAccept和thenAcceptAsync\n\n```java\nCompletableFuture<Integer> task1 = CompletableFuture.supplyAsync(()->{\n    System.out.println(\"task1\"+Thread.currentThread()+\"->\"+1);\n    return 1;\n},myThreadPool);\n\nCompletableFuture<Void> task2 = task1.thenAccept((result) -> {\n    System.out.println(\"task2\" + Thread.currentThread() + \"->\" + (1 + result));\n});\n\nCompletableFuture<Void> task3 = task1.thenAcceptAsync((result) -> {\n    System.out.println(\"task3\" + Thread.currentThread() + \"->\" + (1 + result));\n},myThreadPool);\nSystem.out.println(task2.get());\n```\n\n```text\ntask1Thread[pool-1-thread-1,5,main]->1\ntask2Thread[pool-1-thread-1,5,main]->2\ntask3Thread[pool-1-thread-2,5,main]->2\nnull\n```\n这个结果也是乱序了，而且async的与之前一致。不再赘述。\n\n### thenRun和thenRunAsync\n\n无入参数无返回值，其他都一样\n\n## 多任务\n\n### thenCombine、thenAcceptBoth 和runAfterBoth\n\ncombine就是有参数有返回值，accept就是有参数没返回值，run就是无参数无返回值\n\n```java\nCompletableFuture<Integer> task1 = CompletableFuture.supplyAsync(()->{\n    System.out.println(\"task1\"+Thread.currentThread()+\"->\"+1);\n    return 1;\n});\n\nCompletableFuture<Integer> task2 = CompletableFuture.supplyAsync(() -> {\n    System.out.println(\"task2\" + Thread.currentThread() + \"->\" + 2);\n    return 2;\n});\nCompletableFuture<Integer> task3 = task1.thenCombine(task2, (a, b) -> {\n    System.out.println(\"task3\" + Thread.currentThread() + \"->\" + (a + b));\n    return a + b;\n});\n\nSystem.out.println(task3.get());\n```\n按道理来说task3应该是跟task1一样的线程，有一次结果是一样的，不知道这里为什么是main\n```text\ntask1Thread[ForkJoinPool.commonPool-worker-25,5,main]->1\ntask2Thread[ForkJoinPool.commonPool-worker-18,5,main]->2\ntask3Thread[main,5,main]->3\n3\n```\n\n**回答：为什么默认是主线程**\n\n1. 主线程调用get方法：当主线程调用get方法时，如果task1和task2已经完成，组合任务task3会立即执行，而不需要切换到另一个线程。这样可以减少线程切换的开销，提高性能。\n\n2. 完成的线程：如果任务在某个线程中完成，且没有其他线程等待结果，那么回调任务可能在完成任务的线程中执行。这种行为由CompletableFuture的默认执行策略决定。\n\n使用thenCombineAsync可以显式指定在异步线程中执行回调任务，从而避免在主线程中执行组合任务。\n\n> 随后试了一下延长处理时间，发现还真是：\n> \n> task1Thread[pool-1-thread-1,5,main]->1\n> \n> task2Thread[pool-1-thread-2,5,main]->2\n> \n> task3Thread[pool-1-thread-1,5,main]->3\n> \n> 3\n> \n> 说明确实是处理时间短，处理时间短会由于固定的策略直接main做，减少线程开销。所以需要async异步，显示给出\n\n### applyToEither，acceptEither，runAfterEither\n\n两个里面做完一个就可以了，其他都一样\n\n```java\nCompletableFuture<Integer> task1 = CompletableFuture.supplyAsync(()->{\n    System.out.println(\"task1\"+Thread.currentThread()+\"->\"+1);\n    try {\n        Thread.sleep(100);\n    } catch (InterruptedException e) {\n        throw new RuntimeException(e);\n    }\n    return 1;\n},myThreadPool);\n\nCompletableFuture<Integer> task2 = CompletableFuture.supplyAsync(() -> {\n    System.out.println(\"task2\" + Thread.currentThread() + \"->\" + 2);\n    try {\n        Thread.sleep(10);\n    } catch (InterruptedException e) {\n        throw new RuntimeException(e);\n    }\n    return 2;\n},myThreadPool);\nCompletableFuture<Integer> task3 = task1.applyToEither(task2,(first)->{\n    System.out.println(\"task3\" + Thread.currentThread() + \"->\" + first);\n    return first;\n});\n\n\nSystem.out.println(task3.get());\n```\n\n```text\ntask1Thread[pool-1-thread-1,5,main]->1\ntask2Thread[pool-1-thread-2,5,main]->2\ntask3Thread[pool-1-thread-2,5,main]->2\n2\n```\n\n### allOf / anyOf\n\nallOf：CompletableFuture是多个任务都执行完成后才会执行，只有有一个任务执行异常，则返回的CompletableFuture执行get方法时会抛出异常，如果都是正常执行，则get返回null。\n\nanyOf ：CompletableFuture是多个任务只要有一个任务执行完成，则返回的CompletableFuture执行get方法时会抛出异常，如果都是正常执行，则get返回执行完成任务的结果。\n\n```java\nCompletableFuture<Integer> task1 = CompletableFuture.supplyAsync(()->{\n    System.out.println(\"task1\"+Thread.currentThread()+\"->\"+1);\n    try {\n        Thread.sleep(100);\n    } catch (InterruptedException e) {\n        throw new RuntimeException(e);\n    }\n    return 1;\n},myThreadPool);\n\nCompletableFuture<Integer> task2 = CompletableFuture.supplyAsync(() -> {\n    System.out.println(\"task2\" + Thread.currentThread() + \"->\" + 2);\n    try {\n        Thread.sleep(100);\n    } catch (InterruptedException e) {\n        throw new RuntimeException(e);\n    }\n    return 2;\n},myThreadPool);\nCompletableFuture<Integer> task3 = CompletableFuture.supplyAsync(()->{\n    System.out.println(\"task3\" + Thread.currentThread() + \"->\" + 3);\n    try {\n        Thread.sleep(100);\n    } catch (InterruptedException e) {\n        throw new RuntimeException(e);\n    }\n    return 3;\n},myThreadPool);\n\n\nSystem.out.println(CompletableFuture.allOf(task1,task2,task3).get());\n```\n\n```text\ntask1Thread[pool-1-thread-1,5,main]->1\ntask2Thread[pool-1-thread-2,5,main]->2\ntask3Thread[pool-1-thread-3,5,main]->3\nnull\n\n\n//下面这是task2有1/0的情况\ntask1Thread[pool-1-thread-1,5,main]->1\ntask2Thread[pool-1-thread-2,5,main]->2\ntask3Thread[pool-1-thread-3,5,main]->3\nException in thread \"main\" java.util.concurrent.ExecutionException: java.lang.ArithmeticException: / by zero\n\tat java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)\n\tat java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)\n\tat com.hmdp.service.impl.BlogServiceImpl.main(BlogServiceImpl.java:319)\nCaused by: java.lang.ArithmeticException: / by zero\n\tat com.hmdp.service.impl.BlogServiceImpl.lambda$main$4(BlogServiceImpl.java:301)\n\tat java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n```\n> anyof很简单就不演示了\n","tags":["Java"]},{"title":"redis项目-缓存和读写一致性","url":"/2024/06/01/redis项目-缓存和读写一致性/","content":"> 今天确实学到了蛮多东西的，忙里偷闲的感觉真好\n> \n> 回顾一下缓存，如同计组里面cache和内存之间的关系。在java项目中redis作为缓存，mysql就相当于内存。基本的逻辑是先找缓存，如果缓存没有命中就找mysql，然后再写到缓存中。\n> 这里还有很多可以考虑的点，写策略和调度，后续都会考虑一遍。\n\n首先是redis如何作为缓存的，很简单：\n\n```java\n@Override\n    public Result queryById(Long id) throws InterruptedException {\n        //读写锁\n        RLock lock = redissonClient.getLock(\"lock:shop:write\");\n        while (!lock.tryLock(1000,2000,TimeUnit.MILLISECONDS)){\n            Thread.sleep(100);\n        }\n        try {\n            //return Result.ok(getById(id));\n            //1.先去查redis\n            String shopJsonStr = stringRedisTemplate.opsForValue().get(CACHE_SHOP + id);\n            //2.如果redis没有就找数据库\n            if (shopJsonStr == null || shopJsonStr.isEmpty()){\n                //2.1 找数据库\n                Shop shop = getById(id);\n                //2.2 然后再写回redis里\n                String jsonString = JSONUtil.toJsonStr(shop);\n                stringRedisTemplate.opsForValue().set(CACHE_SHOP + id,jsonString);\n                //设置30s的过期时间\n                stringRedisTemplate.expire(CACHE_SHOP + id,30, TimeUnit.MINUTES);\n                return Result.ok(shop);\n            }\n            //3.如果redis有就返回\n            else {\n                Shop shop = JSONUtil.toBean(shopJsonStr, Shop.class);\n                //刷新过期时间\n                stringRedisTemplate.expire(CACHE_SHOP + id,30, TimeUnit.MINUTES);\n                return Result.ok(shop);\n            }\n        }\n        finally {\n            lock.unlock();\n        }\n\n    }\n```\n\n\n这里主要考虑双写一致性。先删后写和先写后删都会有问题，详情见原来的blog。主要有三种解决方法：\n- 延迟双删：删除->写->删除，这样可以解决第一次删除之前读操作变更redis的脏数据，这里的最后一次删除为什么要延迟，因为至少得等存数据库操作做完才行，这是异步的，一般都以业务的平均时间作为延迟时间。\n- 分布式锁：直接上读写锁，就没有这么多事情了\n- 先写后删：其实这样的概率挺低的，一种投机方法。\n\n### 延迟双删\n\n这里用一个异步线程池完成，在写数据库的时候就开一个新的线程，最后根据延迟时间删除就行了。\n\n值得注意的是这里可以用aop+注解的方式完成无侵入实现。相比前面的，后面这种的实用性更广泛。\n\n```java\n/**\n * 延时双删\n * @param shop\n * @return\n */\npublic Result updateShopDoubleDel(Shop shop) {\n    stringRedisTemplate.delete(CACHE_SHOP + shop.getId().toString());\n    //更新数据库\n    updateById(shop);\n    //开启一个新的线程延时删除\n    shopDoubleDelThreadPool.submit(new doubleDelThread(CACHE_SHOP + shop.getId().toString()));\n    return Result.ok();\n}\nprivate ExecutorService shopDoubleDelThreadPool = Executors.newFixedThreadPool(4);\n/**\n * 延迟双删\n * @param\n * @return\n */\nprivate class doubleDelThread implements Callable<Result>{\n    private String id;\n    public doubleDelThread(String id) {\n        this.id = id;\n    }\n    @Override\n    public Result call() {\n        try {\n            Thread.sleep(DELAY_TIME);\n            stringRedisTemplate.delete(id);\n            log.debug(\"延迟1秒删除\");\n            return Result.ok();\n        } catch (InterruptedException e) {\n            log.debug(\"延迟双删出错\");\n            return Result.fail(\"延迟双删出错\");\n        }\n    }\n}\n```\n\n**aop+注解**\n\n```java\n@Documented\n@Retention(RetentionPolicy.RUNTIME)\n@Target(ElementType.METHOD)\npublic @interface DelayDoubleDelete {\n    //必须填就不要写default\n    String redisKey();\n    int delayTime() default 1000;\n}\n```\n这是第一次开发注解，踩了不少坑：\n- 首先aop只能作用于接口上，未在接口中声明的成员方法是不生效的，具体的可以看这篇文章：https://blog.csdn.net/Show_line/article/details/136786252?ops_request_misc=&request_id=&biz_id=102&utm_term=aop%E5%BF%85%E9%A1%BB%E4%BD%9C%E7%94%A8%E4%BA%8E%E6%8E%A5%E5%8F%A3%E5%90%97&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-1-136786252.142^v100^pc_search_result_base1&spm=1018.2226.3001.4187\n> gpt的回答也很有意思：在 Spring 中，AOP 是通过代理对象来实现的，代理对象的创建方式有两种主要模式：JDK 动态代理和 CGLIB 代理。默认情况下，Spring 会根据目标类是否实现了接口来决定使用哪种代理机制：\n> 1. **JDK 动态代理**：如果目标类实现了一个或多个接口，Spring 会使用 JDK 动态代理。JDK 动态代理只能代理接口中的方法。\n> 2. **CGLIB 代理**：如果目标类没有实现任何接口，Spring 会使用 CGLIB 来生成目标类的子类，从而创建代理对象。CGLIB 代理可以代理类中的所有方法（包括没有在接口中声明的方法）。\n\n> 如果您在某个实现类的成员方法上使用注解但没有在接口中声明该方法，而该类实现了接口，那么默认情况下，Spring AOP 使用 JDK 动态代理，导致代理对象无法拦截实现类中没有在接口中声明的方法。这是因为 JDK 动态代理只能代理接口中的方法。\n> 1. **使用 CGLIB 代理**：明确要求 Spring 使用 CGLIB 代理。这可以通过在 Spring 配置中设置代理模式来实现。\n> 2. **确保接口中声明方法**：将需要代理的方法声明在接口中，以便 JDK 动态代理能够正常工作。\n- 其次是，注解是无法直接访问被注解方法的参数的，但是可以进行隐式处理，proceedingJoinPoint.getArgs();可以得到这个函数的参数。这里用了around，因为延迟双删除刚好是执行业务的上下\n```java\n@Aspect\n@Component\n@Slf4j\npublic class DelayDoubleDeleteAspect\n{\n    @Resource\n    private StringRedisTemplate stringRedisTemplate;\n    private ExecutorService shopDoubleDelThreadPool = Executors.newFixedThreadPool(4);\n    /**\n     * 延迟双删\n     * @param\n     * @return\n     */\n    private class doubleDelThread implements Callable<Result> {\n        private String id;\n        private int DELAY_TIME;\n        public doubleDelThread(String id,int DELAY_TIME) {\n            this.id = id;\n            this.DELAY_TIME = DELAY_TIME;\n        }\n        @Override\n        public Result call() {\n            try {\n                Thread.sleep(DELAY_TIME);\n                stringRedisTemplate.delete(id);\n                log.debug(\"延迟1秒删除\");\n                return Result.ok();\n            } catch (InterruptedException e) {\n                log.debug(\"延迟双删出错\");\n                return Result.fail(\"延迟双删出错\");\n            }\n        }\n    }\n\n    @Pointcut(\"@annotation(com.hmdp.annotation.DelayDoubleDelete)\")\n    public void pointCut(){\n\n    }\n\n    @Around(\"pointCut()\")\n    public Object aroundAdvice(ProceedingJoinPoint proceedingJoinPoint){\n        //方法签名\n        MethodSignature signature = (MethodSignature) proceedingJoinPoint.getSignature();\n        //被环绕的方法名\n        String methodName = signature.getName();\n        //方法参数\n        Object[] args = proceedingJoinPoint.getArgs();\n        Shop shop = (Shop) args[0];\n        //找到注解\n        DelayDoubleDelete annotation = AnnotationUtil.getAnnotation(signature.getMethod(), DelayDoubleDelete.class);\n        String redisKey = annotation.redisKey();\n        int delayTime = annotation.delayTime();\n        stringRedisTemplate.delete(redisKey+shop.getId());\n        //proceed用来接受业务产生的结果\n        Object proceed = null;\n        //由于最后一个删除是要业务都做完了，所以需要在之后进行线程提交\n        try {\n            //继续业务\n            proceed = proceedingJoinPoint.proceed();\n        } catch (Throwable e) {\n            throw new RuntimeException(e);\n        }\n        //最后删除\n        shopDoubleDelThreadPool.submit(new doubleDelThread(redisKey+shop.getId(), delayTime));\n        //不用修改直接返回\n        return proceed;\n    }\n}\n```\n\n### 分布式锁\n\n**写操作**\n```java\n/**\n     * 分布式锁\n     * @param shop\n     * @return\n     */\n    public Result updateShopLock(Shop shop) throws InterruptedException {\n        RLock lock = redissonClient.getLock(\"lock:shop:write\");\n        while (!lock.tryLock(1000,2000,TimeUnit.MILLISECONDS)){\n            Thread.sleep(100);\n        }\n        try {\n            //更新数据库\n            updateById(shop);\n            stringRedisTemplate.delete(CACHE_SHOP + shop.getId().toString());\n            return Result.ok(shop);\n        }\n        finally {\n            lock.unlock();\n        }\n    }\n```\n\n**读操作**\n\n```java\n@Override\npublic Result queryById(Long id) throws InterruptedException {\n    //读写锁\n    RLock lock = redissonClient.getLock(\"lock:shop:write\");\n    while (!lock.tryLock(1000,2000,TimeUnit.MILLISECONDS)){\n        Thread.sleep(100);\n    }\n    try {\n        //return Result.ok(getById(id));\n        //1.先去查redis\n        String shopJsonStr = stringRedisTemplate.opsForValue().get(CACHE_SHOP + id);\n        //2.如果redis没有就找数据库\n        if (shopJsonStr == null || shopJsonStr.isEmpty()){\n            //2.1 找数据库\n            Shop shop = getById(id);\n            //2.2 然后再写回redis里\n            String jsonString = JSONUtil.toJsonStr(shop);\n            stringRedisTemplate.opsForValue().set(CACHE_SHOP + id,jsonString);\n            //设置30s的过期时间\n            stringRedisTemplate.expire(CACHE_SHOP + id,30, TimeUnit.MINUTES);\n            return Result.ok(shop);\n        }\n        //3.如果redis有就返回\n        else {\n            Shop shop = JSONUtil.toBean(shopJsonStr, Shop.class);\n            //刷新过期时间\n            stringRedisTemplate.expire(CACHE_SHOP + id,30, TimeUnit.MINUTES);\n            return Result.ok(shop);\n        }\n    }\n    finally {\n        lock.unlock();\n    }\n\n}\n```\n","tags":["Redis"]},{"title":"redis项目-分布式锁","url":"/2024/05/30/redis项目-分布式锁/","content":"## 秒杀业务\n\n秒杀下单应该思考的内容：\n\n下单时需要判断两点：\n\n* 秒杀是否开始或结束，如果尚未开始或已经结束则无法下单\n* 库存是否充足，不足则无法下单\n\n下单核心逻辑分析：\n\n当用户开始进行下单，我们应当去查询优惠卷信息，查询到优惠卷信息，判断是否满足秒杀条件\n\n比如时间是否充足，如果时间充足，则进一步判断库存是否足够，如果两者都满足，则扣减库存，创建订单，然后返回订单id，如果有一个条件不满足则直接结束。\n\n![](../img/hmdp/img_3.png)\n\n> 最基本的逻辑，首先判断时间和库存，再进行库存扣减，如果成功扣减库存就下订单\n\n```java\n@Override\npublic Result seckillVoucher(Long voucherId) {\n    // 1.查询优惠券\n    SeckillVoucher voucher = seckillVoucherService.getById(voucherId);\n    // 2.判断秒杀是否开始\n    if (voucher.getBeginTime().isAfter(LocalDateTime.now())) {\n        // 尚未开始\n        return Result.fail(\"秒杀尚未开始！\");\n    }\n    // 3.判断秒杀是否已经结束\n    if (voucher.getEndTime().isBefore(LocalDateTime.now())) {\n        // 尚未开始\n        return Result.fail(\"秒杀已经结束！\");\n    }\n    // 4.判断库存是否充足\n    if (voucher.getStock() < 1) {\n        // 库存不足\n        return Result.fail(\"库存不足！\");\n    }\n    //5，扣减库存\n    boolean success = seckillVoucherService.update()\n            .setSql(\"stock= stock -1\")\n            .eq(\"voucher_id\", voucherId).update();\n    if (!success) {\n        //扣减库存\n        return Result.fail(\"库存不足！\");\n    }\n    //6.创建订单\n    VoucherOrder voucherOrder = new VoucherOrder();\n    // 6.1.订单id\n    long orderId = redisIdWorker.nextId(\"order\");\n    voucherOrder.setId(orderId);\n    // 6.2.用户id\n    Long userId = UserHolder.getUser().getId();\n    voucherOrder.setUserId(userId);\n    // 6.3.代金券id\n    voucherOrder.setVoucherId(voucherId);\n    save(voucherOrder);\n\n    return Result.ok(orderId);\n\n}\n```\n\n### 超卖问题\n> 多线程并发，这样的逻辑当出现最后一张的时候多个线程涌入就会变成负数，无需多言\n\n### 一人一单\n\n几个阶段：\n- 最简单逻辑，只要加上一个对于订单的查询就可，查询条件是当前的用户id和秒杀券id\n- 但是发现这样对于两个同用户的线程还是会造成不是一人一单的情况，这个时候我们需要悲观锁\n```java\n@Transactional\npublic synchronized Result createVoucherOrder(Long voucherId) {\n    ...\n}\n```\n- 首先是加在方法上，这样粒度有点大，锁的是整个方法，我们考虑细化锁，用代码块，那么这个锁的参数是什么呢\n```java\n@Transactional\npublic  Result createVoucherOrder(Long voucherId) {\n\tLong userId = UserHolder.getUser().getId();\n\tsynchronized(userId.toString().intern()){\n        ...\n    }\n}\n```\n- 对于一个用户的id肯定是可以互相区分的，我们就锁这个id，但是id的tostring方法是一个new的过程，也就是说我们用tostring都是一个新的，这个时候需要用jvm的知识了，直接在字符串常量池里面找到这个对象，这样就确保了是唯一的一个对象。\n```java\nsynchronized (user.getId().toString().intern()){\n    //代理对象是spring在初始化的时候包装在我们类上的另一个类，他会在代理对象中通过trycatch实现事务\n    //那么如果我们直接使用了proxy这个类，就可以得到事务的支持\n    //通过aop实现，但是要在启动类上暴露aop\n    IVoucherOrderService proxy = (IVoucherOrderService) AopContext.currentProxy();\n    return proxy.createVoucherOrder(voucherId);\n}\n```\n- 但是事务是在锁的外面的，也就是说有可能锁释放了但是还没有写入数据库，这样还是有隐患，所以我们需要将锁的范围比事务大，而事务是写在函数上的，那么就需要在调用这个函数的地方上锁，而不是在这个函数里面上锁。\n- 最后需要让事务生效，要用到代理对象，这样就有了一个完整的方案。\n```java\n@Service\npublic class VoucherOrderServiceImpl extends ServiceImpl<VoucherOrderMapper, VoucherOrder> implements IVoucherOrderService {\n    @Autowired\n    private ISeckillVoucherService seckillVoucherService;\n    @Autowired\n    private RedisIdWorker redisIdWorker;\n    @Autowired\n    private StringRedisTemplate stringRedisTemplate;\n\n    @Override\n    public Result seckillVoucher(Long voucherId) {\n        SeckillVoucher voucher = seckillVoucherService.getById(voucherId);\n        if (voucher.getBeginTime().isAfter(LocalDateTime.now())){\n            return Result.fail(\"秒杀尚未开始\");\n        }\n        if (voucher.getEndTime().isBefore(LocalDateTime.now())){\n            return Result.fail(\"秒杀已经结束\");\n        }\n        if (voucher.getStock() < 1){\n            return Result.fail(\"库存不足\");\n        }\n\n        //一人一单，是同一个用户的并发安全问题，所以加锁的是userId\n        UserDTO user = UserHolder.getUser();\n\n        //分布式锁\n        SimpleRedisLock lock = new SimpleRedisLock(stringRedisTemplate,\"order:\"+user.getId());\n        boolean isLock = lock.tryLock(1200);\n        //这里如果没有获取到锁，说明有一个相同用户id的人已经在下单了，所以直接返回不允许重复\n        if (!isLock){\n            return Result.fail(\"不允许重复下单\");\n        }\n        /*synchronized (user.getId().toString().intern()){\n            //代理对象是spring在初始化的时候包装在我们类上的另一个类，他会在代理对象中通过trycatch实现事务\n            //那么如果我们直接使用了proxy这个类，就可以得到事务的支持\n            //通过aop实现，但是要在启动类上暴露aop\n            IVoucherOrderService proxy = (IVoucherOrderService) AopContext.currentProxy();\n            return proxy.createVoucherOrder(voucherId);\n        }*/\n        try {\n            IVoucherOrderService proxy = (IVoucherOrderService) AopContext.currentProxy();\n            return proxy.createVoucherOrder(voucherId);\n        }\n        finally {\n            lock.unlock();\n        }\n    }\n\n    /**\n     * 事务的作用域要小于锁，否则会出现锁释放了而事务没结束，有安全隐患\n     * @param voucherId\n     * @return\n     */\n    @Transactional\n    @Override\n    public Result createVoucherOrder(Long voucherId){\n        //一人一单\n        UserDTO user = UserHolder.getUser();\n        Integer count = lambdaQuery()\n                .eq(VoucherOrder::getUserId,user.getId())\n                .eq(VoucherOrder::getVoucherId,voucherId)\n                .count();\n        if (count>0){\n            return Result.fail(\"用户已经购买过了\");\n        }\n        //更新库存\n        LambdaUpdateWrapper<SeckillVoucher> lambdaUpdateWrapper = new LambdaUpdateWrapper<>();\n        lambdaUpdateWrapper.setSql(\"stock = stock-1\")\n                .eq(SeckillVoucher::getVoucherId,voucherId)\n                .ge(SeckillVoucher::getStock,0);\n        boolean success = seckillVoucherService.update(lambdaUpdateWrapper);\n        if (!success){\n            return Result.fail(\"库存不足\");\n        }\n        //创建订单\n        long orderId = redisIdWorker.nextId(\"order\");\n        VoucherOrder voucherOrder = new VoucherOrder();\n        voucherOrder.setId(orderId);\n        voucherOrder.setVoucherId(voucherId);\n        voucherOrder.setUserId(user.getId());\n        save(voucherOrder);\n        return Result.ok(orderId);\n    }\n}\n```\n\n## 分布式锁\n\n几个阶段：\n- 分布式锁提出是为了解决锁不可见的问题，有了一个全局的锁就可以进行跨服务器的上锁\n- 主要的实现操作为setnx，也就是set if not exist为什么这个操作能当锁呢？当第一个线程setnx了，那么value就会是自己的那个，并且返回为真。另一个线程到了上锁的这一段，也setnx，此时由于他要的key已经有了另一个值，不满足not exist，所以就返回否；这个的主要原因还是在于setnx是一个原子命令，一次执行一行就能满足并发的需求\n- 第一阶段：我们就用setnx来完成，上锁过程直接将线程号作为值，解锁根据锁名称删除\n- 第二阶段：可能出现这样一种情况，线程拿到锁以后卡了，过了释放时间释放给另一个线程，此时线程1缓过来了，执行到解锁过程就把别人的给解锁了。解决方法很简单，只要判断现在这个锁里面的值是不是自己的就可以\n- 第三阶段：如果线程1在判断完了之后卡了，判断的结果确实是自己的，但是刚好超时了，这个时候又被别人抢走了，最后释放的还是别人的锁，这是因为锁不是原子性的，检查是否一致和释放要变成原子操作。需要用到lua脚本\n- 第四阶段：用到了lua脚本完成了判断\n```java\npublic class SimpleRedisLock implements ILock{\n    private StringRedisTemplate stringRedisTemplate;\n    private static final String KEY_PREFIX = \"lock:\";\n    private String name;\n    private static final String ID_PREFIX = UUID.randomUUID().toString(true) + \"-\";\n\n    public SimpleRedisLock(StringRedisTemplate stringRedisTemplate, String name) {\n        this.stringRedisTemplate = stringRedisTemplate;\n        this.name = name;\n    }\n\n    @Override\n    public boolean tryLock(long timeoutsec) {\n        //这里加uuid是因为在不同机器上的线程标识可能一致\n        String id = ID_PREFIX+Thread.currentThread().getId();\n        Boolean b = stringRedisTemplate.opsForValue()\n                .setIfAbsent(KEY_PREFIX+name, id , timeoutsec, TimeUnit.SECONDS);\n        return Boolean.TRUE.equals(b);\n    }\n    private static final DefaultRedisScript<Long> UNLOCK_SCRIPT;\n    static {\n        UNLOCK_SCRIPT = new DefaultRedisScript<>();\n        UNLOCK_SCRIPT.setLocation(new ClassPathResource(\"unlock.lua\"));\n        UNLOCK_SCRIPT.setResultType(Long.class);\n    }\n    @Override\n    public void unlock() {\n        //lua\n        stringRedisTemplate.execute(UNLOCK_SCRIPT, Collections.singletonList(KEY_PREFIX+name),ID_PREFIX+Thread.currentThread().getId());\n\n    }\n\n    /**\n     * 这是非原子操作的解锁\n     */\n    public void unlockNotSafe(){\n        //要判断是不是当前线程才能删除，否则会误删\n        String redisValue = stringRedisTemplate.opsForValue().get(KEY_PREFIX + name);\n        String id = ID_PREFIX+Thread.currentThread().getId();\n        if (id.equals(redisValue)){\n            stringRedisTemplate.delete(KEY_PREFIX+name);\n        }\n    }\n}\n\n```\n### 手写可重入分布式锁\n\n> 与reentrantLock相似，实现可重入的过程就是一个hash数据结构，小key为线程id，大key为锁名字，值为进入的次数。可重入指的是一个线程可以多次获得这个锁，每次线程进入，value都要+1，同理，出去了就要-1，要保证最后退出的时候为0.\n> \n> 这里使用lua脚本完成，第一次手写遇到了很多困难，主要原因是一个空指针异常，估计是因为我用的模板是stringRedisTemplate，有一个long类型的数据一直报错。\n\n主要思路是，首先看redis中是否有这个锁，如果没有就直接上锁。如果有的话进一步检查线程号是否是自己的，如果是自己的就+1，不是的话就直接退出。\n\n```lua\n---key[1]为大key，argv[1]为小key，argv[2]为时间\n---如果不存在大key，就直接setnx\nif redis.call('exists',KEYS[1]) == 0 then\n    --- 设置值为1\n    redis.call('hset',KEYS[1],ARGV[1],1)\n    --- 设置过期时间\n    redis.call('pexpire',KEYS[1],ARGV[2])\n    return 1\nend\n---如果没进入前面的判断，那么就是存在这个大key，要进一步判断是不是自己的key\nif redis.call('hexists',KEYS[1],ARGV[1]) == 1 then\n    redis.call('hincrby',KEYS[1],ARGV[1],1)\n    redis.call('pexpire',KEYS[1],ARGV[2])\n    return 1\nend\n--- 如果都没有出去，说明两个条件都不满足，不是自己的锁，需要等待，返回剩余的时间\nreturn 0\n```\n\n主要思路是判断是否是自己的，如果不是自己的直接退出，如果是自己的在判断是否减到0了，如果减少到0直接删除这个hash\n\n```lua\n--- 判断是否是自己的，然后在判断是否减掉之后为0，为0则删除\nif redis.call('hexists',KEYS[1],ARGV[1]) == 0 then\n    return 0\nelseif redis.call('hincrby',KEYS[1],ARGV[1],-1) == 0 then\n    redis.call('del',KEYS[1])\n    return 1\nelse\n    return 1\nend\n\n```\n改进后的代码如下所示：\n\n```java\n//可重入的分布式上锁\nprivate static final DefaultRedisScript<Boolean> REENTRANTLOCK;\nstatic {\n    REENTRANTLOCK = new DefaultRedisScript<>();\n    REENTRANTLOCK.setLocation(new ClassPathResource(\"reentrantLock.lua\"));\n    REENTRANTLOCK.setResultType(Boolean.class);\n}\n//可重入的分布式解锁\nprivate static final DefaultRedisScript<Boolean> REENTRANTUNLOCK;\nstatic {\n    REENTRANTUNLOCK = new DefaultRedisScript<>();\n    REENTRANTUNLOCK.setResultType(Boolean.class);\n    REENTRANTUNLOCK.setLocation(new ClassPathResource(\"reentrantUnlock.lua\"));\n}\n\npublic String getId() {\n    return Thread.currentThread().getId() + \":\" + ID_PREFIX;\n}\n@Override\npublic boolean tryLock(long timeoutsec) throws InterruptedException {\n    Boolean execute = stringRedisTemplate.execute(REENTRANTLOCK, Collections.singletonList(KEY_PREFIX+lockName), getId(), timeoutsec+\"\");\n    if (Boolean.TRUE.equals(execute)){\n        //开启看门狗线程\n        WatchDogThread watchDogThread = new WatchDogThread(stringRedisTemplate,KEY_PREFIX + lockName);\n        watchDogThreadThreadLocal.set(watchDogThread);\n        watchDogThread.start();\n        log.debug(\"开启看门狗进程\");\n        return true;\n    }\n    else {\n        return false;\n    }\n}\n\n@Override\npublic void unlock() {\n    long ThreadId = Thread.currentThread().getId();\n    Boolean execute = stringRedisTemplate.execute(REENTRANTUNLOCK, Collections.singletonList(KEY_PREFIX + lockName), getId());\n    if (Boolean.FALSE.equals(execute)) {\n        throw new IllegalMonitorStateException(\"解锁失败，这不是你的锁\");\n    }\n    // 停止看门狗\n    WatchDogThread watchDogThread = watchDogThreadThreadLocal.get();\n    watchDogThread.interrupt();\n    log.debug(\"停止看门狗进程\");\n    watchDogThreadThreadLocal.remove();\n\n\n}\n```\n\n### 看门狗机制\n\n开启一个线程监视ttl，如果到了某个阈值程序还没结束就续期\n\n```java\nstatic class WatchDogThread extends Thread {\n\n    private String watchKey;\n    private StringRedisTemplate stringRedisTemplate;\n\n    WatchDogThread(StringRedisTemplate redisTemplate, String key) {\n        this.stringRedisTemplate = redisTemplate;\n        this.watchKey = key;\n    }\n\n    @Override\n    public void run() {\n        while (!Thread.interrupted()) {\n            try {\n                Long expire = stringRedisTemplate.getExpire(watchKey, TimeUnit.MILLISECONDS);\n                //5秒钟续2秒\n                if (expire != null && expire.intValue() < 300) {\n                    stringRedisTemplate.expire(watchKey, 1, TimeUnit.SECONDS);\n                }\n\n            } catch (RedisSystemException e) {\n            }\n\n        }\n    }\n}\n```\n\n> 剩下的内容我觉得他课程没啥用，自己用消息队列就解决了，没啥技术含量就不写，主要是解耦的操作。\n> 改造后就变成了前面校验完全用lua脚本，保证原子性的同时也不用上锁了，因为redis是单线程。有订单的就mq，kafka处理。\n> 课程也有可取之处，BlockingQueue是我第一次见，这就是一个简易版本的消息队列，在队列为空的时候会阻塞消费者进程，满的时候会阻塞生产者进程。\n","tags":["Redis"]},{"title":"redis项目-登录逻辑","url":"/2024/05/30/redis项目-登录逻辑/","content":"> 毕业设计和各种考试耽搁了快两个月了，突然意识到再不继续学跟我的想法就会越走越远了，遂开始黑马点评的学习\n\n## 基于session实现登录\n\n**发送验证码**\n\n首先校验手机号，通过正则表达式，然后再随机生成一个长度为6的字符串，保存在session中\n\n**登录注册**\n\n如果输入的验证码和存在session中的是一样的，那么就通过校验找对应的user实体类，user为空就直接注册（也就是insert），然后将user放在session中\n\n**检验登录状态**\n\n用户在请求时候，会从cookie中携带者JsessionId到后台，后台通过JsessionId从session中拿到用户信息，如果没有session信息，则进行拦截，如果有session信息，则将用户信息保存到threadLocal中，并且放行\n\n![](../img/hmdp/img.png)\n\n### 发送验证码\n> Todo：后续可以根据Ruoyi那个图片验证码改进，但是逻辑都差不多\n```java\n@Override\npublic Result sendCode(String phone, HttpSession session) {\n    // 1.校验手机号\n    if (RegexUtils.isPhoneInvalid(phone)) {\n        // 2.如果不符合，返回错误信息\n        return Result.fail(\"手机号格式错误！\");\n    }\n    // 3.符合，生成验证码\n    String code = RandomUtil.randomNumbers(6);\n\n    // 4.保存验证码到 session\n    session.setAttribute(\"code\",code);\n    // 5.发送验证码\n    log.debug(\"发送短信验证码成功，验证码：{}\", code);\n    // 返回ok\n    return Result.ok();\n}\n```\n\n### 登录\n\n```java\n@Override\npublic Result login(LoginFormDTO loginForm, HttpSession session) {\n    // 1.校验手机号\n    String phone = loginForm.getPhone();\n    if (RegexUtils.isPhoneInvalid(phone)) {\n        // 2.如果不符合，返回错误信息\n        return Result.fail(\"手机号格式错误！\");\n    }\n    // 3.校验验证码\n    Object cacheCode = session.getAttribute(\"code\");\n    String code = loginForm.getCode();\n    if(cacheCode == null || !cacheCode.toString().equals(code)){\n         //3.不一致，报错\n        return Result.fail(\"验证码错误\");\n    }\n    //一致，根据手机号查询用户\n    User user = query().eq(\"phone\", phone).one();\n\n    //5.判断用户是否存在\n    if(user == null){\n        //不存在，则创建\n        user =  createUserWithPhone(phone);\n    }\n    //7.保存用户信息到session中\n    session.setAttribute(\"user\",user);\n\n    return Result.ok();\n}\n```\n\n### 拦截\n> 初始版本看session中是否有user对象，如果有就把他放到ThreadLocal中，方便后续调用，为什么不每次都在session中取呢？我猜是因为http的request不是随时随地哪个方法都要写的，threadlocal可以比较方便\n\n先写登录的拦截器，由于要进行处理，就跟aop一样，有一个pre有一个after，实现的是HandlerInterceptor\n\n```java\npublic class LoginInterceptor implements HandlerInterceptor {\n\n    @Override\n    public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception {\n        //1.获取session\n        HttpSession session = request.getSession();\n        //2.获取session中的用户\n        Object user = session.getAttribute(\"user\");\n        //3.判断用户是否存在\n        if(user == null){\n            //4.不存在，拦截，返回401状态码\n            response.setStatus(401);\n            return false;\n        }\n        //5.存在，保存用户信息到Threadlocal\n        UserHolder.saveUser((User)user);\n        //6.放行\n        return true;\n    }\n}\n```\n\n有了这个拦截器但是还要让他生效，被springmvc管理，需要一个配置文件\n\n```java\npackage com.hmdp.config;\n\nimport com.hmdp.interceptor.LoginInterceptor;\nimport com.hmdp.interceptor.RefreshInterceptor;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.context.annotation.Configuration;\nimport org.springframework.data.redis.core.StringRedisTemplate;\nimport org.springframework.web.servlet.config.annotation.InterceptorRegistry;\nimport org.springframework.web.servlet.config.annotation.WebMvcConfigurer;\n\nimport javax.annotation.Resource;\n\n@Configuration\npublic class MvcConfig implements WebMvcConfigurer {\n    @Autowired\n    private StringRedisTemplate stringRedisTemplate;\n    @Override\n    public void addInterceptors(InterceptorRegistry registry) {\n        // 登录拦截器\n        registry.addInterceptor(new LoginInterceptor())\n                .excludePathPatterns(\n                        \"/shop/**\",\n                        \"/voucher/**\",\n                        \"/shop-type/**\",\n                        \"/upload/**\",\n                        \"/blog/hot\",\n                        \"/user/code\",\n                        \"/user/login\"\n                ).order(1);\n        // token刷新的拦截器\n        registry.addInterceptor(new RefreshInterceptor(stringRedisTemplate)).addPathPatterns(\"/**\").order(0);\n    }\n}\n```\n\nWebMvcConfigurer配置类其实是Spring内部的一种配置方式，采用JavaBean的形式来代替传统的xml配置文件形式进行针对框架个性化定制，可以自定义一些Handler，Interceptor，ViewResolver，MessageConverter。基于java-based方式的spring mvc配置，需要创建一个配置类并实现WebMvcConfigurer 接口；\n\n常用的方法：\n\n**addInterceptors：拦截器**\n\n- addInterceptor：需要一个实现HandlerInterceptor接口的拦截器实例\n- addPathPatterns：用于设置拦截器的过滤路径规则；addPathPatterns(\"/**\")对所有请求都拦截\n- excludePathPatterns：用于设置不需要拦截的过滤规则\n- 拦截器主要用途：进行用户登录状态的拦截，日志的拦截等。\n\n**addViewControllers：页面跳转**\n\n拦截到一个路径就跳转到对应的页面\n\n```java\n@Override\npublic void addViewControllers(ViewControllerRegistry registry) {\n    registry.addViewController(\"/toLogin\").setViewName(\"login\");\n}\n```\n\n这个方法意思估计就是当/toLogin\"路径的时候跳转到login页面\n\n> Todo：以后慢慢补充\n\n## Redis代替session的业务\n> code和user都存在session中，而session是本地的，在集群模式下会失效，所以需要一个全局的解决方案。\n\n基本的解决思路就是把验证码和user都存在redis里面，设定过期时间，拦截器变成续期即可。有一个问题就是以什么数据结构来存。code可以以string类型来存储。\nuser其实也可以，我这里原本想的是用JSON存，但是不够直观，而且存储效率也没有hash好，所以还是跟着他用了hash。\n\n![](../img/hmdp/img_1.png)\n\n```java\n@Service\n@Slf4j\npublic class UserServiceImpl extends ServiceImpl<UserMapper, User> implements IUserService {\n    @Autowired\n    private StringRedisTemplate redisTemplate;\n\n    @Override\n    public Result sendCode(String phone, HttpSession session) {\n        if (RegexUtils.isPhoneInvalid(phone)){\n            return Result.fail(\"手机号格式错误\");\n        }\n        String code = RandomUtil.randomNumbers(6);\n        //这里用Redis完成\n        //session.setAttribute(\"code\",code);\n        redisTemplate.opsForValue().set(redisConstants.LOGINREDISCODE + phone,code, 60,TimeUnit.SECONDS);\n        log.debug(\"验证码为：\"+code);\n        return Result.ok();\n    }\n\n    @Override\n    public Result login(LoginFormDTO loginForm, HttpSession session) {\n        if (RegexUtils.isPhoneInvalid(loginForm.getPhone())||RegexUtils.isCodeInvalid(loginForm.getCode())){\n            return Result.fail(\"手机号格式错误\");\n        }\n        //这里改用Redis\n        //String sessionCode = session.getAttribute(\"code\").toString();\n        String code = redisTemplate.opsForValue().get(redisConstants.LOGINREDISCODE + loginForm.getPhone());\n        if (code == null || !loginForm.getCode().equals(code)){\n            return Result.fail(\"验证码有误\");\n        }\n        LambdaQueryWrapper<User> lambdaQueryWrapper = new LambdaQueryWrapper<>();\n        lambdaQueryWrapper.eq(User::getPhone,loginForm.getPhone());\n        User user = getOne(lambdaQueryWrapper);\n        if (user == null){\n            User temp = new User();\n            temp.setPhone(loginForm.getPhone());\n            save(temp);\n            user = temp;\n        }\n        String token = UUID.randomUUID(true).toString();\n        //用json存储\n        //String jsonStr = JSONUtil.toJsonStr(user);\n        //用hash存储\n        UserDTO userDTO = BeanUtil.copyProperties(user, UserDTO.class);\n        Map<String, Object> userMap = BeanUtil.beanToMap(userDTO, new HashMap<>(),\n                CopyOptions.create()\n                        .setIgnoreNullValue(true)\n                        .setFieldValueEditor((fieldName, fieldValue) -> fieldValue.toString()));\n\n        redisTemplate.opsForHash().putAll(redisConstants.LOGINUSER+token,userMap);\n        redisTemplate.expire(redisConstants.LOGINUSER,30,TimeUnit.MINUTES);\n        return Result.ok(token);\n    }\n}\n```\n\n> 主要就是用了redisTemplate的两种方法，string类型的就用opsForValue，hash用opsForHash，值得注意的是这里putAll是将一个map全部存进去，只有两个参数，而put可能指的是**在这个key下的map中的其中一行**，也就是说有三个参数，key，name和value。\n> \n> 这里的beanToMap主要记住setFieldValueEditor是编辑域的，那当然有两个参数，修改对应fieldName下的fieldValue\n> \n> 还有一个是setFieldNameEditor编辑name的，比如你想让name变成大写，就用UpperCase\n\n## 解决登录刷新问题\n> 我们之前的拦截器会排除一些路径进行刷新，但是我们要这些路径被访问的时候也要刷新，所以选择了连个拦截器的方案，其中第一个完成所有redis的续期，后面一个延续拦截指定路径校验登录状态。\n\n![](../img/hmdp/img_2.png)\n\n```java\n@Slf4j\npublic class RefreshInterceptor implements HandlerInterceptor {\n    private StringRedisTemplate stringRedisTemplate;\n    public RefreshInterceptor(StringRedisTemplate stringRedisTemplate) {\n        this.stringRedisTemplate = stringRedisTemplate;\n    }\n\n    @Override\n\n    public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception {\n        //1.检验是否有token\n        String token = request.getHeader(\"authorization\");\n        if (token == null || token.isEmpty()){\n            //1.1 如果没有token直接放行给下一个拦截器，这样肯定会被拦截\n            return true;\n        }\n        //2.检验是否在redis中\n        //entries得到的是一个map，get查具体的，所以有两个参数key和fieldName\n        Map<Object, Object> objectMap = stringRedisTemplate.opsForHash().entries(redisConstants.LOGINUSER + token);\n        if (objectMap.isEmpty()){\n            return true;\n        }\n        UserDTO dto = BeanUtil.mapToBean(objectMap, UserDTO.class, CopyOptions.create());\n        //3.保存在threadLocal中\n\n        //UserDTO userDTO = BeanUtil.fillBeanWithMap(objectMap, new UserDTO(), false);\n        log.debug(dto.toString());\n        UserHolder.saveUser(dto);\n        //4.刷新时间\n        stringRedisTemplate.expire(redisConstants.LOGINUSER + token,30, TimeUnit.MINUTES);\n        return true;\n    }\n    @Override\n    public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception {\n        // 移除用户\n        UserHolder.removeUser();\n    }\n}\n\n```\n\n```java\n@Slf4j\npublic class LoginInterceptor implements HandlerInterceptor {\n    @Override\n    public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception {\n        UserDTO user = UserHolder.getUser();\n        if (user == null){\n            response.setStatus(401);\n            return false;\n        }\n        log.debug(\"当前用户:\"+user.getNickName());\n        return true;\n    }\n}\n```\n","tags":["Redis"]},{"title":"java-虚拟机（上）","url":"/2024/04/08/java-虚拟机（上）/","content":"# JVM组成\n按照大的来分可以分为三部分：\n1. 类加载器，由于java是纯面向对象语言，类加载器会把java类转换成成字节码\n2. 运行时数据区（内存分区）：再细分可以分为共享的方法区和堆，线程不共享的虚拟机栈和本地方法栈，还有每个线程的指针\n3. 执行引擎：将中间代码转换成机器指令（x86，arm等）\n4. 本地库接口\n\n# 运行时数据区详细介绍\n- 堆（线程共享）\n- 方法区（线程共享）\n- 程序计数器（线程独占）\n- 虚拟机栈（线程独占）\n- 本地方法栈（线程独占）\n## 程序计数器\n就如同cpu中的pc，虚拟机中也有对应代码的pc，为了实现并发，每一个线程程序运行到哪里都不一样，也就需要保存，恢复上下文也方便。所以在jvm中程序计数器是私有的。\n\n注意：pc是唯一一个不会发生OOM的内存区域，估计是因为本来就放一个指针，再怎么样也超不出去。生存周期，随着线程的创建而创建，随着线程死亡而死亡。\n\n## 虚拟机栈\n这里的虚拟机栈跟真实的用c语言编译的栈类似，都是存函数调用的，有返回地址，局部变量，操作数，这里还有一个**动态链接**\n- 局部变量表：主要存放了编译期可知的各种数据类型（boolean、byte、char、short、int、float、long、double）、对象引用（reference 类型，它不同于对象本身，可能是一个指向对象起始地址的引用指针，也可能是指向一个代表对象的句柄或其他与此对象相关的位置）。\n- 操作数栈：主要是作为方法调用的中间站，比如addi a,b,a（估计不是这么写的，反正就是a+b计算出来的值再赋给a），存放的那个临时变量就a+b就放在操作数栈\n- 动态链接：运行到一定位置的时候可能会需要调用其他的类或者方法，这个时候就要把**符号引用转换为调用方法的直接引用**，因为再编译的时候都是用的常量池，在常量池引用的，一层套一层，这个时候就要把最核心的那个函数给拿出来，变成直接引用\n虚拟机栈超出会报错的，栈帧数量不能多于一个值。比如无限递归，最后报错报的是栈溢出，而不是堆溢出，因为在爆堆之前就已经爆栈\n\n## 本地方法栈\n这个跟虚拟机栈很像，但是虚拟机栈是为了java语句服务的，本地方法栈是使用到的本地native方法服务\n\n## 堆\n> 注意，我们在这里说的这五个其实是逻辑部分，就好像计算机组成原理中那五个部分一样，但是实际cpu又是控制器和运算器组合的。\n> \n> 这里也是类似，可以理解为这是jvm的逻辑设计图，具体实现的比如jdk1.7的持久代和1.8的元空间，其实也只是方法区的一个实现罢了\n\n## 持久代和元空间\n\n最大的一块，jdk1.7主要是三块，新生代，老年代和持久代，1.8把持久代取消了，取而代之的是元空间\n\n个人理解，本来在堆中就完成了方法区的设计，在持久区放静态变量，代码块和编译好的代码，但是由于可能会出现oom，以及越来越多的动态类，时的很容易爆堆，这个时候不如把它提出虚拟机吧！放在实际内存下面，这样就有更广阔的空间给你爆了。\n\n于是元空间这个概念就出来了，但是也不是无限扩大，有大小限制的\n\n**为什么使用元空间？**\n- 1）由于 PermGen 内存经常会溢出，引发OutOfMemoryError，因此 JVM 的开发者希望这一块内存可以更灵活地被管理，不要再经常出现这样的 OOM。\n- 2）移除 PermGen 可以促进 HotSpot JVM 与 JRockit VM 的融合，因为 JRockit 没有永久代。\n- 3）减轻gc的负担，放在外面的元空间可以不用gc\n\n准确来说，Perm 区中的字符串常量池被移到了堆内存中是在 Java7 之后，Java 8 时，PermGen 被元空间代替，其他内容比如**类元信息、字段、静态属性、方法、常量**等都移动到元空间区。比如 java/lang/Object 类元信息、静态属性 System.out、整型常量等。\n\n元空间的本质和永久代类似，都是对 JVM 规范中方法区的实现。不过元空间与永久代之间最大的区别在于：元空间并不在虚拟机中，而是使用本地内存。因此，默认情况下，元空间的大小仅受本地内存限制。\n\n## 新生代和老年代\n\n- 年轻代被划分为三部分，Eden区和两个大小严格相同的Survivor区，根据JVM的策略，在经过几次垃圾收集后，任然存活于Survivor的对象将被移动到老年代区间。\n- 老年代主要保存生命周期长的对象，一般是一些老的对象\n\n这就涉及到后面的垃圾回收算法了\n\n## 方法区\n一般来存静态变量，常量以及编译好的代码\n\n> 这里要注意，既然方法区里面有常量，那也会有运行时常量池。这里要区分**字符串常量池**，jdk1.7之前，字符串常量池是跟持久代放在一起的，是持久代的一个组成部分，1.7之后，字符串常量池就放在堆空间里了，也就是说提出来了，直到现在也还是在堆空间\n> \n> **为什么？**\n> \n> 主要是因为永久代（方法区实现）的 GC 回收效率太低，只有在整堆收集 (Full GC)的时候才会被执行 GC。Java 程序中通常会有大量的被创建的字符串等待回收，将字符串常量池放到堆中，能够更高效及时地回收字符串内存。\n> \n> 而运行时常量池是放在方法区的，也就是元空间\n\n# 类加载器与双亲委派模型\n> 这里的加载有点像dns的递归查找\n- bootstrapClassLoader：根加载器，每个都会从它开始\n- ExtClassLoader：扩展功能的一些jar包里面的类\n- AppClassLoader：应用类加载器\n- 自定义，可以重写方法\n\n**双亲委派模型**\n\n如果一个类加载器在接到加载类的请求时，它首先不会自己尝试去加载这个类，而是把这个请求任务委托给父类加载器去完成，依次递归，如果父类加载器可以完成类加载任务，就返回成功；只有父类加载器无法完成此加载任务时，才由下一级去加载。 \n\n双亲委派模型保证了 Java 程序的稳定运行，可以避免类的重复加载（JVM 区分不同类的方式不仅仅根据类名，相同的类文件被不同的类加载器加载产生的是两个不同的类），也保证了 Java 的核心 API 不被篡改。如果没有使用双亲委派模型，而是每个类加载器加载自己的话就会出现一些问题，比如我们编写一个称为 java.lang.Object 类的话，那么程序运行的时候，系统就会出现两个不同的 Object 类。双亲委派模型可以保证加载的是 JRE 里的那个 Object 类，而不是你写的 Object 类。这是因为 AppClassLoader 在加载你的 Object 类时，会委托给 ExtClassLoader 去加载，而 ExtClassLoader 又会委托给 BootstrapClassLoader，BootstrapClassLoader 发现自己已经加载过了 Object 类，会直接返回，不会去加载你写的 Object 类\n\n\n好处：\n- 防止核心库被篡改\n- 不重复加载类\n\n# 打破双亲委派\ntomcat，重写了loadClass方法\n","tags":["Java"]},{"title":"java-多线程-ThreadLocal和线程池","url":"/2024/04/07/java-多线程（中/","content":"# ThreadLocal\n\n通常情况下，我们创建的变量是可以被任何一个线程访问并修改的。如果想实现每一个线程都有自己的专属本地变量该如何解决呢？\n\nJDK 中自带的ThreadLocal类正是为了解决这样的问题。 ThreadLocal类主要解决的就是让每个线程绑定自己的值，可以将ThreadLocal类形象的比喻成存放数据的盒子，盒子中可以存储每个线程的私有数据。\n\n如果你创建了一个ThreadLocal变量，那么访问这个变量的每个线程都会有这个变量的本地副本，这也是ThreadLocal变量名的由来。他们可以使用 get() 和 set() 方法来获取默认值或将其值更改为当前线程所存的副本的值，从而避免了线程安全问题。\n\n再举个简单的例子：两个人去宝屋收集宝物，这两个共用一个袋子的话肯定会产生争执，但是给他们两个人每个人分配一个袋子的话就不会出现这样的问题。如果把这两个人比作线程的话，那么 ThreadLocal 就是用来避免这两个线程竞争的。\n\n## 数据结构\n\n![](../img/Java/img_5.png)\n\n其实不是ThreadLocal有这个数据结构，是Thread持有的，有一个ThreadLocalMap的数组，专门放键值对，K为ThreadLocal的类对象，V为ThreadLocal泛型的数据。\n\n也就是说，在一个线程中，如果有多个ThreadLocal，查找Map键为这个ThreadLocal变量，就可以很轻松拿到存的值。\n\n```java\npublic void set(T value) {\n    //获取当前请求的线程\n    Thread t = Thread.currentThread();\n    //取出 Thread 类内部的 threadLocals 变量(哈希表结构)\n    ThreadLocalMap map = getMap(t);\n    if (map != null)\n        // 将需要存储的值放入到这个哈希表中\n        map.set(this, value);\n    else\n        createMap(t, value);\n}\nThreadLocalMap getMap(Thread t) {\n    return t.threadLocals;\n}\n```\n## 内存泄露问题\n\n这个Map里面，Key是弱引用的，也就是说每次gc都会回收key，而value是强引用的。这个时候就会出现，key被gc回收了为null，value还有的情况。这个时候就会产生内存泄露。\n\n解决方法：释放的时候手动remove。\n\n# 线程池的四个种类\n1. newCachedThreadPool创建一个可缓存的线程池，默认阻塞队列是SynchronousQueue\n2. newFixedThreadPool创建一个定长的线程池，默认阻塞队列是LinkedBlockingQueue\n3. newSingleThreadExecutor创建一个单例线程，默认也是LinkedBlockingQueue\n4. newScheduled创建一个可以设置定时任务的线程\n\n# 线程池的核心参数\n\n除了上面四种封装好的，还可以自己创建\n\n```java\n    /**\n     * 用给定的初始参数创建一个新的ThreadPoolExecutor。\n     */\n    public ThreadPoolExecutor(int corePoolSize,//线程池的核心线程数量\n                              int maximumPoolSize,//线程池的最大线程数\n                              long keepAliveTime,//当线程数大于核心线程数时，多余的空闲线程存活的最长时间\n                              TimeUnit unit,//时间单位\n                              BlockingQueue<Runnable> workQueue,//任务队列，用来储存等待执行任务的队列\n                              ThreadFactory threadFactory,//线程工厂，用来创建线程，一般默认即可\n                              RejectedExecutionHandler handler//拒绝策略，当提交的任务过多而不能及时处理时，我们可以定制策略来处理任务\n                               ) {\n        if (corePoolSize < 0 ||\n            maximumPoolSize <= 0 ||\n            maximumPoolSize < corePoolSize ||\n            keepAliveTime < 0)\n            throw new IllegalArgumentException();\n        if (workQueue == null || threadFactory == null || handler == null)\n            throw new NullPointerException();\n        this.corePoolSize = corePoolSize;\n        this.maximumPoolSize = maximumPoolSize;\n        this.workQueue = workQueue;\n        this.keepAliveTime = unit.toNanos(keepAliveTime);\n        this.threadFactory = threadFactory;\n        this.handler = handler;\n    }\n```\n\n七个核心参数：\n1. 核心线程数量\n2. 最大线程数量\n3. 过期时间：如果线程池的线程数量大于核心线程数量，如果没有新的任务提交，那么已经到期的线程不会立刻销毁，而是等一段时间销毁\n4. 过期时间单位：可以是秒，毫秒\n5. 阻塞队列：刚刚提的那些，后面还会说\n6. 饱和缩略\n7. 线程工厂类：一般都是默认的，可以定制线程对象的创建，例如设置线程名字、是否是守护线程等\n\n# 饱和策略\n当阻塞队列满了，而且最大线程数量也满了，就会触发饱和策略\n1. 抛出异常，不让加了\n2. 线程不走线程池，提交线程的那个线程自己来运行\n3. 不报错，直接丢弃\n4. 丢弃队列最前面那个，然后加进队列\n\n# 如何确定线程数量\n- io密集型：2n+1\n- cpu密集型：n+1\n这里的n都是当前机器的虚拟内核数量，io密集型主要都是io时间多，对于cpu负载并不大\n","tags":["Java"]},{"title":"java-多线程-volatile，synchronized和lock","url":"/2024/04/07/zjava-多线程-上/","content":"> 由于东西很多很乱，就按照Q&A的方式整理以下\n\n# 进程与线程的区别\n\n- 调度方面：在传统计算机中，进程是调度的基本单位，但是在引入线程之后，线程是调度的基本单位。同一个进程下的线程切换不会引起进程切换，但是不同的就会影响\n- 拥有资源方面：进程是拥有资源的单位，线程之间共享内存空间\n- 切换方面：进程切换要保存上下文程序计数器很多东西，而线程切换只用保存一些寄存器。开销远小于进程\n- 并发性：同一个进程下的线程可以并发，不同进程下的线程也可以并发\n\n# volatile关键字\nvolatile是不稳定的意思，不止用于java，c++也有使用，声明了这个关键字的变量会禁用缓存，每一次都从内存找最新的值，这样就能保证可见性，但是不会保证原子性，例如下面这段代码\n```java\npublic class VolatileAtomicityDemo {\n    public volatile static int inc = 0;\n\n    public void increase() {\n        inc++;\n    }\n\n    public static void main(String[] args) throws InterruptedException {\n        ExecutorService threadPool = Executors.newFixedThreadPool(5);\n        VolatileAtomicityDemo volatileAtomicityDemo = new VolatileAtomicityDemo();\n        for (int i = 0; i < 5; i++) {\n            threadPool.execute(() -> {\n                for (int j = 0; j < 500; j++) {\n                    volatileAtomicityDemo.increase();\n                }\n            });\n        }\n        // 等待1.5秒，保证上面程序执行完成\n        Thread.sleep(1500);\n        System.out.println(inc);\n        threadPool.shutdown();\n    }\n}\n```\n这里得不到结果的主要原因是，inc++不是一个原子操作，其实是由三个操作并成，先读取inc，再++，最后写回，可能两个线程同时都读到inc为100，这个时候再同时更改写回内存，这样就会达不到预期，所以volatile关键之**只能保证可见性，不能保证原子性**\n\n如果要保证原子性，可以使用：\n- synchronized关键字上锁increase方法\n- 既然可以用synchronized方法，那肯定也可以用ReentrantLock，关于这两者的区别后文再说\n- ReentrantLock基于CAS和AQS，那肯定用CAS的原子方法也可以解决，后文再说\n\n# 乐观锁和悲观锁\n- 悲观锁：悲观的觉得临界区一定会有人来竞争，所以每次访问的时候一定要上锁，synchronized和ReentrantLock就是比较典型的悲观锁\n- 乐观锁：很投机的觉得，这段代码可能不会有别的线程访问，就算有我也不上锁，用其他的方法，例如队列，链表的方式，比较典型的就是CAS（其实我觉得MVCC多少也有点这种思想）\n最大的区别就是锁的粒度不一样，悲观锁由于每一次都要上锁，肯定效率没有乐观锁高，反而言之，如果频繁发生冲突，乐观锁的效率也是很低的。总而言之，在读多写少的场景，可以使用乐观锁，写多读少的情况下，使用悲观锁。\n\n# CAS和自旋锁\n## CAS\ncompare and swap，比较并且交换，主要思想是给出三个数据：要修改的数据的更新值，这个要修改的数据的预期值，要修改数据的实际值。什么意思呢，例如我们要把一个等于1的数据修改成6，我对于这个要修改数据的预期值就是1，当且仅当数据是1的时候，我才会把它修改成6.\n这个思想怎么在并发的环境下体现呢？比如这个时候两个线程都要修改这个数据，有一个别的线程抢占先机，由于这个是由volatile修饰的，另一边立即会就看到修改成了另外一个值，不再是预期值1，这个时候cas就会失效，从而保证了并发。\n\n**为什么这样就能作为锁呢？**\n\n理由很简单，cas**保证原子性**，为什么他敢理直气壮的说自己是原子的。这就要涉及到一个神奇的类Unsafe，在实现java虚拟机的时候，由于jvm的隔离性，使得java很难接近底层操作系统，Unsafe这个类里面全都是native方法，也就是用的本地方法，对于cas而言，这个底层的方法不是java实现的，是由Unsafe调用dll动态库，也就是c++实现的，cas这条操作可以理解成汇编的一个原子指令，既然是一条指令，那么肯定是可以保证原子性的。\n\n那么问题来了，为什么不直接弄一条锁的原子指令，而这么拐弯抹角的用到这个语句间接完成乐观锁，小编也不知道\n\n## 自旋锁\n\n我们有了这个CAS操作，到底怎么用到锁上面呢？我们先看一个用CAS的例子：\n\nAtomicInteger中调用unsafe进行自增操作的源码中的do-while循环就是一个自旋操作，如果修改数值失败则通过循环来执行自旋，直至修改成功。AtomicInteger是一个操作Integer类型的原子操作类，这里的getAndAddInt可以保证线程安全，获取Object对象在内存偏移量，然后在+i\n```java\npublic final int getAndAddInt (Object var1, Long var2, int var4) {\n\tint var5;\n    //自循环的思想，每一次循环都找Object的偏移量为var2的那个值，这个获得的就是预期值，\n    //循环条件是如果跟他一样就断开，如果不一样，不阻塞，不释放CPU，但是也达到了锁的功能\n    //能加就加，加不上就一只循环一直尝试\n\tdo {\n\t\tvar5 = this.getIntVolatile(var1, var2);\n\t} while( !this.compareAndSwapInt(var1, var2, var5, var5 + var4));\n\t\n\treturn var5;\n}\n\n\n```\n主要思想是加的上就加，加不上一直尝试，即使没有加锁，但是也达到了锁的功能。\n\n这种思想称为**自旋**，我们把这种dowhile的思想运用到类上就是**自旋锁**，我猜取这个名字的原因是因为一直循环，很像自己一个人在转\n```java\npublic class SpinLock {\n    private AtomicReference<Thread> owner = new AtomicReference<Thread>();\n\n    public void lock() {\n        Thread currentThread = Thread.currentThread();\n        // 如果锁未被占用，则设置当前线程为锁的拥有者\n        while (!owner.compareAndSet(null, currentThread)) {\n        }\n    }\n\n    public void unlock() {\n        Thread currentThread = Thread.currentThread();\n        // 只有锁的拥有者才能释放锁\n        owner.compareAndSet(currentThread, null);\n    }\n}\n```\nAtomicReference是一个原子类，提供的compareAndSet方法封装了CAS方法，参数中前面是预期值，后面是要修改的值，如果预期值和实际值是一样的，那么就设置为修改值。\n\n上锁过程：owner这里空参数构造，所以我的预期值是空，如果这个时候只有一个线程，那么没有人上锁肯定也是空，所以实际值也是空，满足条件，那么就把空设置为当前线程。如果有两个线程，都同时要上锁，先进来的那个上了锁，下一个compareAndSet方法肯定是假，就会一直while，不会释放cpu\n\n解锁过程：预期值为当前线程，这个线程刚刚上锁了，所以实际值肯定存的也是这个，最后满足条件就把锁设置为null，此时第二个线程终于不用自旋了，就会获取锁。\n\n通过这个过程也看到了，自旋的优点和缺点都很明显，优点是如果时间短的话，可以免去线程的上下文切换时间；但是如果时间长，那就是很消耗cpu的，一直不释放，这个缺点有改进措施，可以用**自适应自旋锁**，也就说，不再是无限循环，可以设置一个最大循环次数，就不会导致死循环的情况。\n\n## ABA问题\nCAS存在一个问题，判断实际值和预期值相同，其实不一定就能说明没改过，有可能预期值是A，实际值从A变成了B再变成了A，这个问题其实蛮严重的，如果是变量还好，如果在链表中，判断头节点没变化，不代表接下来的节点没有变化，就会导致完全不一样的结果。\n\n如何避免？\n\n加时间戳，上文用了AtomicReference，这回用AtomicStampedReference，这里使用了时间戳，只有当时间戳相同并且实际值等于预期值，才可以。\n\n# Synchronized关键字\n\n在 Java 早期版本中，synchronized 属于 重量级锁，效率低下。这是因为监视器锁（monitor）是依赖于底层的操作系统的 Mutex Lock 来实现的，Java 的线程是映射到操作系统的原生线程之上的。如果要挂起或者唤醒一个线程，都需要操作系统帮忙完成，而操作系统实现线程之间的切换时需要从用户态转换到内核态，这个状态之间的转换需要相对比较长的时间，时间成本相对较高。\n\n不过，在 Java 6 之后， synchronized 引入了大量的优化如自旋锁、适应性自旋锁、锁消除、锁粗化、偏向锁、轻量级锁等技术来减少锁操作的开销，这些优化让 synchronized 锁的效率提升了很多。因此， synchronized 还是可以在实际项目中使用的，像 JDK 源码、很多开源框架都大量使用了 synchronized 。\n\n关于偏向锁多补充一点：由于偏向锁增加了 JVM 的复杂性，同时也并没有为所有应用都带来性能提升。因此，在 JDK15 中，偏向锁被默认关闭（仍然可以使用 -XX:+UseBiasedLocking 启用偏向锁），在 JDK18 中，偏向锁已经被彻底废弃（无法通过命令行打开）。\n\n## 如何使用Synchronized关键字？\n1. 修饰方法\n2. 修饰静态方法（锁住当前类）：注意修饰静态方法是没有互斥性的，因为静态方法是放在进程中静态代码块那一部分的，所有线程共享，每一个线程都可以访问\n3. 修饰代码块：\n- synchronized(object) 表示进入同步代码库前要获得 给定对象的锁。\n- synchronized(类.class) 表示进入同步代码前要获得 给定 Class 的锁\n\n总结：\n- synchronized 关键字加到 static 静态方法和 synchronized(class) 代码块上都是是给 Class 类上锁；\n- synchronized 关键字加到实例方法上是给对象实例上锁；\n- 尽量不要使用 synchronized(String a) 因为 JVM 中，字符串常量池具有缓存功能。\n\n## 构造方法可以被Synchronized修饰吗》\n不可以，因为本身构造方法就已经是线程安全的了\n\n## Synchronized底层原理\n\n基于jvm的monitor,在java编译的时候会给加锁代码加上monitorenter和两个monitorexit，在这中间的就是互斥的，为什么要有两个呢，线程有的时候会抛出异常或者错误，这个时候就会走第二个exit，用两个是为了保证一定能退出。\n\nmonitor也是基于c++的，分为三个部分，一个是当前获取锁的对象，还有一个是等待队列，一个是阻塞队列。\n\n上锁成功的标志是monitor的拥有者为当前线程，当释放的时候会唤醒阻塞进程，有一点要说明的是这个过程是非公平，也就是运行不在队列里的新进程也来抢。\n\n## Synchronized锁升级原理\n\n> 这一块暂时还没有细细研究。\n\n有四种锁等级，锁的重量依次递增：\n1. 不加锁\n2. 加偏向锁：一段很长的时间内都只被一个线程使用锁，可以使用了偏向锁，在第一次获得锁时，会有一个CAS操作，之后该线程再获取锁，只需要判断mark word中是否是自己的线程id即可，而不是开销相对较大的CAS命令。这也是某种投机行为，乐观锁思想，在很少有进程竞争的时候比较节约资源\n3. 加轻度锁：也就是自旋锁等，每次操作都是cas，还是乐观锁思想，这个就比偏向锁稍微控制了一点\n4. 重度锁，使用monitor那种悲观锁\n\nSynchronized发生竞争会依次升级锁，高并发下变成悲观锁\n\n## Synchronized和volatile什么区别\n\n1. Synchronized修饰方法，静态方法，代码块等，而volatile修饰变量\n2. volatile是线程同步的轻量实现，具有可见性但是没有原子性，Synchronized可以保证可见性和原子性\n3. volatile性能肯定比Synchronized好，但是主要解决的是变量的问题，Synchronized解决方法的同步和互斥。\n\n# ReentrantLock\n\n可重入锁，实现了Lock接口，相比Synchronized功能更多，实现了轮询，超时，中断，公平锁和非公平锁。\n\n公平和非公平锁主要是Sync里面实现的，而Sync继承自**AQS**，也就是说ReentrantLock用的就是AQS思想。\n\n## AQS\n主要数据结构是CLH，最早的CLH是一个虚拟的单向链表，为什么是虚拟的呢，因为甚至连指针都没有，一个CLH节点就包含两个结构，一个是当前线程，另外一个是一个bool变量，用来表示是否获得了锁。\n\n**那是怎么运作的呢？**\n\n初始化一个尾节点，线程那部分为空，bool部分为true，也就是说可以被上锁，每一个线程都用**自旋的方式**尝试获得来的时候的尾节点的bool值，那么这个时候来了一个线程，用CAS尝试获取现在的尾节点的bool，为真，那就可以上锁为false，同时自己的bool也为false，表示前面的节点都不能用，所以下一个也不能。\n\n来了一个线程2，现在的尾节点是线程1，他的bool是false，线程2会一直自旋的获取前一个节点的bool值，后面的也是，如果此时线程1好了，那么线程2监听到前面一个变成了true，那就会开始线程2，以此类推。\n\n优点很明显：如果时间短，那么获取和释放锁的开销小。CLH 的锁状态不再是单一的原子变量，而是分散在每个节点的状态中，降低了自旋锁在竞争激烈时频繁同步的开销。还有就是公平，先进先出\n\n这样的CLH有什么缺点呢，首先是都是自旋，如果时间短还好，长了那肯定浪费资源。功能单一，不能支持ReentrantLock的那些接口功能。\n\n**AQS对于CLH的改造**\n\n针对自旋，AQS都改造成了阻塞，还是不能用自旋。针对第二个功能单一的缺点，使用了双向链表和一个全局变量state（其实我觉得之前那种看上一个节点情况的机制还蛮好的，这不是又重新变成信号量那种机制了）\n\nstate属性表示资源的状态，为0表示可以抢了，为1表示不能，为1时候来的线程进阻塞队列\n- 公平与非公平，公平就是直接每次都唤醒队列头的，非公平就是来的也一起竞争，但是可能会导致饥饿\n- 可重入：一个线程多次进入可以state自增，如果重入两次那么state就是2，但是要保证加锁几次就解锁几次，保证最终还是0\n\n# ReentrantLock和synchronized区别\n\n1. 首先两者都是悲观锁，也都是可重入的，synchronized是一个关键字，基于jvm实现，ReentrantLock是一个是实现类，实现Lock接口，使用的时候需要声明。\n2. ReentrantLock功能比较多，实现了可中断（中断也就说如果得不到锁就去干别的事情了，synchronized不可中断），可超时（ReentrantLock实现了，如果超时多少就不去争抢锁了，syn实现不了），锁可以绑定多个条件（Condition类，就相当于不同的信号量，可以控制一部分Condition下的进程等待，另一部分的执行）\n3. 没有竞争的时候synchronized优化很多，锁没有升级的时候性能很好；重度的时候Lock的性能比较好，因为底层还是用了cas的一部分乐观机制，相比synchronized的monitor还是好一点\n\n","tags":["Java"]},{"title":"mysql-索引和索引失效","url":"/2024/04/05/mysql-索引和索引失效/","content":"# 索引失效\n## 左匹配或者左右匹配\n当我们使用左或者左右模糊匹配的时候，也就是 like %xx 或者 like %xx% 这两种方式都会造成索引失效。\n\n**为什么 like 关键字左或者左右模糊匹配无法走索引呢？**\n\n因为索引 B+ 树是按照「索引值」有序排列存储的，只能根据前缀进行比较。\n\n举个例子，下面这张二级索引图（图中叶子节点之间我画了单向链表，但是实际上是双向链表，原图我找不到了，修改不了，偷个懒我不重画了，大家脑补成双向链表就行），是以 name 字段有序排列存储的。\n\n![](../img/Mysql/img_13.png)\n\n\n假设我们要查询 name 字段前缀为「林」的数据，也就是 name like '林%'，扫描索引的过程（这里中文都是按照某种排序，不知道是不是按照utf-8的编码值比较的，但是这个规则不影响寻找到逻辑）：\n- 节点1判断出“林”小于“周”并且大于“陈”（这个步骤是通过**页目录**进行搜索的，回顾一下：假设这里头指针是一个分组，陈是一个分组，剩下两个和尾指针是一个分组，二分查找目录项，由于目录项指向分组的最大值，那么“林”是大于陈小于周的，按照目录项的规则，从*包含尾指针的那个分组开始*，但是由于存的是最大值也就是尾指针，为了从头开始遍历，就要找到上一个槽的指针，往后数一个就是当前的最小值，结果发现当前的最小值“周”还是大于“林”，这样就定位在“陈”和“周”之间，按照存储规则，**非叶子节点存的是下一个节点的最小值**，那就要在“陈”这个节点找）\n- 节点2继续比较：节点2的第一个索引值中的陈字的拼音大小比林字小，所以继续看下一个索引值，发现节点2有与林字前缀匹配的索引值，于是就往叶子节点查询，即叶子节点4\n- 节点4查询比较：节点4的第一个索引值的前缀符合林字，于是就读取该行数据，接着继续往右匹配，直到匹配不到前缀为林的索引值。\n  \n如果使用 name like '%林' 方式来查询，因为查询的结果可能是「陈林、张林、周林」等之类的，所以不知道从哪个索引值开始比较，于是就只能通过全表扫描的方式来查询。\n> 总而言之，索引是从左边开始匹配的，如果左边第一个都不能确定，那就要全部遍历\n## 对索引使用函数\n有时候我们会用一些 MySQL 自带的函数来得到我们想要的结果，这时候要注意了，如果查询条件中对索引字段使用函数，就会导致索引失效。\n\n**为什么对索引使用函数，就无法走索引了呢？**\n\n因为索引保存的是索引字段的原始值，而不是经过函数计算后的值，自然就没办法走索引了。\n\n不过，从 MySQL 8.0 开始，索引特性增加了函数索引，即可以针对函数计算后的值建立一个索引，也就是说该索引的值是函数计算后的值，所以就可以通过扫描索引来查询数据。\n\n举个例子，我通过下面这条语句，对 length(name) 的计算结果建立一个名为 idx_name_length 的索引。\n\n```text\nalter table t_user add key idx_name_length ((length(name)));\n```\n> 总而言之，索引保存的是原始值，使用函数后重新计算导致不能索引到，就失效了，但是如果对这个函数也建立一个索引，那也可以走索引\n## 数据类型强制转换\n> 这个印象比较深刻：首先举一个例子，select \"10\" > 9，通过这个例子可以看出来在mysql是怎么处理字符串的\n> - 如果是结果是1，也就是select 10>9，那么内部会把字符串处理成数字\n> - 如果结果是0，也就是select \"10\" > \"9\"，字符串是比较首位，1肯定小于9，所以这个结果是0，那么内部把数字处理成字符串\n> 结果是1，也就是**mysql内部把字符串变成数字进行比较**\n\n假设phone在数据库是用varchar存的，有了这个结论，那么\n```text\nselect * from t_user where phone = 1300000001;\n```\n这个代码就会变成：\n```text\nselect * from t_user where CAST(phone AS signed int) = 1300000001;\n```\n因为内部会把索引的字符串变成整数进行比较，相当于给索引使用了函数，所以就会失效\n\n如果查询条件是字符串，id在数据库用int存的\n```text\nselect * from t_user where id = \"1\";\n```\n那么就会变成：\n```text\nselect * from t_user where id = CAST(\"1\" AS signed int);\n```\n也就是说，只有这个查询条件会要用这个函数，索引没有用，索引就不失效\n## 对索引进行表达式计算\n在查询条件中对索引进行表达式计算，也是无法走索引的。\n\n比如，下面这条查询语句，执行计划中 type = ALL，说明是通过全表扫描的方式查询数据的：\n\n```text\nexplain select * from t_user where id + 1 = 10;\n```\n\n**为什么对索引进行表达式计算，就无法走索引了呢？**\n\n原因跟对索引使用函数差不多。\n\n因为索引保存的是索引字段的原始值，而不是 id + 1 表达式计算后的值，所以无法走索引，只能通过把索引字段的取值都取出来，然后依次进行表达式的计算来进行条件判断，因此采用的就是全表扫描的方式。\n\n有的同学可能会说，这种对索引进行简单的表达式计算，在代码特殊处理下，应该是可以做到索引扫描的，比方将 id + 1 = 10 变成 id = 10 - 1。\n\n是的，是能够实现，但是 MySQL 还是偷了这个懒，没有实现。\n\n我的想法是，可能也是因为，表达式计算的情况多种多样，每种都要考虑的话，代码可能会很臃肿，所以干脆将这种索引失效的场景告诉程序员，让程序员自己保证在查询条件中不要对索引进行表达式计算。\n\n## 联合索引的左匹配问题\n对主键字段建立的索引叫做聚簇索引，对普通字段建立的索引叫做二级索引。\n\n那么多个普通字段组合在一起创建的索引就叫做联合索引，也叫组合索引。\n\n创建联合索引时，我们需要注意创建时的顺序问题，因为联合索引 (a, b, c) 和 (c, b, a) 在使用的时候会存在差别。\n\n联合索引要能正确使用需要遵循最左匹配原则，也就是按照最左优先的方式进行索引的匹配。\n\n比如，如果创建了一个 (a, b, c) 联合索引，如果查询条件是以下这几种，就可以匹配上联合索引：\n- where a=1；\n- where a=1 and b=2 and c=3；\n- where a=1 and b=2；\n需要注意的是，因为有查询优化器，所以 a 字段在 where 子句的顺序并不重要。\n\n但是，如果查询条件是以下这几种，因为不符合最左匹配原则，所以就无法匹配上联合索引，联合索引就会失效:\n- where b=2；\n- where c=3；\n- where b=2 and c=3；\n> 其实类似第一种情况，不管是字符串还是索引，都是按照最左匹配的，如果连第一个都没有，那就会导致索引失效，这里还有一个**索引下推**的机制，等以后再来补。\n## or导致的失效问题\n\n在 WHERE 子句中，如果在 OR 前的条件列是索引列，而在 OR 后的条件列不是索引列，那么索引会失效。\n\n这是因为 OR 的含义就是两个只要满足一个即可，因此只有一个条件列是索引列是没有意义的，只要有条件列不是索引列，就会进行全表扫描。\n\n> or就是两个里面满足一个，也就是索引要有两个，才可以一起用索引，and其实是只要有一个是索引就可以走索引的，要注意","tags":["Mysql"]},{"title":"mysql-Buffer Pool","url":"/2024/04/05/mysql-Buffer-Pool/"},{"title":"mysql-事务隔离级别与MVCC","url":"/2024/04/05/mysql-事务隔离级别与MVCC/","content":"# 事务的特性（ACID）\n- 原子性（Atomicity）：要么一起成功，要么一起失败，是一个操作，不可分割\n- 一致性（Consistency）：是指事务的执行不能破坏数据库数据的完整性和一致性，一个事务在执行前后，数据库都必须处于一致性状态。换句话说，事务的执行结果必须是使数据库从一个一致性状态转变到另一个一致性状态。\n- 隔离性（Isolation）：多个并发事务操作是隔离的，不会彼此打扰\n- 持久性（Durability）：修改之后是持久的，不会掉电就失效了\n# 并发导致的不一致性\n- 脏读\n**如果一个事务「读到」了另一个「未提交事务修改过的数据」，就意味着发生了「脏读」现象。**\n\n假设有 A 和 B 这两个事务同时在处理，事务 A 先开始从数据库中读取小林的余额数据，然后再执行更新操作，如果此时事务 A 还没有提交事务，而此时正好事务 B 也从数据库中读取小林的余额数据，那么事务 B 读取到的余额数据是刚才事务 A 更新后的数据，即使没有提交事务。\n\n![](../img/Mysql/img_14.png)\n\n因为事务 A 是还没提交事务的，也就是它随时可能发生回滚操作，如果在上面这种情况事务 A 发生了回滚，那么事务 B 刚才得到的数据就是过期的数据，这种现象就被称为脏读\n> 也就是事务B在A还没提交之前就已经读到了修改的数据，但是A因为某些原因回滚了，这个时候B就读到了脏数据，称为脏读\n- 不可重复读\n**在一个事务内多次读取同一个数据，如果出现前后两次读到的数据不一样的情况，就意味着发生了「不可重复读」现象。**\n\n假设有 A 和 B 这两个事务同时在处理，事务 A 先开始从数据库中读取小林的余额数据，然后继续执行代码逻辑处理，在这过程中如果事务 B 更新了这条数据，并提交了事务，那么当事务 A 再次读取该数据时，就会发现前后两次读到的数据是不一致的，这种现象就被称为不可重复读。\n\n![](../img/Mysql/img_15.png)\n\n> 也就是A读到了B修改前和修改后的两次数据，不一致的问题，就是不可重复读\n- 幻读\n\n","tags":["Mysql"]},{"title":"mysql-B+树与数据页","url":"/2024/04/03/mysql-B-树与数据块/","content":"> 我们先从最基本的存储引擎InnoDB开始学起，[原文链接](https://blog.csdn.net/liang921119/article/details/130556995)\n# InnoDB 是如何存储数据的？\nMySQL 支持多种存储引擎，不同的存储引擎，存储数据的方式也是不同的，我们最常使用的是 InnoDB 存储引擎，所以就跟大家图解下InnoDB 是如何存储数据的。\n\n记录是按照行来存储的，但是数据库的读取并不以「行」为单位，否则一次读取（也就是一次 I/O 操作）只能处理一行数据，效率会非常低。\n\n因此，InnoDB 的数据是按「数据页」为单位来读写的，也就是说，当需要读一条记录的时候，并不是将这个记录本身从磁盘读出来，而是以页为单位，将其整体读入内存。\n# 数据页\n数据库的 I/O 操作的最小单位是页，InnoDB 数据页的默认大小是 16KB，意味着数据库每次读写都是以 16KB 为单位的，一次最少从磁盘中读取 16K 的内容到内存中，一次最少把内存中的 16K 内容刷新到磁盘中。\n\n数据页包括七个部分，结构如下图：\n\n![](../img/Mysql/img.png)\n\n![](../img/Mysql/img_1.png)\n> 这里最重要的是最小和最大记录，以及用户记录\n\n## 用户真实记录在数据页中的存储（Free Space）\n在页的7个组成部分中，我们自己存储的记录会按照我们指定的行格式存储到User Records部分。但是在一开始生成页的时候，其实并没有User Records这个部分，每当我们插入一条记录，都会从Free Space部分，也就是尚未使用的存储空间中申请一个记录大小的空间划分到User Records部分，当Free Space部分的空间全部被User Records部分替代掉之后，也就意味着这个页使用完了，如果还有新的记录插入的话，就需要去申请新的页了。这个过程的图示如下：\n\n![](../img/Mysql/img_2.png)\n\n为了更好的管理在User Records中的这些记录，InnoDB可费了一番力气呢，在哪费力气了呢？不就是把记录按照指定的行格式一条一条摆在User Records部分么？其实这话还得从记录行格式的记录头信息中说起\n\n## 记录头信息引出的数据页“记录”结构\n我们这里先创建一张表\n\n```text\nmysql> create table demo5 (c1 int, c2 int, c3 varchar(10000),primary key (c1)) charset=ascii row_format=compact;\n\nQuery OK, 0 rows affected (0.04 sec)\n```\n\n这个新建的表有三个列，c1和c2列是用来存储整数的，c3存储的字符串，但是我们指定c1位主键，所以在具体的行格式中，Innodb就没有必要给我们创建row_id隐藏列了 **（这里说的隐藏列是在没有明确给主键的时候，mysql会自己找一个unique并且没有空值的列为主键索引，这个就是隐藏列）**。所以表中的行格式示意图如下：\n\n![](../img/Mysql/img_3.png)\n\n![](../img/Mysql/img_4.png)\n\n\n| 名称\t          | ⼤⼩（单位：bit）\t | 描述                                              |\n|--------------|-------------|-------------------------------------------------|\n| 预留位1\t        | 1           | \t没有使⽤                                           | \n| 预留位2\t        | 1\t          | 没有使⽤                                            | \n| delete_mask\t | 1\t          | 标记该记录是否被删除                                      |\n| min_rec_mask | \t1\t         | B+树的每层⾮叶⼦节点中的最⼩记录都会添加该标记                        |\n| n_owned\t     | 4\t          | 表示当前记录拥有的记录数                                    | \n| heap_no\t     | 13\t         | 表示当前记录在记录堆的位置信息                                 |  \n| record_type  | \t3\t         | 表示当前记录的类型，0表示普通记录，1表示B+树⾮叶⼦节点记录，2表示最⼩记录，3表示最⼤记录 |   \n| next_record\t | 16\t         | 表示下⼀条记录的相对位置                                    |\n\n假设我们插入了几条数据：\n\n![](../img/Mysql/img_5.png)\n\n接下来会一个个解释这几个字段\n\n### delete_mask\n这个属性标记当前记录是否被删除，占用一个二进制位，为0为没有删除，为1被删除\n> 被删除的记录不立即从磁盘上移除，因为移除它们之后把其他的记录在磁盘上重新排列需要性能消耗，所以只是打一个删除标记而已，所有被删除掉的记录都会组成一个所谓的垃圾链表，在这个链表中的记录占用的空间称之为所谓的可重用空间，之后如果有新记录插入到表中的话，可能把这些被删除的记录占用的存储空间覆盖掉。这个delete_mask位设置为1和将被删除的记录加入到垃圾链表中是两个阶段\n\n### min_rec_mask\nB+树的每层非叶子节点中的最小记录都会添加该标记。值为1，表示该条记录是B+树的非叶子节点中的最小记录；值为0，意味着该条数据不是B+树的非叶子节点中的最小记录 **（暂时还不知道这个具体作用，可能就是记录每一层的头节点？）**\n\n### n_owned\n这个涉及到**分组**的概念了，存的就是当前分组有多少个元素，后续会介绍\n\n### heap_no\n这个属性表示当前记录在本页中的位置（类似页表中的本页地址字段，但是这个单纯用来比较顺序）。MySQL自动给每个页里边儿加了两个记录，由于这两个记录并不是我们自己插入的，所以有时候也称为伪记录或者虚拟记录。这两个伪记录一个代表最小记录，一个代表最大记录（这就是最前面介绍的7部分中的 **最大最小记录**）。\n\n记录也可以比大小，对于一条完整的记录来说，比较记录的大小就是比较主键的大小。但是不管我们向页中插入了多少自己的记录，InnoDB规定他们定义的两条伪记录分别为最小记录与最大记录。这两条记录的构造十分简单，都是由5字节大小的记录头信息和8字节大小的一个固定的部分组成的。\n\n![](../img/Mysql/img_6.png)\n> 这里规定的最小记录的heapno为0，最大记录的为1\n\n### record_type\n这个属性表示当前记录的类型，一共有4种类型的记录，0表示普通记录，1表示B+树非叶节点记录，2表示最小记录，3表示最大记录。从图中我们也可以看出来，我们自己插入的记录就是普通记录，它们record_type值都是0，而最小记录和最大记录的record_type值分别为2和3，至于record_type为1的情况，我们之后在说索引的时候会重点强调的。\n\n### next_record\n\n这个信息非常重要，表示从当前记录的真实数据到下一条记录的真实数据的地址偏移量。比方说第一条记录的next_record值为32，意味着从第一条记录的真实数据的地址处向后找32个字节便是下一条记录的真实数据。如果你熟悉数据结构的话，就立即明白了，这其实是个链表，可以通过一条记录找到它的下一条记录。但是需要注意注意再注意的一点是，下一条记录指的并不是按照我们插入顺序的下一条记录，而是按照主键值由小到大的顺序的下一条记录。而且规定Infimum记录（也就是最小记录） 的下一条记录就是本页中主键值最小的用户记录，而本页中主键值最大的用户记录的下一条记录就是Supremum记录（也就是最大记录） ，为了更形象的表示一下这个next_record起到的作用，我们用箭头来替代一下next_record中的地址偏移量：\n\n![](../img/Mysql/img_7.png)\n\n假设删掉第2条记录后，由于删除只是标记为删除，实际上没有删除，就会变成：\n\n![](../img/Mysql/img_8.png)\n\n从图中可以看出来，删除第2条记录前后主要发生了这些变化：\n\n- 第2条记录并没有从存储空间中移除，而是把该条记录的delete_mask值设置为1。\n- 第2条记录的next_record值变为了0，意味着该记录没有下一条记录了。\n- 第1条记录的next_record值变为了64，指向了第3条记录。\n- 最大记录的n_owned值从5变成了4，关于这一点的变化我们稍后会详细说明的。\n所以，不论我们怎么对页中的记录做增删改操作，InnoDB始终会维护一条记录的单链表，链表中的各个节点是按照主键值由小到大的顺序连接起来的\n\n## 页目录（重要）\n> 页目录的存在意义主要是加速检索速度，并不是将他们重新分组，我最开始以为是将记录再分成几块进行检索，但是其实只是一个方便二分查找的工具\n\n现在我们了解了记录在页中按照主键值由小到大顺序串联成一个单链表，那如果我们想根据主键值查找页中的某条记录该咋办呢？比如说这样的查询语句：\n```text\nselect * from where c1=3;\n```\n最笨的办法：从Infimum记录（最小记录）开始，沿着链表一直往后找，总会找到。在找的时候还能投机取巧，因为链表中各个记录的值是按照从小到大顺序排列的，所以当链表的某个节点代表的记录的主键值大于你想要查找的主键值时，你就可以停止查找了，因为该节点后边的节点的主键值依次递增。\n\n但是InnoDB能用这么笨的办法么，当然是要设计一种更快的查找方式，于是乎从书的目录中找到了灵感。\n\n我们平常想从一本书中查找某个内容的时候，一般会先看目录，找到需要查找的内容对应的书的页码，然后到对应的页码查看内容。InnoDB为我们的记录也制作了一个类似的目录，他们的制作过程是这样的：\n\n- 将所有正常的记录（包括最大和最小记录，不包括标记为已删除的记录）划分为几个组。\n- 每个组的最后一条记录（也就是组内最大的那条记录）的头信息中的n_owned属性表示该记录拥有多少条记录，也就是该组内共有几条记录。\n- 将每个组的最后一条记录的地址偏移量单独提取出来按顺序存储到靠近页的尾部的地方，这个地方就是所谓的Page Directory，也就是页目录。页面目录中的这些地址偏移量被称为槽（英文名：Slot），所以这个页面目录就是由槽组成的。\n需要注意的是，Page Directory是逆序存放的，每个槽占2字节\n\n![](../img/Mysql/img_9.png)\n> 这里第一个slot为什么是99？因为是从页的第一位开始算 ~~（文件头+页头+最小记录前的6字节=38+56+5=100，但是是0开始的，所以）~~ 最小记录单成一个分组，那slot也就指向的是最小记录的数据\n\n从这个图中我们需要注意这么几点：\n\n现在页目录部分中有两个槽，也就意味着我们的记录被分成了两个组，槽1中的值是112，代表最大记录的地址偏移量（就是从页面的0字节开始数，数112个字节）；槽0中的值是99，代表最小记录的地址偏移量。\n\n注意最小和最大记录的头信息中的n_owned属性\n\n最小记录的n_owned值为1，这就代表着以最小记录结尾的这个分组中只有1条记录，也就是最小记录本身。\n\n最大记录的n_owned值为5，这就代表着以最大记录结尾的这个分组中只有5条记录，包括最大记录本身还有我们自己插入的4条记录。\n\n99和112这样的地址偏移量很不直观，我们用箭头指向的方式替代数字，这样更易于我们理解，所以修改后的示意图就是这样：\n\n![](../img/Mysql/img_10.png)\n\n![](../img/Mysql/img_11.png)\n\n\nInnoDB对每个分组中的记录条数是有规定的：对于最小记录所在的分组只能有1条记录，最大记录所在的分组拥有的记录条数只能在1 ~ 8条之间，剩下的分组中记录的条数范围只能在是4 ~ 8条之间。所以分组是按照下边的步骤进行的：\n- 初始情况下一个数据页里只有最小记录和最大记录两条记录，它们分属于两个分组。\n- 之后每插入一条记录，都会从页目录中找到主键值比本记录的主键值大并且差值最小的槽，然后把该槽对应的记录的n_owned值加1，表示本组内又添加了一条记录，直到该组中的记录数等于8个。\n- 在一个组中的记录数等于8个后再插入一条记录时，会将组中的记录拆分成两个组，一个组中4条记录，另一个5条记录。这个过程会在页目录中新增一个槽来记录这个新增分组中最大的那条记录的偏移量。\n\n### 举例\n\n```text\ninsert into demo5 values(1,100,'aaaa'),(2,200,'bbbb'),(3,300,'cccc'),(4,400,'dddd');\ninsert into demo5 values(5, 500, 'eeee');\ninsert into demo5 values(6, 600, 'ffff');\ninsert into demo5 values(7, 700, 'gggg');\ninsert into demo5 values(8, 800, 'hhhh');\ninsert into demo5 values(9, 900, 'iiii');\ninsert into demo5 values(10, 1000, 'jjjj');\ninsert into demo5 values(11, 1100, 'kkkk');\ninsert into demo5 values(12, 1200, 'llll');\ninsert into demo5 values(13, 1300, 'mmmm');\ninsert into demo5 values(14, 1400, 'nnnn');\ninsert into demo5 values(15, 1500, 'oooo');\ninsert into demo5 values(16, 1600, 'pppp');\n```\n![](../img/Mysql/img_12.png)\n\n因为把16条记录的全部信息都画在一张图里太占地方，让人眼花缭乱的，所以只保留了用户记录头信息中的n_owned和next_record属性，也省略了各个记录之间的箭头，我没画不等于没有啊！现在看怎么从这个页目录中查找记录。因为各个槽代表的记录的主键值都是从小到大排序的，所以我们可以使用所谓的二分法来进行快速查找。5个槽的编号分别是：0、1、2、3、4，所以初始情况下最低的槽就是low=0，最高的槽就是high=4。比方说我们想找主键值为6的记录，过程是这样的：\n- 计算中间槽的位置：(0+4)/2=2，所以查看槽2，对应记录的主键值为8，又因为8 > 6，所以设置high=2，low保持不变。\n- 重新计算中间槽的位置：(0+2)/2=1，所以查看槽1对应的主键值为4，又因为4 < 6，所以设置low=1，high保持不变。\n- 因为high - low的值为1，所以确定主键值为6的记录在槽2对应的组中，此刻我们需要找到槽2中主键值最小的那条记录，然后沿着单向链表遍历槽2中的记录。但是我们前边又说过，每个槽对应的记录都是该组中主键值最大的记录，这里槽2对应的记录是主键值为8的记录，怎么定位一个组中最小的记录呢？别忘了各个槽都是挨着的，我们可以很轻易的拿到槽1对应的记录（主键值为4），该条记录的下一条记录就是槽2中主键值最小的记录，该记录的主键值为5。所以我们可以从这条主键值为5的记录出发，遍历槽2中的各条记录，直到找到主键值为6的那条记录即可。由于一个组中包含的记录条数只能是1~8条，所以遍历一个组中的记录的代价是很小的。\n\n所以在一个数据页中查找指定主键值的记录的过程分为两步：\n- 通过二分法确定该记录所在的槽，并找到该槽所在分组中主键值最小的那条记录。\n- 通过记录的next_record属性遍历该槽所在的组中的各个记录\n\n# B+树","tags":["Mysql"]},{"title":"redis-读写一致性","url":"/2024/04/03/redis-读写一致性/","content":"# Mysql如何与Redis保持同步\n\n![](../img/Redis/img_24.png)\n\n类比计算机组成原理中的cache和内存，redis也就是mysql的缓存，那么保持读写一致性也是十分重要的，我们在修改数据库的同时，缓存中也要对应更新。\n- 读操作：缓存命中就直接返回，缓存未命中就查找数据库，然后更新缓存\n- 写操作：**延迟双删**\n![](../img/Redis/img_25.png)\n\n## 先删除缓存，再更新数据库\n1. **正常情况**\n\n![](../img/Redis/img_26.png)\n\n首先线程1删除缓存，然后更新数据库，此时没有任何线程来打扰，完成操作以后线程2再来查找缓存，这个时候由于线程1已经删除了缓存，那就会查找数据库找到刚刚更新的数据，最后写入缓存。这个过程不会出现读写一致性问题\n\n2. **特殊情况**\n> 其实下面先写后删除的特殊情况也适用于本例中，首先线程1查询到了一个过期key，去数据库找，暂时保存，但是没写进redis。此时另一个进程是写进程，先删除了这个key，随后更新，最后才轮到线程1，写回刚刚的旧数据到redis，同样会有数据不一致\n\n![](../img/Redis/img_27.png)\n\n我们假设线程1删除缓存之后，由于线程是并发的，线程2来查询，此时由于还没有更新数据库，找到的还是原来的数据，随后放回缓存，这个时候才轮到线程1，更新数据库，但是此时redis还是用的以前的数据，数据库的是刚刚更新的，就出现了读写一致性问题\n## 先更新数据库，再更新缓存\n> 那如果我们先更新数据库，在更新缓存是不是正确的呢？**答案是还是会有读写一致性问题**\n\n1. **正常情况**\n\n ![](../img/Redis/img_28.png)\n线程2先更新数据库，然后删除，跟上面的一样，只要两个操作是原子性的就不会有问题，但是只要是线程是并发的，那就肯定不一致\n\n2. **特殊情况**\n\n![](../img/Redis/img_29.png)\n\n>如果在不在这个刚刚过期的时间节点，那会怎么样呢？其实如果不是刚刚要过期，那就直接会拿走缓存中的数据，也不会删除缓存，那写进程后续先改再写redis也不会出现读写一致性问题，个人觉得核心的问题在于读进程的机制，没查到或者过期的缓存就会主动去数据库里调用，会扰乱正常的写进程导致不一致\n> \n> 如果是命中的读进程，那就根本不会扰乱写进程，只是脏数据的问题，redis和mysql最终都是一致的。而这里讨论的刚好过期，才会导致读进程写redis，才会有不一致性的问题发生）\n\n线程1先去查询缓存，这个时候刚好key过期，就要去数据库找，在数据库找到的是旧数据，先保存下来，但是此时线程2更新了数据库，然后删除，最后才是线程1写回旧数据到redis，出现不一致。\n\n## 解决方法\n- **分布式锁**：在更新缓存前先加个分布式锁，保证同一时间只运行一个请求更新缓存，就会不会产生并发问题了，当然引入了锁后，对于写入的性能就会带来影响。\n- **延迟双删**：针对「先删除缓存，再更新数据库」方案在「读 + 写」并发请求而造成缓存不一致的解决办法是「延迟双删」。\n- - 为什么要双删？因为刚刚在[先删除缓存，再更新数据库]方案中会出现读写一致问题，这个时候只要再去删除一次缓存就可以了，下一个来的读请求会发现不存在对应的key，然后从数据库找，最终达到一致性 \n- - 为什么要延迟？主要是为了确保请求 A 在睡眠的时候，请求 B 能够在这这一段时间完成「从数据库读取数据，再把缺失的缓存写入缓存」的操作，然后请求 A 睡眠完，再删除缓存。\n所以，请求 A 的睡眠时间就需要大于请求 B 「从数据库读取数据 + 写入缓存」的时间。因为这个延时时间不好控制，在极端情况下还是会出现读写不一致的现象。\n- **使用「先更新数据库，再删除缓存」方案**：其实这种方案的特殊情况很难遇见，因为**缓存的写入通常要远远快于数据库的写入**，所以在实际中很难出现请求 B 已经更新了数据库并且删除了缓存，请求 A 才更新完缓存的情况。\n而一旦请求 A 早于请求 B 删除缓存之前更新了缓存，那么接下来的请求就会因为缓存不命中而从数据库中重新读取数据，所以不会出现这种不一致的情况。**（意思是即使B先写了数据库，A再写旧的值，因为数据库涉及io（扯远点就是io中断）阻塞进程B直到时间片到了，进程会交替执行而io搁一边，可能当执行到A写完了旧值，更新才刚刚结束，这个时候B再执行删除，就可以保证一致性）**\n\n## 在不那么要求强一致性的场景\n有的时候可以容忍一瞬间的脏数据，但是要保持最终一致性的场景，可以使用以下两种方法：\n- 基于消息队列（例如kafka）：写入数据库后，发一个消息给mq通知redis更新缓存，这种方式的可靠性主要取决于mq，肯定是有一定的延时的，但是最终会保证一致\n- 基于阿里巴巴的Canal中间件：「先更新数据库，再删缓存」的策略的第一步是更新数据库，那么更新数据库成功，就会产生一条变更日志，记录在 binlog 里。\nCanal 模拟 MySQL 主从复制的交互协议，把自己伪装成一个 MySQL 的从节点，向 MySQL 主节点发送 dump 请求，MySQL 收到请求后，就会开始推送 Binlog 给 Canal，Canal 解析 Binlog 字节流之后，转换为便于读取的结构化数据，供下游程序订阅使用。\n","tags":["Redis"]},{"title":"leetcode-2024-4-2","url":"/2024/04/02/leetcode-2024-4-2/","content":"# 894 所有可能的真二叉树\n给你一个整数 n ，请你找出所有可能含 n 个节点的 真二叉树 ，并以列表形式返回。答案中每棵树的每个节点都必须符合 Node.val == 0 。\n\n答案的每个元素都是一棵真二叉树的根节点。你可以按 任意顺序 返回最终的真二叉树列表。\n\n真二叉树 是一类二叉树，树中每个节点恰好有 0 或 2 个子节点。\n\n![](../img/coding/2024_4_2_1.png)\n> 感觉很像之前写过的一道题\n## 记忆化搜索\n由分析知道，n不能为偶数，叶子节点为（n+1）/2，度为二的节点为(n-1)/2，n=1的时候就只有一个节点，n可以拆分出[x,1,y]三个，x和y都必须是奇数，x和y都比n小而且一定在之前的遍历中已经计算过，只要深拷贝n=x和n=y的数组，作为左边右边，就可以得到最终答案\n\n\n```java\n/**\n * Definition for a binary tree node.\n * public class TreeNode {\n *     int val;\n *     TreeNode left;\n *     TreeNode right;\n *     TreeNode() {}\n *     TreeNode(int val) { this.val = val; }\n *     TreeNode(int val, TreeNode left, TreeNode right) {\n *         this.val = val;\n *         this.left = left;\n *         this.right = right;\n *     }\n * }\n */\nclass Solution {\n    //记忆化保存的数组\n    List<List<TreeNode>> list = new ArrayList<>();\n    List<TreeNode> treeNodeList;\n    public List<TreeNode> allPossibleFBT(int n) {\n        //排除偶数\n        if (n%2==0){\n            return new ArrayList<>();\n        }\n        treeNodeList = new ArrayList<>();\n        treeNodeList.add(new TreeNode(0));\n        list.add(treeNodeList);\n        for (int k = 3; k <= n; k+=2) {\n            treeNodeList = new ArrayList<>();\n            for (int i = 1; i < k-1; i+=2) {\n                //System.out.println(i);\n                //由于i是+=2的，对应的list下标是++，所以要/2\n                List<TreeNode> left = new ArrayList<>(list.get(i/2));\n                List<TreeNode> right = new ArrayList<>(list.get((k-1-i)/2));\n                for (int p = 0; p < left.size(); p++) {\n                    for (int q = 0; q < right.size(); q++) {\n                        TreeNode root = new TreeNode(0);\n                        root.left = left.get(p);\n                        root.right = right.get(q);\n                        //先保存到treenodeList里\n                        treeNodeList.add(root);\n                    }\n                }\n            }\n            //System.out.println(treeNodeList.size());\n            //最后加上这个\n            list.add(treeNodeList);\n        }\n        //System.out.println(list.get(n/2).size());\n        //返回记忆化搜索最后一个\n        return list.get(n/2);\n\n    }\n}\n```","tags":["刷题笔记"]},{"title":"redis-高可用篇","url":"/2024/04/02/redis-高可用篇/","content":"# 主从复制\n\n单点Redis的并发能力是由上限的，如果都存储在一台服务器上，出事了就会有很严重的影响。\n- 如果出现了宕机，那么数据恢复需要时间，而且主进程在此期间不能服务新请求，\n- 硬盘出了问题那就会造成数据丢失。\n如何解决这种问题，并且还要进一步提高并发性？可以搭建**主从模式**，实现读写分离。\n\n> **读写分离**:\n> 主服务器可以进行读写操作，当发生写操作时自动将写操作同步给从服务器，而从服务器一般是只读，并接受主服务器同步过来写操作命令，然后执行这条命令。\n> ![](../img/Redis/img_12.png)\n> \n> 也就是说，所有的数据修改只在主服务器上进行，然后将最新的数据同步给从服务器，这样就使得主从服务器的数据是一致的。\n\n## 第一次同步\n多台服务器之间要通过什么方式来确定谁是主服务器，或者谁是从服务器呢？\n\n我们可以使用 replicaof（Redis 5.0 之前使用 slaveof）命令形成主服务器和从服务器的关系。\n\n比如，现在有服务器 A 和 服务器 B，我们在服务器 B 上执行下面这条命令：\n\n```text\n# 服务器 B 执行这条命令\nreplicaof <服务器 A 的 IP 地址> <服务器 A 的 Redis 端口号>\n```\n\n接着，服务器 B 就会变成服务器 A 的「从服务器」，然后与主服务器进行第一次同步。\n\n主从服务器间的第一次同步的过程可分为三个阶段：\n\n- 第一阶段是建立链接、协商同步；\n- 第二阶段是主服务器同步数据给从服务器；\n- 第三阶段是主服务器发送新写操作命令给从服务器。\n\n![](../img/Redis/img_13.png)\n\n> 1. 一个从服务器要加入主服务器的从节点，首先执行上面这条replicaof，告诉主服务器我是新加入的\n> 2. 然后建立连接。这里有两个关键参数**replication id**，标识一个数据集，每一个master都会有一个这个id，相当于标记了一块空间，当slave加入时，并不知道master的这个id，就给“？”，还有一个参数**offset**也就是偏移量，表示复制的进度，初始化为-1，（***有点类似TCP的序号和确认号***）。\n> 执行psync命令，告诉主服务器一个从节点要建立连接了\n> 3. 主服务器判断来的这个replication id是否是自己的，如果不是自己的说明是第一次加入的从服务器，返回前面的这两个值，自己的id和当前的复制进度。\n> 4. 从服务器收到响应之后会记录这两个值，准备进行**全量复制**\n> 5. 此时主服务器fork一个子进程完成bgsave命令，生成**RDB**文件，然后传输给从服务器。此时不会阻塞主进程，还是能接收新的数据，那么就需要一个数据结构来存这些刚刚加入，但是没有写入RDB文件的数据，这里就有一个**replication buffer**缓冲区用来存这些数据\n> 6. 从服务器收到了bump.rdb文件，然后进行载入，在此之前清空当前数据\n> 7. 对于主服务器在**复制传输和子服务器重建**过程中新到的数据，缓冲区内的数据发送新的写命令给从服务器。\n\n**第一阶段：建立链接、协商同步**\n\n执行了 replicaof 命令后，从服务器就会给主服务器发送 psync 命令，表示要进行数据同步。\n\npsync 命令包含两个参数，分别是主服务器的 runID 和复制进度 offset。\n\nrunID，每个 Redis 服务器在启动时都会自动生产一个随机的 ID 来唯一标识自己。当从服务器和主服务器第一次同步时，因为不知道主服务器的 run ID，所以将其设置为 \"?\"。\noffset，表示复制的进度，第一次同步时，其值为 -1。\n主服务器收到 psync 命令后，会用 FULLRESYNC 作为响应命令返回给对方。\n\n并且这个响应命令会带上两个参数：主服务器的 runID 和主服务器目前的复制进度 offset。从服务器收到响应后，会记录这两个值。\n\nFULLRESYNC 响应命令的意图是采用全量复制的方式，也就是主服务器会把所有的数据都同步给从服务器。\n\n所以，第一阶段的工作时为了全量复制做准备。\n\n那具体怎么全量同步呀呢？我们可以往下看第二阶段。\n\n**第二阶段：主服务器同步数据给从服务器**\n\n接着，主服务器会执行 bgsave 命令来生成 RDB 文件，然后把文件发送给从服务器。\n\n从服务器收到 RDB 文件后，会先清空当前的数据，然后载入 RDB 文件。\n\n这里有一点要注意，主服务器生成 RDB 这个过程是不会阻塞主线程的，因为 bgsave 命令是产生了一个子进程来做生成 RDB 文件的工作，是异步工作的，这样 Redis 依然可以正常处理命令。\n\n但是，这期间的写操作命令并没有记录到刚刚生成的 RDB 文件中，这时主从服务器间的数据就不一致了。\n\n那么为了保证主从服务器的数据一致性，主服务器在下面这三个时间间隙中将收到的写操作命令，写入到 replication buffer 缓冲区里：\n\n- 主服务器生成 RDB 文件期间；\n- 主服务器发送 RDB 文件给从服务器期间；\n- 「从服务器」加载 RDB 文件期间；\n\n**第三阶段：主服务器发送新写操作命令给从服务器**\n\n在主服务器生成的 RDB 文件发送完，从服务器收到 RDB 文件后，丢弃所有旧数据，将 RDB 数据载入到内存。完成 RDB 的载入后，会回复一个确认消息给主服务器。\n\n接着，主服务器将 replication buffer 缓冲区里所记录的写操作命令发送给从服务器，从服务器执行来自主服务器 replication buffer 缓冲区里发来的命令，这时主从服务器的数据就一致了。\n\n至此，主从服务器的第一次同步的工作就完成了。\n\n## 命令传播\n主从服务器在完成第一次同步以后就会维护一个Tcp连接。\n\n![](../img/Redis/img_14.png)\n\n后续主服务器可以通过这个连接继续将写操作命令传播给从服务器，然后从服务器执行该命令，使得与主服务器的数据库状态相同。\n\n而且这个连接是长连接的，目的是避免频繁的 TCP 连接和断开带来的性能开销。\n\n上面的这个过程被称为基于长连接的命令传播，通过这种方式来保证第一次同步后的主从服务器的数据一致性。\n\n## 增量复制\n> 主从服务器在第一次同步之后，就会建立一个tcp长连接完成命令传输。但是此时网络产生延迟或者断开。那么就不能进行命令传播了（这里的断开网络指的是主从服务器之间的同步被打断），那么客户端还是可以从从服务器读到旧的数据产生不一致性。\n\n![](../img/Redis/img_15.png)\n\n如果此时网络又恢复正常了，从服务器已经落后主服务器，要进行再次同步，这个时候用**全局复制**造成的开销会很大，可以采用**增量复制**的方法进行同步，只会把网络断开期间主服务器收到的写操作同步给从服务器，利用的就是offset之间的差值。\n![](../img/Redis/img_16.png)\n>1. 恢复连接后从服务器给主服务器发送id和offset\n>2. 主服务器知道id是自己的，但是offest落后的有点多，这个时候就会计算自己的offset和从服务器给的offset之间差了多少，然后发送CONTINUE命令，告诉从服务器要发送增量数据。\n>3. 从服务器重新执行这些命令\n\n**offset具体怎么计算？**\n- repl_backlog_buffer，是一个「环形」缓冲区，用于主从服务器断连后，从中找到差异的数据；\n- replication offset，标记上面那个缓冲区的同步进度，主从服务器都有各自的偏移量，主服务器使用 master_repl_offset 来记录自己「写」到的位置，从服务器使用 slave_repl_offset 来记录自己「读」到的位置。\n\n**那 repl_backlog_buffer 缓冲区是什么时候写入的呢？**\n\n在主服务器进行命令传播时，不仅会将写命令发送给从服务器，还会将写命令写入到 repl_backlog_buffer 缓冲区里，因此 这个缓冲区里会保存着最近传播的写命令。\n\n网络断开后，当从服务器重新连上主服务器时，从服务器会通过 psync 命令将自己的复制偏移量 slave_repl_offset 发送给主服务器，主服务器根据自己的 master_repl_offset 和 slave_repl_offset 之间的差距，然后来决定对从服务器执行哪种同步操作：\n\n- 如果判断出从服务器要读取的数据还在 repl_backlog_buffer 缓冲区里，那么主服务器将采用增量同步的方式；\n- 相反，如果判断出从服务器要读取的数据已经不存在 repl_backlog_buffer 缓冲区里，那么主服务器将采用全量同步的方式。（这是由于缓冲区是环形的，时间过长就会导致原来的数据被覆盖）\n\n**环形缓冲区repl_backlog_buffer**\n\n当主服务器在 repl_backlog_buffer 中找到主从服务器差异（增量）的数据后，就会将增量的数据写入到 replication buffer 缓冲区，这个缓冲区我们前面也提到过，它是缓存将要传播给从服务器的命令。\n\nrepl_backlog_buffer 缓行缓冲区的默认大小是 1M，并且由于它是一个环形缓冲区，所以当缓冲区写满后，主服务器继续写入的话，就会覆盖之前的数据。因此，当主服务器的写入速度远超于从服务器的读取速度，缓冲区的数据一下就会被覆盖。\n\n那么在网络恢复时，如果从服务器想读的数据已经被覆盖了，主服务器就会采用全量同步，这个方式比增量同步的性能损耗要大很多。\n\n因此，为了避免在网络恢复时，主服务器频繁地使用全量同步的方式，我们应该调整下 repl_backlog_buffer 缓冲区大小，尽可能的大一些，减少出现从服务器要读取的数据被覆盖的概率，从而使得主服务器采用增量同步的方式。\n\n**repl_backlog_buffer 缓冲区具体要调整到多大呢？**\n\n$$\nsecond * write_size_per_second\n$$\n \n- second为从服务器掉线以后重新连上主服务器所需的平均时间（以秒计算）\n- write_size_per_second为主服务器平均每秒产生的写命令数据量大小\n\n> 环形缓冲区的大小不能低于平均掉线时间*主服务器平均每秒产生的数据量，要不然会频繁bgsave影响主进程，开销大\n> \n> 举个例子，如果主服务器平均每秒产生 1 MB 的写命令，而从服务器断线之后平均要 5 秒才能重新连接主服务器。\n>\n> 那么 repl_backlog_buffer 大小就不能低于 5 MB，否则新写地命令就会覆盖旧数据了。\n>\n> 当然，为了应对一些突发的情况，可以将 repl_backlog_buffer 的大小设置为此基础上的 2 倍，也就是 10 MB。\n> \n> 关于 repl_backlog_buffer 大小修改的方法，只需要修改配置文件里下面这个参数项的值就可以。 repl-backlog-size 1mb\n\n\n# 哨兵机制\n\n在 Redis 的主从架构中，由于主从模式是读写分离的，如果主节点（master）挂了，那么将没有主节点来服务客户端的写操作请求，也没有主节点给从节点（slave）进行数据同步了。\n![](../img/Redis/img_17.png)\n这时如果要恢复服务的话，需要人工介入，选择一个「从节点」切换为「主节点」，然后让其他从节点指向新的主节点，同时还需要通知上游那些连接 Redis 主节点的客户端，将其配置中的主节点 IP 地址更新为「新主节点」的 IP 地址。\n\n这样也不太“智能”了，要是有一个节点能监控「主节点」的状态，当发现主节点挂了，它自动将一个「从节点」切换为「主节点」的话，那么可以节省我们很多事情啊！\n\nRedis 在 2.8 版本以后提供的**哨兵（Sentinel）机制**，它的作用是实现**主从节点故障转移**。它会监测主节点是否存活，如果发现主节点挂了，它就会选举一个从节点切换为主节点，并且把新主节点的相关信息通知给从节点和客户端\n\n\n## 哨兵的功能\n- 监控：Sentinel会定期检查master和slave是否按照预期工作（通过心跳机制）\n- 自动故障恢复：如果master故障，就选举出一个slave作为新的master，然后进行主节点故障迁移\n- 通知：当发生故障转移时，会将最新信息发送给Redis客户端 ![](../img/Redis/img_18.png)\n\n## 服务状态监控\n\n> Sentinel基于心跳机制ping-pong，每隔1秒向每个集群内的实例发送ping。\n> - 主观下线：如果某sentinel节点发现实例没有在规定时间内响应，那就标记成**主观下线**，这个规定的时间是配置项down-after-milliseconds 参数设定的，单位是毫秒。\n> - 客观下线：有的时候主节点其实并没有发生故障，只是因为网络拥塞，导致没有在规定时间响应Ping。为了减少误判的情况，哨兵不会之配置成一个节点（至少有三台机器来部署哨兵集群）如果超过指定数量（quorum）的哨兵认为该实例主观下线，那么该节点就是**客观下线**，那就被认为是故障。\n\n哨兵会每隔 1 秒给所有主从节点发送 PING 命令，当主从节点收到 PING 命令后，会发送一个响应命令给哨兵，这样就可以判断它们是否在正常运行。\n\n如果主节点或者从节点没有在规定的时间内响应哨兵的 PING 命令，哨兵就会将它们标记为「主观下线」。这个「规定的时间」是配置项 down-after-milliseconds 参数设定的，单位是毫秒。\n\n之所以针对「主节点」设计「主观下线」和「客观下线」两个状态，是因为有可能「主节点」其实并没有故障，可能只是因为主节点的系统压力比较大或者网络发送了拥塞，导致主节点没有在规定时间内响应哨兵的 PING 命令。\n\n所以，为了减少误判的情况，哨兵在部署的时候不会只部署一个节点，而是用多个节点部署成哨兵集群（最少需要三台机器来部署哨兵集群），通过多个哨兵节点一起判断，就可以就可以避免单个哨兵因为自身网络状况不好，而误判主节点下线的情况。同时，多个哨兵的网络同时不稳定的概率较小，由它们一起做决策，误判率也能降低。\n\n具体是怎么判定主节点为「客观下线」的呢？\n\n当一个哨兵判断主节点为「主观下线」后，就会向其他哨兵发起命令，其他哨兵收到这个命令后，就会根据自身和主节点的网络状况，做出赞成投票或者拒绝投票的响应。\n\n![](../img/Redis/img_19.png)\n\n当这个哨兵的赞同票数达到哨兵配置文件中的 quorum 配置项设定的值后，这时主节点就会被该哨兵标记为「客观下线」。\n\n例如，现在有 3 个哨兵，quorum 配置的是 2，那么一个哨兵需要 2 张赞成票，就可以标记主节点为“客观下线”了。这 2 张赞成票包括哨兵自己的一张赞成票和另外两个哨兵的赞成票（**包括自己的一票**）。\n\nPS：quorum 的值一般设置为哨兵个数的二分之一加 1，例如 3 个哨兵就设置 2。\n\n哨兵判断完主节点客观下线后，哨兵就要开始在多个「从节点」中，选出一个从节点来做新主节点\n\n## 由哪个哨兵进行主从故障转移？\n> 假设刚刚在哨兵的内部已经将主节点标记成了**客观下线**，那么哨兵集群哪个节点来对其进行故障转移呢？\n> \n> 这个时候需要选举一个哨兵集群的leader来进行故障转移，但是在投票之前肯定需要一个**候选者**，这个候选者一般是首先发现主节点的哨兵。\n> \n> 那么怎么样成为leader呢，哨兵集群需要进行投票，每个哨兵只有一次投票机会，只有候选者能够投给自己（一般也只有一个候选者，除非出现同一时间点有两个哨兵发现了主节点故障发起主观下线）\n> \n> 在投票过程中，只要候选者达到以下条件就可以变成leader：\n> - 第一，拿到半数以上的赞成票\n> - 第二，票数还要同时大于等于quorum 值\n\n**如果某个时间点，刚好有两个哨兵节点判断到主节点为客观下线，那这时不就有两个候选者了？这时该如何决定谁是 Leader 呢？**\n\n每位候选者都会先给自己投一票，然后向其他哨兵发起投票请求。如果投票者先收到「候选者 A」的投票请求，就会先投票给它，如果投票者用完投票机会后，收到「候选者 B」的投票请求后，就会拒绝投票。这时，候选者 A 先满足了上面的那两个条件，所以「候选者 A」就会被选举为 Leader。\n\n**Redis 1 主 4 从，5 个哨兵，quorum 设置为 3，如果 2 个哨兵故障，当主节点宕机时，哨兵能否判断主节点“客观下线”？主从能否自动切换？**\n\n- 哨兵集群可以判定主节点“客观下线”。哨兵集群还剩下 3 个哨兵，当一个哨兵判断主节点“主观下线”后，询问另外 2 个哨兵后，有可能能拿到 3 张赞同票，这时就达到了 quorum 的值，因此，哨兵集群可以判定主节点为“客观下线”。\n\n- 哨兵集群可以完成主从切换。当有个哨兵标记主节点为「客观下线」后，就会进行选举 Leader 的过程，因为此时哨兵集群还剩下 3 个哨兵，那么还是可以拿到半数以上（5/2+1=3）的票，而且也达到了 quorum 值，满足了选举 Leader 的两个条件，所以就能选举成功，因此哨兵集群可以完成主从切换。\n\n如果 quorum 设置为 2，并且如果有 3 个哨兵故障的话。此时哨兵集群还是可以判定主节点为“客观下线”，但是哨兵不能完成主从切换了，大家可以自己推演下。\n\n如果 quorum 设置为 3，并且如果有 3 个哨兵故障的话，哨兵集群即不能判定主节点为“客观下线”，也不能完成主从切换了。\n\n可以看到，quorum 为 2 的时候，并且如果有 3 个哨兵故障的话，虽然可以判定主节点为“客观下线”，但是不能完成主从切换，这样感觉「判定主节点为客观下线」这件事情白做了一样，既然这样，还不如不要做，quorum 为 3 的时候，就可以避免这种无用功。\n\n**所以，quorum 的值建议设置为哨兵个数的二分之一加 1，例如 3 个哨兵就设置 2，5 个哨兵设置为 3，而且哨兵节点的数量应该是奇数。**\n\n## 主从故障转移的过程\n\n> 主从故障转移操作包含以下四个步骤： \n> - 第一步：在已下线主节点（旧主节点）属下的所有「从节点」里面，挑选出一个从节点，并将其转换为主节点。\n> - 第二步：让已下线主节点属下的所有「从节点」修改复制目标，修改为复制「新主节点」；\n> - 第三步：将新主节点的 IP 地址和信息，通过「发布者/订阅者机制」通知给客户端； \n> - 第四步：继续监视旧主节点，当这个旧主节点重新上线时，将它设置为新主节点的从节点\n\n### 步骤一：选出新节点\n\n故障转移操作第一步要做的就是在已下线主节点属下的所有「从节点」中，挑选出一个状态良好、数据完整的从节点，然后向这个「从节点」发送 SLAVEOF no one 命令，将这个「从节点」转换为「主节点」。\n\n那么多「从节点」，到底选择哪个从节点作为新主节点的？\n\n随机的方式好吗？随机的方式，实现起来很简单，但是如果选到一个网络状态不好的从节点作为新主节点，那么可能在将来不久又要做一次主从故障迁移。\n\n所以，我们首先要把网络状态不好的从节点给过滤掉。首先把已经下线的从节点过滤掉，然后把以往网络连接状态不好的从节点也给过滤掉。\n\n怎么判断从节点之前的网络连接状态不好呢？\n\nRedis 有个叫 down-after-milliseconds * 10 配置项，其 down-after-milliseconds 是主从节点断连的最大连接超时时间。如果在 down-after-milliseconds 毫秒内，主从节点都没有通过网络联系上，我们就可以认为主从节点断连了。如果发生断连的次数超过了 10 次，就说明这个从节点的网络状况不好，不适合作为新主节点。\n\n至此，我们就把网络状态不好的从节点过滤掉了，接下来要对所有从节点进行三轮考察：**优先级、复制进度、ID 号**。在进行每一轮考察的时候，哪个从节点优先胜出，就选择其作为新主节点。\n\n1. 第一轮考察：哨兵首先会根据从节点的优先级来进行排序，优先级越小排名越靠前， \n2. 第二轮考察：如果优先级相同，则查看复制的下标，哪个从「主节点」接收的复制数据多，哪个就靠前（也就是offset更靠近主节点那个）\n3. 第三轮考察：如果优先级和下标都相同，就选择从节点 ID 较小的那个。\n\n![](../img/Redis/img_20.png)\n\n### 步骤二：将从节点指向主节点\n\n当新主节点出现之后，哨兵 leader 下一步要做的就是，让已下线主节点属下的所有「从节点」指向「新主节点」，这一动作可以通过向「从节点」发送 SLAVEOF 命令来实现。\n\n如下图，哨兵 leader 向所有从节点（server3 和 server4）发送 SLAVEOF ，让它们成为新主节点的从节点。\n\n![](../img/Redis/img_21.png)\n\n![](../img/Redis/img_22.png)\n\n### 步骤三：通知客户的主节点已更换\n\n经过前面一系列的操作后，哨兵集群终于完成主从切换的工作，那么新主节点的信息要如何通知给客户端呢？\n\n这主要通过 Redis 的发布者/订阅者机制来实现的。每个哨兵节点提供发布者/订阅者机制，客户端可以从哨兵订阅消息。\n\n> 类似kafka那种订阅机制，也可以想象成计网的那种广播信道，每个哨兵进行广播，客户端接受相应表头的时间，完成通知\n\n哨兵提供的消息订阅频道有很多，不同频道包含了主从节点切换过程中的不同关键事件，几个常见的事件如下：\n![](../img/Redis/img_23.png)\n\n客户端和哨兵建立连接后，客户端会订阅哨兵提供的频道。主从切换完成后，哨兵就会向 +switch-master 频道发布新主节点的 IP 地址和端口的消息，这个时候客户端就可以收到这条信息，然后用这里面的新主节点的 IP 地址和端口进行通信了。\n\n通过发布者/订阅者机制机制，有了这些事件通知，客户端不仅可以在主从切换后得到新主节点的连接信息，还可以监控到主从节点切换过程中发生的各个重要事件。这样，客户端就可以知道主从切换进行到哪一步了，有助于了解切换进度。\n\n### 步骤四：将旧主节点变为从节点\n\n故障转移操作最后要做的是，继续监视旧主节点，当旧主节点重新上线时，哨兵集群就会向它发送 SLAVEOF 命令，让它成为新主节点的从节点\n\n至此，整个主从节点的故障转移的工作结束","tags":["Redis"]},{"title":"redis-持久化篇","url":"/2024/04/01/redis-持久化篇/","content":"# Redis持久化\nRedis 是**内存数据库**，如果不将内存中的数据库状态保存到磁盘，那么一旦服务器进程退出，服务器中的数据库状态也会消失。所以 Redis 提供了**持久化功能 **!\n## RDB\nRDB全称Redis Database Backup file（Redis数据备份文件），也被叫做Redis数据快照。简单来说就是定期把内存所有数据都记录到磁盘中。\n\n一旦Redis实例出现故障重启，就会从磁盘中读取快照文件，恢复数据\n\n> 什么是快照？可以理解成定期给当前的内存中的redis数据拍一张照，然后保存下来。\n\n### RDB触发机制\n  可以分为手动触发和自动触发，手动的save占用主进程，bgsave是fork一个新进程来完成快照保存，在此期间主进程通过copyonwrite的机制（类似copyonwrite的Arraylist那样），拷贝一份当前数据，在拷贝的地方进行修改，等到子进程完成磁盘写之后再进行覆盖。\n\n  自动触发写在redis.conf上，save X Y，表示在X秒内如果有Y个key修改，那么就save（注意这里触发的还是bgsave）\n![](../img/Redis/img_7.png)\n- 手动触发\n![](../img/Redis/img_6.png)\n用bgsave可以防止主进程被阻塞，提高效率\n\nfork采用的是copy-on-write\n- 当主进程执行读操作的时候可以访问共享内存\n- 当主进程执行写操作的时候，则会拷贝一份数据\n\n![](../img/Redis/img_9.png)\n> 子进程和主线程怎么共享内存，也就是怎么让子进程知道redis的内存是那些块？通过拷贝主进程的页表（没错就是计组那个页表），这样就知道主进程占用的虚拟地址和物理地址的映射关系，从而找到这些块。\n> \n> copyonwrite会将此时的共享内存变成**只读**，主进程来读的时候就可以直接读，写的时候拷贝一份。\n- 自动触发\n![](../img/Redis/img_8.png)\n> 注意这里save其实也是bgsave，一般都不会影响主进程\n\n### 优缺点\n**优点**：\n- 只有一个dump.rdb，方便持久化，是一个紧凑的二进制文件，恢复的时候速度也比AOF更快，在数据量大的时候更明显。\n- 实现了性能的最大化，fork子进程来完成而不影响主进程，保证了Redis的高性能。\n\n**缺点**：\n- 可能存在数据丢失，在两次RDB的时间里，如果出现宕机，这段时间没有写入的数据都将丢失\n- 没有办法做到秒级持久化/实时持久化\n- 对于cpu和内存的开销比较大，毕竟要fork一个新进程占用cpu，还要进行COW占用内存而，AOF主要占用的是IO资源\n\n**使用场景**：\n- 可以容忍数分钟数据丢失，追求更快的启动速度和恢复速度。\n\n## AOF\nAOF（append only file） 持久化，采用日志的形式来记录每个写操作，追加到AOF文件的末尾。\n> 意思就是把操作的每一条语句都记录下来，那肯定占用的空间大\n\nRedis默认情况是不开启AOF的。重启时再重新执行AOF文件中的命令来恢复数据。它主要解决数据持久化的实时性问题。\n\n![](../img/Redis/img_10.png)\n\nAOF是**执行完命令后才记录日志**的。为什么不先记录日志再执行命令呢？这是因为Redis在向AOF记录日志时，不会先对这些命令进行语法检查，如果先记录日志再执行命令，日志中可能记录了错误的命令，Redis使用日志回复数据时，可能会出错。\n\n正是因为执行完命令后才记录日志，所以不会阻塞当前的写操作。但是会存在两个风险：\n\n- 更执行完命令还没记录日志时，宕机了会导致数据丢失\n- AOF不会阻塞当前命令，但是可能会阻塞下一个操作。\n\n> 这两个风险最好的解决方案是折中妙用AOF机制的三种写回策略 appendfsync：\n>- always，同步写回，每个子命令执行完，都立即将日志写回磁盘。\n>- everysec，每个命令执行完，只是先把日志写到AOF内存缓冲区，每隔一秒同步到磁盘。\n>- no：只是先把日志写到AOF内存缓冲区，有操作系统去决定何时写入磁盘。\n\n![](../img/Redis/img_11.png)\n\nalways同步写回，可以基本保证数据不丢失，no策略则性能高但是数据可能会丢失，一般可以考虑折中选择everysec。\n\n如果接受的命令越来越多，AOF文件也会越来越大，文件过大还是会带来性能问题。日志文件过大怎么办呢？\n> **AOF重写机制**就是随着时间推移，AOF文件会有一些冗余的命令如：无效命令、过期数据的命令等等，AOF重写机制就是把它们合并为一个命令（类似批处理命令），从而达到精简压缩空间的目的。\n> \n> AOF重写会阻塞嘛？AOF日志是由主线程会写的，而重写则不一样，重写过程是由后台子进程bgrewriteaof完成。\n\n\n### 优缺点\n**优点**：\n- 实时持久化，数据安全，AOF持久化配置为always就可以基本上避免丢失，但是开销比较大，通常用容忍1s内丢失的everysec\n- 通过append模式写文件，那么即使中途服务器宕机，也可以尽量解决一致性问题（取决于刷盘策略）\n- 重写机制，在文件太大的时候可以压缩空间\n\n**缺点**：\n- 文件比RDB大，恢复时间也会慢很多\n- 启动效率低\n- 文件体积迅速变大，需要定期执行重写机制降低文件体积。\n\n\n**使用场景**：\n- 对数据安全性有较高需求的场景\n\n> Redis4.0开始支持RDB和AOF的混合持久化，就是内存快照以一定频率执行，两次快照之间，再使用AOF记录这期间的所有命令操作。\n\n","tags":["Redis"]},{"title":"redis-缓存三兄弟","url":"/2024/03/31/redis-缓存三兄弟/","content":"# 缓存穿透\n> 这里的redis可以全部想象成cache，数据库可以想象成cpu，穿透有点绕过redis的意思\n\n![](../img/Redis/img.png)\n假设有一个请求api/news/getById/1，但是这个id为1的键并**不存在**于数据库，所以也不会存在于缓存，那么如果有很多这种请求，那么都会绕过redis去查询数据库，造成数据库的负载。\n\n## 解决方法\n1. 缓存空数据，就算是数据库找不到也要缓存一个空数据给redis，要不然就会一直会去绕过redis去找数据库。比如给的key=1但是mysql没有相应数据，那么在redis上缓存一个key=1，value=null的键值对。\n- **优点**：非常简单，维护方便\n- **缺点**：会造成额外的内存消耗，因为redis是存在内存中的，有多少个没有的键就会有多少个空，浪费内存。还可能造成**短期不一致**问题，如果在redis里面存了这个空但是这个时候数据库更新了这个键值对，那么在这一段时间访问redis的还是null，就会造成不一致的问题\n2. 布隆过滤器\n在客户端与Redis之间加了一个布隆过滤器，对于请求进行过滤。\n\n![](../img/Redis/img_1.png)\n\n> **布隆过滤器**\n> - bitmap（位图）:bit位的数组，数组的元素是0或者1，所以布隆过滤器占用的内存小\n> - ![](../img/Redis/img_2.png)\n> - 作用：可以用来检索元素，相当于一种校验电路\n> - 实现原理：类似hashmap，假设来了一个键，用三种不同的hash算法得到位图的三个位置，然后这些位置都置为1，这里要跟redis缓存的保持一致，在初始化的时候就要预热进布隆过滤器。那么就会有很多重复的，数组越大这种情况会越少。那么此时有一个**确实不存在的元素**，但是计算出来的三个哈希值就是都是1，那么还是会发生缓存穿透的现象\n> - 判断不存在的时候一定不存在，也就是说3个地方一定都不为1\n> - 判断存在的时候不一定存在，因为有可能会有误判\n> - **优点**：内存占用小，没有多余key\n> - **缺点**：实现比较复杂，存在误判，大概在5%左右，但是对于数据库也是可以接受的\n\n布隆过滤器的大致的原理：布隆过滤器中存放二进制位。数据库的数据通过hash算法计算其hash值并存放到布隆过滤器中，之后判断数据是否存在的时候，就是判断该hash值是0还是1。\n\n但是这个玩意是一种概率上的统计，当其判断不存在的时候就一定是不存在；当其判断存在的时候就不一定存在。所以有一定的穿透风险！！！\n\n# 缓存击穿\n缓存击穿，指的是一个key在不断地支撑高并发，高并发持续对这个点进行访问，当这个点在失效的瞬间，50ms左右，大量的高并发就冲坡缓存请求数据库，造成数据库瘫痪。\n\n对于一般的网站而言很难有缓存击穿的级别，一般是热门网站或者秒杀瞬时高并发。\n## 解决方法\n1. 互斥锁\n\n![](../img/Redis/img_3.png)\n\n类似pv操作，给临界区上互斥锁，一段时间只要一个线程在重建数据就行，但是这样会造成其他线程等待。通常是需要强一致性的应用需要这种策略。\n- **优点**：强一致性，在redis没有更新完之前都不许访问，只能要最新的，一般涉及钱的都会有这种逻辑\n- **缺点**：性能差\n\n2. 逻辑过期\n\n![](../img/Redis/img_4.png)\n\n逻辑过期就是在一个键值对的最后加上逻辑时间，并不真实设置过期时间。也就是说，不会真正的过期只会逻辑上的过期。\n\n当一个线程去访问一个逻辑时间到了的键值对，那么其实redis也还有这部分，他还是**那这部分旧的数据**，但是会开启一个新的线程来完成数据库到mysql的更新\n- **优点**：可用性强，性能强在不注重强一致性的场景，不会有其他线程等待的情况，但是拿的都是旧数据\n- **缺点**：一致性弱\n\n\n# 缓存雪崩\n\n如果缓存集中在一段时间内过期，那么会有大量的缓存穿透，所有的查询都落在数据库上，造成缓存雪崩\n\n> 缓存雪崩和缓存击穿的区别：缓存击穿是针对某一个key，缓存雪崩是针对很多key集中过期，\n\n![](../img/Redis/img_5.png)\n\n## 解决方法：\n1. 给不同的key的TTL添加随机值\n2. 多个redis集群提高服务的可用性","tags":["Redis"]},{"title":"周赛2024-3-31","url":"/2024/03/31/周赛2024-3-31/","content":"> 因为第三题long被卡了三发wa，所以即使做出了第三题也没上次排名高\n\n# 第一题：哈沙德数\n\n如果一个整数能够被其各个数位上的数字之和整除，则称之为 **哈沙德数**（Harshad number）。给你一个整数 x 。如果 x 是 **哈沙德数** ，则返回 x 各个数位上的数字之和，否则，返回 -1 。\n\n> 字符串拆分成数组，然后累计和，最后做个判断\n```java\nclass Solution {\n    public int sumOfTheDigitsOfHarshadNumber(int x) {\n        String num = \"\"+x;\n        String[] split = num.split(\"\");\n        int ans = 0;\n        //System.out.println(Arrays.toString(split));\n        for (int i = 0; i < split.length; i++) {\n            ans+=Integer.parseInt(split[i]);\n        }\n        if (x % ans ==0){\n            return ans;\n        }\n        else {\n            return -1;\n        }\n    }\n}\n```\n# 第二题：换水问题\n给你两个整数 numBottles 和 numExchange 。\n\nnumBottles 代表你最初拥有的满水瓶数量。在一次操作中，你可以执行以下操作之一：\n\n- 喝掉任意数量的满水瓶，使它们变成空水瓶。\n- 用 numExchange 个空水瓶交换一个满水瓶。然后，将 numExchange 的值增加 1 。\n注意，你不能使用相同的 numExchange 值交换多批空水瓶。例如，如果 numBottles == 3 并且 numExchange == 1 ，则不能用 3 个空水瓶交换成 3 个满水瓶。\n\n返回你 **最多** 可以喝到多少瓶水。\n\n![](../img/coding/2024_3_31_2.png)\n> 小学奥数貌似碰到过这样的问题，就是没兑换一次，兑换的要求就多一瓶，直接暴力模拟就行\n```java\nclass Solution {\n    public int maxBottlesDrunk(int numBottles, int numExchange) {\n        int empty = numBottles;\n        int ans = numBottles;\n        //直到换不了了\n        while (empty >= numExchange){\n            //兑换新的之后剩下的\n            empty -=numExchange;\n            numExchange++;\n            //兑换多的\n            empty +=1;\n            ans++;\n        }\n        return ans;\n    }\n}\n```\n# 第三题：交替子数组计数\n给你一个**二进制数组**nums 。\n\n如果一个子数组中 **不存在** 两个 **相邻** 元素的值 **相同** 的情况，我们称这样的子数组为 **交替子数组** 。\n\n返回数组 nums 中交替子数组的数量。\n\n示例 1：\n\n输入： nums = [0,1,1,1]\n\n输出： 5\n\n解释： 以下子数组是交替子数组：[0] 、[1] 、[1] 、[1] 以及 [0,1] 。\n> 这里有个用例真的好恶心，wa三次都是这一个用例没过，需要全开long或者long long才能过\n## 记忆化搜索超出内存限制\n> 最早的思路是用一个二维数组存dp[i][j]是否是一个交替子数组，为1表示i到j是一个交替子数组，判断条件$$ dp[i][j-1] == 1 && nums[j] != nums[j-1] $$\n> \n> 其实完全不用这么大的空间，二维数组超空间也是应该的\n```java\nclass Solution {\n    public long countAlternatingSubarrays(int[] nums) {\n        //记忆化搜索\n        int[][] dp = new int[nums.length][nums.length];\n        for (int i = 0; i < nums.length; i++) {\n            for (int j = i; j < nums.length; j++) {\n                //对角线肯定是交替子数组\n                if (i==j){\n                    dp[i][j] = 1;\n                }\n                else {\n                    //只有前面是一个交替子数组并且不等于最后的那个元素\n                    if (dp[i][j-1] == 1 && nums[j] != nums[j-1]){\n                        dp[i][j] = 1;\n                    }\n                    else {\n                        dp[i][j] = 0;\n                    }\n                }\n            }\n        }\n        //最后再遍历一次数组，每一个1就是一个交替子数组\n        long ans = 0;\n        for (int i = 0; i < nums.length; i++) {\n            for (int j = i; j < nums.length; j++) {\n                if (dp[i][j] == 1){\n                    ans+=1;\n                }\n            }\n        }\n        return ans;\n\n    }\n}\n```\n\n## 双指针(AC)\n> 思路是，如果一个数组是交替数组，那么子数组肯定也是，利用双指针找到尽量最长的交替子数组存起来，最后遍历这些最长的交替子数组。由于他们的子数组肯定也是交替的，那就是等差数列求和就可以算出。最后累加得到答案\n```java\nclass Solution {\n    public long countAlternatingSubarrays(int[] nums) {\n        //恶心的long\n        List<long[]> list = new ArrayList<>();\n        long head = 0;\n        long rear = 0;\n        //双指针找到长的数组，存起来他们的下标\n        for (int i = 1; i < nums.length; i++) {\n            if (nums[i] == nums[i-1]){\n                list.add(new long[]{head,rear});\n                head = i;\n            }\n            rear++;\n        }\n        //最后一个没加上就退出循环了，需要最后加上\n        list.add(new long[]{head,rear});\n        long ans = 0;\n        for (int i = 0; i < list.size(); i++) {\n            long length = list.get(i)[1]-list.get(i)[0]+1;\n            //子数组的数量\n            ans += length *(length+1)/2;\n        }\n        return ans;\n    }\n}\n```\n时间复杂度:o(n)\n空间复杂度：o(n)","tags":["刷题笔记"]},{"title":"leetcode-2024-3-31","url":"/2024/03/31/leetcode-2024-3-31/","content":"# 331 验证二叉树的前序序列化（Medium）\n\n序列化二叉树的一种方法是使用 **前序遍历** 。当我们遇到一个非空节点时，我们可以记录下这个节点的值。如果它是一个空节点，我们可以使用一个标记值记录，例如 #。\n![](../img/coding/2024_3_31_1.png)\n\n例如，上面的二叉树可以被序列化为字符串 \"9,3,4,#,#,1,#,#,2,#,6,#,#\"，其中 # 代表一个空节点。\n\n给定一串以逗号分隔的序列，验证它是否是正确的二叉树的前序序列化。编写一个在不重构树的条件下的可行算法。\n\n保证 每个以逗号分隔的字符或为一个整数或为一个表示 null 指针的 '#' 。\n\n你可以认为输入格式总是有效的\n\n例如它永远不会包含两个连续的逗号，比如 \"1,,3\" 。\n\n注意：不允许重建树。\n\n## 栈\n> 由于是先序遍历，我们考虑栈。栈的存放一个数组，第一位表示在split中的下标，第二位表示左边是否有节点，第三位表示右边是否有节点\n> \n> 遍历这个数组，然后如果栈非空，当前元素就是栈顶元素的左孩子或者右孩子，更新栈顶数组，每次遍历都要判断栈顶元素是否左右都有，如果有就出栈。\n> 遍历完看栈是否为空，为空说明是二叉树\n\n> 这里有两个细节，\n> - 首先是可能出现多棵树的情况，也就是说以上算法对于两个完整的树拼在一起的字符串结果还是为true，需要一个计数器，如果在循环过程中栈就已经空了，就返回false\n> - 还有就是这里用了trycatch，如果出现异常情况比如栈顶为空了，那肯定是出问题了，直接返回false，也算是一种偷懒吧。\n```java\npublic boolean isValidSerialization(String preorder) {\n    try {\n        int count = 0;\n        String[] split = preorder.split(\",\");\n        if (split.length == 1 && Objects.equals(split[0], \"#\")){\n            return true;\n        }\n        Stack<int[]> stack = new Stack<>();\n        for (int i = 0; i < split.length; i++) {\n            if (!Objects.equals(split[i], \"#\")){\n                if (!stack.isEmpty()){\n                    if (stack.peek()[1] ==0 && stack.peek()[2] == 0){\n                        stack.peek()[1]  = 1;\n                    }\n                    else if (stack.peek()[1] == 1 && stack.peek()[2] == 0){\n                        stack.peek()[2] = 1;\n                    }\n\n                }\n                stack.add(new int[]{i,0,0});\n            }\n            else {\n                if (stack.peek()[1] == 0 && stack.peek()[2] ==0){\n                    stack.peek()[1]  = 1;\n                }\n                else if (stack.peek()[1] == 1 && stack.peek()[2] == 0){\n                    stack.peek()[2] = 1;\n                }\n            }\n            while (!stack.isEmpty() && stack.peek()[1] == 1 && stack.peek()[2] == 1){\n                stack.pop();\n            }\n            if (stack.isEmpty() && i != split.length-1){\n                count++;\n            }\n        }\n        if (count!=0){\n            return false;\n        }\n        return stack.isEmpty();\n    }\n    catch (Exception e){\n        return false;\n    }\n\n}\n```\n时间复杂度：O(n)，其中 n 为字符串的长度。我们每个字符只遍历一次，同时每个字符对应的操作都是常数时间的。\n\n空间复杂度：O(n)。此为栈所需要使用的空间。\n","tags":["刷题笔记"]},{"title":"leetcode-2024-3-30","url":"/2024/03/30/leetcode-2024-3-30/","content":"# 2952 需要添加的硬币的最小数量（Medium）\n> 这道题止中等？\n> \n给你一个下标从 0 开始的整数数组 coins，表示可用的硬币的面值，以及一个整数 target 。\n\n如果存在某个 coins 的子序列总和为 x，那么整数 x 就是一个 **可取得的金额** 。\n\n返回需要添加到数组中的 **任意面值** 硬币的 最小数量 ，使范围 $[1, target]$ 内的每个整数都属于 **可取得的金额** 。\n\n数组的 **子序列** 是通过删除原始数组的一些（**可能不删除**）元素而形成的新的 **非空** 数组，删除过程不会改变剩余元素的相对位置。\n\n\n\n示例 1：\n\n输入：coins = [1,4,10], target = 19\n\n输出：2\n\n解释：需要添加面值为 2 和 8 的硬币各一枚，得到硬币数组 [1,2,4,8,10] 。\n\n可以证明从 1 到 19 的所有整数都可由数组中的硬币组合得到，且需要添加到数组中的硬币数目最小为 2 。\n\n> 自己想没想出来\n## 官方解：贪心算法\n\n为方便描述，把 0也算作可以得到的数。\n\n假设现在得到了区间 $[0,s−1]$ 中的所有整数，如果此时遍历到整数 $x=coins[i]$，那么把 $[0,s−1]$ 中的每个整数都增加 x，我们就得到了区间 $[x,s+x−1]$ 中的所有整数。\n\n把 coins 从小到大排序，遍历 $x=coins[i]$。分类讨论，看是否要添加数字：\n- 如果 x≤s，那么合并 $[0,s−1]$ 和 $[x,s+x−1]$这两个区间，我们可以得到 $[0,s+x−1]$ 中的所有整数。\n- 如果 x>s，或者遍历完了 coins 数组，这意味着我们无法得到 s，那么就一定要把 s 加到数组中（加一个比 s 还小的数字就没法得到更大的数，不够贪），这样就可以得到了 $[s,2s−1]$ 中的所有整数，再与 $[0,s−1]$ 合并，可以得到 $[0,2s−1]$ 中的所有整数。然后再考虑 x 和 2s 的大小关系，继续分类讨论。\n当 s>targets 时，我们就得到了 $[1,target]$ 中的所有整数，退出循环。\n\n```java\nclass Solution {\n    public int minimumAddedCoins(int[] coins, int target) {\n        Arrays.sort(coins);\n        int ans = 0, s = 1, i = 0;\n        while (s <= target) {\n            if (i < coins.length && coins[i] <= s) {\n                s += coins[i++];\n            } else {\n                s *= 2; // 必须添加 s\n                ans++;\n            }\n        }\n        return ans;\n    }\n}\n```","tags":["刷题笔记"]},{"title":"leetcode-2024-3-29","url":"/2024/03/29/leetcode-2024-3-29/","content":"> 经过一天的冲业绩终于上了200题，说实话有点追求数量不追求质量了。但是我确实也感觉记性没以前那么好了，菜就多练。\n> \n> ![](../img/coding/2024_3_29_2.png)\n\n# 2908 元素和最小的山形三元组（Eazy）\n\n给你一个下标从 0 开始的整数数组 nums 。\n\n如果下标三元组$ (i, j, k) $满足下述全部条件，则认为它是一个 **山形三元组** ：\n\n- $i < j < k$\n- $nums[i] < nums[j]$ 且 $nums[k] < nums[j]$\n请你找出 nums 中 **元素和最小** 的山形三元组，并返回其 **元素和** 。如果不存在满足条件的三元组，返回 -1 。\n\n\n\n示例 1：\n\n输入：$nums = [8,6,1,5,3]$\n输出：9\n解释：三元组 (2, 3, 4) 是一个元素和等于 9 的山形三元组，因为：\n- 2 < 3 < 4\n- $nums[2] < nums[3]$ 且 $nums[4] < nums[3]$\n\n这个三元组的元素和等于 $nums[2] + nums[3] + nums[4] = 9$ 。可以证明不存在元素和小于 9 的山形三元组。\n\n## 暴力枚举（居然没超时）\n> 没什么好说的直接遍历所有三元组，判断是否是山形\n```java\nclass Solution {\n    public int minimumSum(int[] nums) {\n        int min = Integer.MAX_VALUE;\n        //遍历所有三元组\n        for (int i = 0; i < nums.length - 2; i++) {\n            for (int j = i+1; j < nums.length-1; j++) {\n                for (int k = j+1; k < nums.length; k++) {\n                    if (nums[i] < nums[j] && nums[j] >nums[k]){\n                        //找最小值\n                        min = Math.min(min,nums[i]+nums[j]+nums[k]);\n                    }\n                }\n            }\n        }\n        if (min == Integer.MAX_VALUE){\n            min = -1;\n        }\n        return min;\n    }\n}\n```\n时间复杂度：o(n^3)，这么高的复杂度没超时就算了，居然还77%？有点搞笑了\n空间复杂度：o(1)，只用了一个min来保存最大值\n\n# 有效的括号（Easy）\n\n> 今天的主菜是各种栈，先从最简单的说起\n\n给定一个只包括 '('，')'，'{'，'}'，'['，']' 的字符串 s ，判断字符串是否有效。\n\n有效字符串需满足：\n\n1. 左括号必须用相同类型的右括号闭合。\n2. 左括号必须以正确的顺序闭合。\n3. 每个右括号都有一个对应的相同类型的左括号。\n\n示例 1：\n\n输入：s = \"()\"\n\n输出：true\n\n示例 2：\n\n输入：s = \"()[]{}\"\n\n输出：true\n\n示例 3：\n\n输入：s = \"(]\"\n\n输出：false\n\n## 简单栈\n> 最基础的括号匹配问题，所有左边括号进栈，如果能和右边括号匹配就出栈，到最后如果还有括号没有匹配上，就返回false，如果为空就是true\n```java\nclass Solution {\n    public boolean isValid(String s) {\n        Stack<Character> stack = new Stack<>();\n        //flag用来表示栈顶不是匹配的，比如“]”，栈顶没有元素，返回false\n        boolean flag = true;\n        for (int i = 0; i < s.length(); i++) {\n            char temp = s.charAt(i);\n            if (temp == '}'){\n                //匹配左边括号，出栈\n                if (!stack.isEmpty()&&stack.peek() == '{'){\n                    stack.pop();\n                }\n                else {\n                    flag = false;\n                }\n            }\n            else if (temp == ')'){\n                //匹配左边括号，出栈\n                if (!stack.isEmpty()&&stack.peek() == '('){\n                    stack.pop();\n                }\n                else {\n                    flag = false;\n                }\n            }\n            else if (temp == ']'){\n                //匹配左边括号，出栈\n                if (!stack.isEmpty()&&stack.peek() == '['){\n                    stack.pop();\n                }\n                else {\n                    flag = false;\n                }\n            }\n            //不是右边括号的话进栈\n            else stack.push(temp);\n        }\n        return flag & stack.isEmpty();\n    }\n}\n```\n时间复杂度:o(n)\n空间复杂度:o(n)，因为维护了一个栈\n\n# 394 字符串解码（Medium）\n\n给定一个经过编码的字符串，返回它解码后的字符串。\n\n编码规则为: $k[encoded_string]$，表示其中方括号内部的 encoded_string 正好重复 k 次。注意 k 保证为正整数。\n\n你可以认为输入字符串总是有效的；输入字符串中没有额外的空格，且输入的方括号总是符合格式要求的。\n\n此外，你可以认为原始数据不包含数字，所有的数字只表示重复的次数 k ，例如不会出现像 3a 或 2[4] 的输入。\n\n示例 1：\n\n输入：s = \"3[a]2[bc]\"\n\n输出：\"aaabcbc\"\n\n示例 2：\n\n输入：s = \"3[a2[c]]\"\n\n输出：\"accaccacc\"\n\n## 简单栈\n> 先进后出，又是用栈的经典问题，维护两个栈，一个放数字，一个放符号，这里考虑到数字也会又多位数字，所以在字符串中需要特意考虑多位数字。\n> 另一个栈用于放字符串，这里刚开始考虑用Char的栈，但是这样就会频繁出栈入栈，所以还不如就用字符串。\n```java\nclass Solution {\n    public String decodeString(String s) {\n        Stack<String> stack = new Stack<>();\n        Stack<Integer> numstack = new Stack<>();\n        //这里i手动改，因为涉及到多位数字的问题，需要拼接字符串\n        for (int i = 0; i < s.length();) {\n            //如果是数字，需要考虑多位数字的情况\n            if (judgeNumOrChar(s.charAt(i))){\n                StringBuilder num = new StringBuilder();\n                //拼接字符串\n                while (judgeNumOrChar(s.charAt(i))){\n                    num.append(s.charAt(i));\n                    i++;\n                }\n                //此时i的位置肯定是[\n                numstack.push(Integer.parseInt(num.toString()));\n            }\n            else {\n                //左括号直接进\n                if (s.charAt(i) != ']'){\n                    stack.push(\"\"+s.charAt(i));\n                }\n                else {\n                    StringBuilder temp = new StringBuilder();\n                    //当前的数字出栈\n                    int count = numstack.pop();\n                    //字符串拼接加在左边，因为先进后出\n                    while (!Objects.equals(stack.peek(), \"[\")){\n                        temp.insert(0, stack.pop());\n                        //System.out.println(stack);\n                    }\n                    //最后一个左括号出栈\n                    stack.pop();\n                    //接下来就是循环count次，字符串入栈，这里其实也可以拼接好了再放一个位置，我怕会有问题就没这么做\n                    for (int j = 0; j < count; j++) {\n                        stack.push(temp.toString());\n                    }\n\n                }\n                i++;\n\n            }\n            //System.out.println(stack);\n            //System.out.println(numstack);\n\n        }\n        StringBuilder ans = new StringBuilder();\n        while (!stack.isEmpty()){\n            ans.insert(0, stack.pop());\n        }\n        return ans.toString();\n    }\n    //判断是否为数字\n    public boolean judgeNumOrChar(Character character){\n        return (character < 'a' || character > 'z') && character != '[' && character != ']';\n    }\n}\n```\n时间复杂度o(n)\n空间复杂度，就是解码出的字符串的长度，这个没办法跟n有关\n\n# 739 每日温度（Medium）\n给定一个整数数组 temperatures ，表示每天的温度，返回一个数组 answer ，其中 answer[i] 是指对于第 i 天，下一个更高温度出现在几天后。如果气温在这之后都不会升高，请在该位置用 0 来代替。\n\n示例 1:\n\n输入: temperatures = [73,74,75,71,69,72,76,73]\n\n输出: [1,1,4,2,1,1,0,0]\n\n## 单调栈\n> 维护一个存储下标的单调递减栈，当要进入的元素比栈顶元素大的时候，弹出栈顶，那么栈顶对应的今日最高温度就是当前这个，数组中放进两者的日期差值，以此循环。直到进入元素比栈顶小\n```java\nclass Solution {\n    public int[] dailyTemperatures(int[] temperatures) {\n        //int[]，0表示当前元素，1表示下标\n        //其实可以完全存下标的，这里浪费空间了\n        Stack<int[]> monoStack = new Stack<>();\n        int[] ans = new int[temperatures.length];\n        for (int i = 0; i < temperatures.length; i++) {\n            //进入元素比栈顶大，直到比栈顶小\n            while (!monoStack.isEmpty() && monoStack.peek()[0] < temperatures[i]){\n                int[] latest = monoStack.pop();\n                //计算当前元素和出栈的差值，放在出栈的那个元素的下标位置\n                ans[latest[1]] = i-latest[1];\n            }\n            monoStack.push(new int[]{temperatures[i],i});\n        }\n        //最后如果留下几个递减的元素，那么表示后续没有更高温度，就全部为0\n        while (!monoStack.isEmpty()){\n            int[] latest = monoStack.pop();\n            ans[latest[1]] = 0;\n        }\n        return ans;\n    }\n}\n```\n时间复杂度o(n)\n空间复杂度o(n)\n> 接下来是两个单调栈难题\n# 84 柱状图中最大的矩形（Hard）\n\n给定 n 个非负整数，用来表示柱状图中各个柱子的高度。每个柱子彼此相邻，且宽度为 1 。\n\n求在该柱状图中，能够勾勒出来的矩形的最大面积。\n\n![](../img/coding/2024_3_29_3.png)\n\n## 单调栈\n\n> 假设当前元素nums[i]，我只要找到在他左边第一个比当前元素小，和右边第一个比当前元素小的，那么他们围成的矩形的高度就是nums[i]，长度就是右边-左边-1（因为实际上不能到比它小的位置），\n> \n> 这样问题就简化了，有了左边右边这些元素，只需要遍历一次，计算（右边-左边-1）*当前元素，找到最大值就是答案。\n> \n> 那么怎么样找到这些第一个比它小的元素呢，用**单调栈**\n> \n> 对于左边来说，维护一个单调递增的栈，那么每一个比栈顶进去大的元素，最左边第一个比它小的元素肯定是栈顶的。如果进入元素比栈顶小，那就一直出栈，直到进入元素比栈顶大，这个时候出栈的元素肯定都是大于当前元素的，\n> 也就是不是最左边比它小的元素，此时栈顶的才比它小，这就算出了最左边比它小的元素位置，但是如果此时栈空了，说明左边的全比自己大，那就返回-1.\n> \n> 右边的同理，只不过栈空对应的是nums.length\n```java\nclass Solution {\n    public int largestRectangleArea(int[] nums) {\n        Stack<Integer> stack = new Stack<>();\n        //保存最左边的比当前元素小的下标\n        int[] left = new int[nums.length];\n        //保存最右边的比当前元素小的下标\n        int[] right = new int[nums.length];\n        for (int i = 0; i < nums.length; i++) {\n            //单调栈的下一个肯定是比当前元素小的，也肯定是最左边的\n            while (!stack.isEmpty() && nums[stack.peek()] >= nums[i]){\n                stack.pop();\n            }\n            if (stack.isEmpty()){\n                left[i] = -1;\n            }\n            else {\n                left[i] = stack.peek();\n            }\n            stack.push(i);\n            //System.out.println(stack);\n        }\n        stack = new Stack<>();\n        for (int i = nums.length-1; i >= 0; i--) {\n            while (!stack.isEmpty() && nums[stack.peek()] >= nums[i]){\n                stack.pop();\n            }\n            if (stack.isEmpty()){\n                right[i] = nums.length;\n            }\n            else {\n                right[i] = stack.peek();\n            }\n            stack.push(i);\n        }\n        //System.out.println(Arrays.toString(left));\n        //System.out.println(Arrays.toString(right));\n        int max = Integer.MIN_VALUE;\n        for (int i = 0; i < nums.length; i++) {\n            //计算宽度\n            max = Math.max(max,(right[i]-left[i]-1)*nums[i]);\n        }\n        return max;\n    }\n}\n```\n> 很重要这个题，这种思想已经不是第一次见到了，分别按照左边右边两个数组，遍历三次\n\n时间复杂度：o(n)\n空间复杂度：o(n)\n\n# 接雨水（Hard）\n\n给定 n 个非负整数表示每个宽度为 1 的柱子的高度图，计算按此排列的柱子，下雨之后能接多少雨水。\n\n![](../img/coding/2024_3_29_4.png)\n\n## 动态规划\n>其实也用了上一题的思想\n\n对于下标 i，下雨后水能到达的最大高度等于下标 i 两边的最大高度的最小值，下标 i 处能接的雨水量等于下标 i 处的水能到达的最大高度减去 height[i]。\n\n朴素的做法是对于数组 height 中的每个元素，分别向左和向右扫描并记录左边和右边的最大高度，然后计算每个下标位置能接的雨水量。假设数组 height 的长度为 n，该做法需要对每个下标位置使用 O(n) 的时间向两边扫描并得到最大高度，因此总时间复杂度是 O(n^2)\n\n上述做法的时间复杂度较高是因为需要对每个下标位置都向两边扫描。如果已经知道每个位置两边的最大高度，则可以在 O(n)的时间内得到能接的雨水总量。使用动态规划的方法，可以在 O(n) 的时间内预处理得到每个位置两边的最大高度。\n\n创建两个长度为 n 的数组 leftMax 和 rightMax。对于 0≤i<n0 ，leftMax[i] 表示下标 i 及其左边的位置中，height 的最大高度，rightMax[i] 表示下标 i 及其右边的位置中，height 的最大高度。\n\n显然，$leftMax[0]=height[0]$，$rightMax[n−1]=height[n−1]$。两个数组的其余元素的计算如下：\n\n- 当 $1≤i≤n−1$ 时，$leftMax[i]=max(leftMax[i−1],height[i])$；\n- 当 $0≤i≤n−2$ 时，$rightMax[i]=max(rightMax[i+1],height[i])$。\n\n因此可以正向遍历数组 height 得到数组 leftMax 的每个元素值，反向遍历数组 height 得到数组 rightMax 的每个元素值。\n\n在得到数组 leftMax 和 rightMax 的每个元素值之后，对于 0≤i<n，下标 i 处能接的雨水量等于 $min(leftMax[i],rightMax[i])−height[i]$。遍历每个下标位置即可得到能接的雨水总量。\n\n![](../img/coding/2024_3_29_5.png)\n\n```java\nclass Solution {\n    public int trap(int[] height) {\n        int[] left = new int[height.length];\n        left[0] = height[0];\n        int[] right = new int[height.length];\n        right[height.length-1] = height[height.length-1];\n        for (int i = 1; i < height.length; i++) {\n            left[i] = Math.max(left[i-1],height[i]);\n        }\n        for (int i = height.length-2; i >=0 ; i--) {\n            right[i] = Math.max(right[i+1],height[i]);\n        }\n        int ans = 0;\n        for (int i = 0; i < height.length; i++) {\n            ans += Math.min(left[i],right[i])-height[i];\n        }\n        return ans;\n    }\n}\n```\n\n## 单调栈\n> 这个挺难理解的\n\n除了计算并存储每个位置两边的最大高度以外，也可以用单调栈计算能接的雨水总量。\n\n维护一个单调栈，单调栈存储的是下标，满足从栈底到栈顶的下标对应的数组 height 中的元素递减。\n\n从左到右遍历数组，遍历到下标 i 时，如果栈内至少有两个元素，记栈顶元素为 top，top 的下面一个元素是 left，则一定有 $height[left]≥height[top]$。如果 $height[i]>height[top]$，则得到一个可以接雨水的区域，该区域的宽度是 $i−left−1$，高度是 $min(height[left],height[i])−height[top]$，根据宽度和高度即可计算得到该区域能接的雨水量。\n\n为了得到 left，需要将 top 出栈。在对 top 计算能接的雨水量之后，left 变成新的 top，重复上述操作，直到栈变为空，或者栈顶下标对应的 height 中的元素大于或等于 height[i]。\n\n在对下标 i 处计算能接的雨水量之后，将 i 入栈，继续遍历后面的下标，计算能接的雨水量。遍历结束之后即可得到能接的雨水总量。\n\n```java\nclass Solution {\n    public int trap(int[] height) {\n        Stack<Integer> stack = new Stack<>();\n        int ans = 0;\n        for (int i = 0; i < height.length; i++) {\n            while (!stack.isEmpty() && height[i] > height[stack.peek()]) {\n                int top = stack.pop();\n                if (stack.isEmpty()) {\n                    break;\n                }\n                int left = stack.peek();\n                int currWidth = i - left - 1;\n                int currHeight = Math.min(height[left], height[i]) - height[top];\n                ans += currWidth * currHeight;\n            }\n\n            stack.push(i);\n        }\n        return ans;\n    }\n}\n```","tags":["刷题笔记"]},{"title":"leetcode-2024-3-28","url":"/2024/03/28/leetcode-2024-3-28/","content":"> 今天斗胆试了一下蓝桥杯的题，实在是感觉太难了，不暴力根本不会做，以后有机会再写题解吧。\n> \n> 我觉得这种程序设计竞赛还是得好好研究算法才行，我这种野路子还有很长一段路要走\n# 1997 访问完所有房间的第一天（Medium）\n你需要访问 n 个房间，房间从 0 到 n - 1 编号。同时，每一天都有一个日期编号，从 0 开始，依天数递增。你每天都会访问一个房间。\n\n最开始的第 0 天，你访问 0 号房间。给你一个长度为 n 且 下标从 0 开始 的数组 nextVisit 。在接下来的几天中，你访问房间的 次序 将根据下面的 规则 决定：\n\n- 假设某一天，你访问 i 号房间。\n- 如果算上本次访问，访问 i 号房间的次数为 奇数 ，那么 第二天 需要访问 nextVisit[i] 所指定的房间，其中 0 <= nextVisit[i] <= i 。\n- 如果算上本次访问，访问 i 号房间的次数为 偶数 ，那么 第二天 需要访问 (i + 1) mod n 号房间。\n\n请返回你访问完所有房间的第一天的日期编号。题目数据保证总是存在这样的一天。由于答案可能很大，返回对 109 + 7 取余后的结果\n## 暴力超时：简陋的动态规划\n> 虽然超时了但是思路跟答案是一样的。\n>\n> 分析题意发现 nextVisited[i]的范围属于 [0,i]，意味着当你首次到达房间 i 时会回退到房间 nextVisited[i]。而只有访问过该房间偶数次时才会到达下一个房间，进而推断出到达 i 时，[0,i)的房间已经被访问过偶数次。\n\n>定义 f[i] 表示从奇数次到房间 i，到奇数次到达房间 i+1 所需要的天数。以下用 to 代表 nextVisited[i]，回退到房间 to 时是奇数次访问，又需要花费 f[to] 才会到达房间 to+1。从 iii 访问 to 和 i+1 又分别需要花费一天，所以有转移方程:\n\n$$\nf[i] = \\sum\\limits_{j=to}^{i-1}f[j]+2\n$$\n![](../img/coding/2024_3_28_1.PNG)\n```java\nclass Solution {\n    public int firstDayBeenInAllRooms(int[] nextVisit) {\n        int[] dp = new int[nextVisit.length];\n        int MOD = 1000000007;\n        for (int i = 0; i < nextVisit.length; i++) {\n            //第一次进来然后去nextVisited[i]，第二次回来才变成偶数\n            int temp = 2;\n            //访问到nextVisited[i]，把之后所有的都会再访问一遍，全部都加起来\n            for (int j = nextVisit[i]; j < i; j++) {\n                temp = (dp[j]+temp)%MOD;\n            }\n            dp[i] = temp;\n        }\n        int ans = 0;\n        //dp存放的只是这个节点的次数，要总体的还要全部加起来\n        for (int i = 0; i < nextVisit.length-1; i++) {\n            ans=(dp[i]+ans)%MOD;\n        }\n        return ans;\n    }\n\n}\n```\n时间复杂度o(n^2)，那肯定暴力超时\n## 优化版：前缀和\n定义前缀和\n$$\ns[0]=0,s[i+1] = \\sum\\limits_{j=0}^{i}f[i]\n$$\n\n对于\n$$\nf[i] = \\sum\\limits_{j=to}^{i-1}f[j]+2\n$$\n可以化简为\n$$\nf[i] = 2+s[i]-s[j]\n$$\n对于前缀和 s，有如下递推式\n$$\ns[i+1] = 2*s[i]-s[j]+2\n$$\n这样我们就不用计算原先的dp数组，通过前缀和就得到答案。\n\n```java\nclass Solution {\n    public int firstDayBeenInAllRooms(int[] nextVisit) {\n        final long MOD = 1_000_000_007;\n        int n = nextVisit.length;\n        long[] s = new long[n];\n        for (int i = 0; i < n - 1; i++) {\n            int j = nextVisit[i];\n            s[i + 1] = (s[i] * 2 - s[j] + 2 + MOD) % MOD; // + MOD 避免算出负数\n        }\n        return (int) s[n - 1];\n    }\n}\n```\n\n# 221 最大正方形（Medium）\n\n在一个由 '0' 和 '1' 组成的二维矩阵内，找到只包含 '1' 的最大正方形，并返回其面积。\n![](../img/coding/2024_3_28_3.png)\n> 由于是在上课的时候看的，没有自己动手写，直接看的答案。我估计自己写也是智能暴力解法\n## 二维动态规划\n> 有点像“编辑距离”和某一天的那个截木块的每日一题，$dp[i][j]$表示以这个点为右下角的最大正方形的边长，那么如果$matrix[i][j]=0$，那dp肯定也为0，我们考虑为1的情况。\n> \n> 这里的状态转移方程是\n> $$\n> dp[i][j] = min(dp[i-1][j],dp[i][j-1],dp[i-1][j-1])+1\n> $$\n> ![](../img/coding/2024_3_28_2.png)\n\n```java\nclass Solution {\n    public int maximalSquare(char[][] matrix) {\n        int[][] dp = new int[matrix.length][matrix[0].length];\n        int ans = 0;\n        for (int i = 0; i < dp.length; i++) {\n            for (int j = 0; j < dp[0].length; j++) {\n                if (i != 0 || j !=0){\n                    if (matrix[i][j] == '1'){\n                        int left = 0,up = 0,up_left = 0;\n                        if (i-1 >= 0){\n                            up = dp[i-1][j];\n                        }\n                        if (j-1 >=0){\n                            left = dp[i][j-1];\n                        }\n                        if (i-1 >=0 && j-1>=0){\n                            up_left = dp[i-1][j-1];\n                        }\n                        dp[i][j] = Math.min(Math.min(left,up),up_left)+1;\n                        ans = Math.max(ans,dp[i][j]);\n                    }\n                }\n                else {\n                    if (matrix[i][j] == '1'){\n                        dp[i][j] = 1;\n                        ans = Math.max(ans,dp[i][j]);\n                    }\n                    else {\n                        dp[i][j] = 0;\n                    }\n                }\n\n            }\n        }\n        //System.out.println(Arrays.deepToString(dp));\n        return ans*ans;\n    }\n}\n```\n","tags":["刷题笔记"]},{"title":"leetcode-2024-3-27","url":"/2024/03/27/leetcode-2024-3-27/","content":"# 2580 统计将重叠区间合并成组的方案数（Medium）\n\n给你一个二维整数数组 ranges ，其中 ranges[i] = [starti, endi] 表示 starti 到 endi 之间（包括二者）的所有整数都包含在第 i 个区间中。\n\n你需要将 ranges 分成 两个 组（可以为空），满足：\n\n每个区间只属于一个组。\n两个有 交集 的区间必须在 同一个 组内。\n如果两个区间有至少 一个 公共整数，那么这两个区间是 有交集 的。\n\n比方说，区间 [1, 3] 和 [2, 5] 有交集，因为 2 和 3 在两个区间中都被包含。\n请你返回将 ranges 划分成两个组的 总方案数 。由于答案可能很大，将它对 109 + 7 取余 后返回。\n\n\n\n示例 1：\n\n输入：ranges = [[6,10],[5,15]]\n输出：2\n解释：\n两个区间有交集，所以它们必须在同一个组内。\n所以有两种方案：\n- 将两个区间都放在第 1 个组中。\n- 将两个区间都放在第 2 个组中。\n\n## 区间合并+组合数\n> 思路是将里面的区间先合并，得到长度。然后就相当于两个桶子，最终的结果为组合数相加，也就是一个杨辉三角，那么一行的总数就是2^n\n\n![](../img/coding/2024_3_27_1.png)\n\n> 区间合并：设置一个动态数组，首先按照左括号排序，接着从后往前遍历，如果两个区间存在重合，就把后面的删除，前面的改成合并以后的新区间。\n>\n> 这样主要是可以避免下标的影响，从前面遍历删除会乱序号\n> \n> 但是这样还不够，比如这个例子[[2,3],[4,5],[6,7],[8,9],[1,10]]，排序以后[[1,10],[2,3],[4,5],[6,7],[8,9]]，这样以来排序也没用，结果是[[1,10],[4,5],[6,7],[8,9]]，面对这种情况需要多循环几次，每次减少1个，当长度不再变化的时候就停止\n```java\nclass Solution {\n    public int countWays(int[][] ranges) {\n        int MOD = 1000000007;\n        int length = merge(ranges).length;\n        int ans = 1;\n        for (int i = 0; i < length; i++) {\n            ans = ans *2%MOD;\n        }\n        return ans;\n    }\n\n    /**\n     * 合并区间的代码\n     * @param intervals\n     * @return\n     */\n    public int[][] merge(int[][] intervals){\n        int[][] temp1 = mergeTemp(intervals);\n        int[][] temp2 = mergeTemp(temp1);\n        while (temp1.length != temp2.length){\n            temp1 = mergeTemp(temp2);\n            temp2 = mergeTemp(temp1);\n        }\n        return temp1;\n    }\n    public int[][] mergeTemp(int[][] intervals) {\n        //先排序\n        Arrays.sort(intervals, new Comparator<int[]>() {\n            @Override\n            public int compare(int[] o1, int[] o2) {\n                if(o1[0]==o2[0]){\n                    return o1[1] - o2[1];\n                }\n                return o1[0] - o2[0];\n            }\n        });\n        List<int[]> arrList = new ArrayList<>();\n        int rear = intervals.length-1;\n        for (int[] interval : intervals) {\n            arrList.add(interval);\n        }\n        while (rear > 0){\n            int[] front = arrList.get(rear-1);\n            int[] behind = arrList.get(rear);\n            int temp;\n            if (front[1] >= behind[0]){\n                temp = Math.max(front[1],behind[1]);\n                arrList.set(rear-1,new int[]{\n                        arrList.get(rear-1)[0],temp\n                });\n                arrList.remove(rear);\n                rear--;\n            }\n            else rear--;\n        }\n        int[][] ans = new int[arrList.size()][2];\n        for (int i = 0; i < arrList.size(); i++) {\n            ans[i] = arrList.get(i);\n        }\n        return ans;\n    }\n}\n```\n\n> 当初在做合并区间的时候就有点侥幸了，这个题后续还要多看\n\n# 25 K个一组翻转链表(Hard)\n\n给你链表的头节点 head ，每 k 个节点一组进行翻转，请你返回修改后的链表。\n\nk 是一个正整数，它的值小于或等于链表的长度。如果节点总数不是 k 的整数倍，那么请将最后剩余的节点保持原有顺序。\n\n你不能只是单纯的改变节点内部的值，而是需要实际进行节点交换\n\n![](../img/coding/2024_3_27_2.png)\n\n>其实也没那么Hard，就是麻烦了一点\n\n## 模拟\n```java\nclass Solution {\n    public ListNode reverseKGroup(ListNode head, int k) {\n        //如果间隔是1，那么就不进行下面的逻辑直接返回\n        if (k==1){\n            return head;\n        }\n        ListNode p = head;\n        //首先遍历统计节点数\n        int count = 0;\n        while (p!=null){\n            count++;\n            p = p.next;\n        }\n        p = head;\n        ListNode pre = head;\n        List<ListNode> listNodes = new ArrayList<>();\n        //对于每一个完整的组，不完整的直接接在尾部就行\n        for (int i = 1; i <= count / k; i++) {\n            //pre找的是下一个的完整组的头节点，也就是下个p\n            pre = getNext(pre,k);\n            //将反转后的头节点放进数组保存\n            listNodes.add(reverse(p,k));\n            p = pre;\n        }\n        //遍历数组，一段一段接起来\n        ListNode temp = listNodes.get(0);\n        for (int i = 0; i < listNodes.size()-1; i++) {\n            temp= listNodes.get(i);\n            while (temp.next != null){\n                temp = temp.next;\n            }\n            temp.next = listNodes.get(i+1);\n        }\n        //这里最后一段还没遍历，为了找到尾指针接上落单的那几个节点\n        while (temp.next != null){\n            temp = temp.next;\n        }\n        //接上落单的几个节点\n        temp.next = pre;\n        //返回最终的头节点\n        ListNode ans = listNodes.get(0);\n        \n        return listNodes.get(0);\n\n    }\n\n    /**\n     * 返回下一段的头节点\n     * @param p\n     * @param k\n     * @return\n     */\n    public ListNode getNext(ListNode p,int k){\n        for (int i = 0; i < k; i++) {\n            p = p.next;\n        }\n        return p;\n    }\n\n    /**\n     * 反转当前段\n     * @param head 原顺序的第一个节点\n     * @param k 本段长多少\n     * @return 返回这一段反转完的头节点\n     */\n    public ListNode reverse(ListNode head,int k){\n        //首先返回的头节点肯定是动过的，头节点是最末尾那个，在这里是k-1次循环后的那个节点\n        ListNode ans = head;\n        for (int i = 0; i < k - 1; i++) {\n            ans = ans.next;\n        }\n        //执行倒序逻辑\n        ListNode pre =null,p = head,pa;\n        for (int i = 0; i < k; i++) {\n            pa = p.next;\n            p.next = pre;\n            pre = p;\n            p = pa;\n        }\n        return ans;\n    }\n}\n```\n\n# 136 只出现一次的数字（Easy）\n\n给你一个 非空 整数数组 nums ，除了某个元素只出现一次以外，其余每个元素均出现两次。找出那个只出现了一次的元素。\n\n你必须设计并实现线性时间复杂度的算法来解决此问题，且该算法只使用常量额外空间。\n\n示例 1 ：\n\n输入：nums = [2,2,1]\n\n输出：1\n\n## 哈希表\n\n> 这里我的思路是遍历一次然后加入哈希表进行统计，再遍历一次哈希表找出其中为1的，这样时间空间复杂度肯定都0(n)了\n\n## 技巧-位运算\n\n> 通过异或，因为只有两个两个的，两个相同的异或结果就是0，0和任何异或都是自身，那么所有进行异或得到的就是落单的\n\n```java\nclass Solution {\n    public int singleNumber(int[] nums) {\n        int single = 0;\n        for (int num : nums) {\n            single ^= num;\n        }\n        return single;\n    }\n}\n\n``` \n\n","tags":["刷题笔记"]},{"title":"Collections集合篇-Map","url":"/2024/03/26/Collections集合篇-Map/","content":"# HashMap\n> 最常用的Map结构，使用的是拉链法\n\n## 数据结构\nHashMap采用Entry数组来存储key-value,Entry有四个属性，Key，value，哈希值和下一个的指针\n![](../img/Java/img_2.png)\n> 这里是1.8的版本，Node是Entry的一种实现\n```java\nstatic class Node<K,V> implements Map.Entry<K,V> {\n    final int hash;\n    final K key;\n    V value;\n    Node<K,V> next;\n\n    Node(int hash, K key, V value, Node<K,V> next) {\n        this.hash = hash;\n        this.key = key;\n        this.value = value;\n        this.next = next;\n    }\n}\n```\n表的数据结构就是一个个的Entry\n```java\ntransient Node<K,V>[] table;\n```\n## 属性\n```java\n/**\n * 初始的大小\n */\nstatic final int DEFAULT_INITIAL_CAPACITY = 1 << 4; // aka 16\n\n/**\n * 为什么这里是2^31呢，因为hashCode是int类型的值，\n * 对于数组下标而言不能有负数，整数的范围就是0-2^31-1\n */\nstatic final int MAXIMUM_CAPACITY = 1 << 30;\n\n/**\n * 阈值的计算参数，太多了可能会造成链表变长而降低查找效率\n * 太低了可能频繁扩容也会影响效率\n * 这个值是通过泊松分布计算出来的\n */\nstatic final float DEFAULT_LOAD_FACTOR = 0.75f;\n\n/**\n * 意味着如果链表的长度大于8就要转化为红黑树\n */\nstatic final int TREEIFY_THRESHOLD = 8;\n\n/**\n * 相反的，如果当前树节点小于6个就要将其转化为链表\n */\nstatic final int UNTREEIFY_THRESHOLD = 6;\n\n/**\n * 链表的长度大于8且数组长度大于64转化为红黑树\n */\nstatic final int MIN_TREEIFY_CAPACITY = 64;\n\n```\n## hashCode()和equals()方法\n首先看Object的hashCode方法：\n\n作用：返回对象的哈希码值，用于在哈希表等数据结构中快速定位对象，提高哈希表的性能。\n\n默认实现：Object类中的hashCode()方法默认返回对象的内存地址的哈希码值。\n\n> 这个默认的方法是根据地址计算出来的，\n> 如果两个对象的hashCode()返回值相同，不一定表示这两个对象相等(一般情况下是可以说明两个对象相等)，因为可能存在哈希冲突。\n> \n> **哈希值不能等价于地址，因为Java是在JVM中执行的，并不是真正的地址。**\n```java\npublic class Hashcode {\n    public static void main(String[] args) {\n        A a = new A();\n        A a1 = new A();\n        A a2 = a1;\n        System.out.println(a.hashCode());//460141958\n        System.out.println(a1.hashCode());//1163157884\n        System.out.println(a2.hashCode());//1163157884\n    }\n}\n \nclass A{\n}\n```\n\n我们再看HashMap重写的hashCode\n> 这里选择高十六位和第十六位进行异或运算，是为了尽量提取全部位的特征参与哈希计算，使得其更加分散\n```java\nstatic final int hash(Object key) {\n    int h;\n    return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);\n}\n```\n\n1. int h;：定义一个整型变量 h，用于存储计算出的哈希值。\n2. (key == null) ? 0 : (h = key.hashCode())：这是一个三元运算符，用于判断给定的键是否为 null。如果键为 null，则将哈希值 h 设为 0；否则，调用键的 hashCode() 方法获取其哈希码，并将结果赋给变量 h。\n3. (h >>> 16)：这是一个无符号右移运算，将变量 h 的二进制表示向右移动 16 位。这么做的目的是为了增加哈希值的随机性，使得哈希值的高位和低位都参与了哈希码的计算。\n4. (h = key.hashCode()) ^ (h >>> 16)：这是一个按位异或运算，将哈希值 h 和右移后的哈希值进行异或运算。按位异或运算是一种常用的混合哈希函数，用于将高位的信息与低位的信息混合在一起，增加哈希值的随机性。\n5. return：将计算出的哈希值返回。\n\n> 在put操作里面是这样判断hashcode和equals的\n```\np.hash == hash && ((k = p.key) == key || (key != null && key.equals(k)))\n```\n- 首先判断两个的哈希是否相等，如果哈希不相等那么肯定不存在冲突\n- 如果哈希相等，可能是哈希碰撞，也可能是真的就是一样的元素，需要对具体的k进行判断\n- p.key == key判断的是**引用是否相等**，也就是指针，有的时候两个类的指针指向的是同一块空间，那么这个时候就肯定是相等的\n- key.equals(k)判断的是**内容是否相等**，有的时候两个指针指得确实是不一样的，但是他们包含的内容是一样的，那么肯定也不能放进哈希表，new Student(\"yyf\")和new Student(\"xxy\")指针肯定是不同的，因为指向不同的内存空间，但是内容不一样。这个检测就需要equal来检测了\n\n> 看源码这一段判断，书里的长篇大论都可以不看了：\n> \n> hashcode一致，内容不一样的两个肯定是能进哈希表的，这就是哈希碰撞\n> \n> hashcode一致，equals返回true那肯定是不能进的，这里逻辑是或，就是内容或者引用满足一个相等就不能放了\n> \n> 重写了hashcode，但是equals返回true是可以放进去的，因为逻辑是与\n\n## put操作\n\n```java\npublic V put(K key, V value) {\n    return putVal(hash(key), key, value, false, true);\n}\n```\n```java\nstatic final int hash(Object key) {\n    int h;\n    return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);\n}\n```\n> 这里主要有几点注意：\n> - (n - 1) & hash是怎么来的，其实这就是一个取余的操作，因为n为2的整数倍，那么n-1肯定是末尾全为1前面全为0的，用位运算会比%快\n> - p.hash == hash && ((k = p.key) == key || (key != null && key.equals(k))在前面已经讲过了\n```java\nfinal V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) {\n    Node<K,V>[] tab; Node<K,V> p; int n, i;\n    //判断数组是否未初始化,这里table已经赋值给tab了，n也同理\n    if ((tab = table) == null || (n = tab.length) == 0)\n        //如果未初始化，调用resize方法 进行初始化\n        n = (tab = resize()).length;\n    //通过 & 运算求出该数据（key）的数组下标并判断该下标位置是否有数据\n    /**\n     * 这里是取余操作，位运算更快\n     * 由于n必然是2的倍数，那么-1就是除了最高位其他都为1，这个时候就相当于掩码，与hash进行与运算\n     * 就可以得到余数，非常神奇\n     */\n    if ((p = tab[i = (n - 1) & hash]) == null)\n        //如果没有，直接将数据放在该下标位置\n        tab[i] = newNode(hash, key, value, null);\n    //该数组下标有数据的情况\n    else {\n        Node<K,V> e; K k;\n        //判断该位置数据的key和新来的数据是否一样\n        if (p.hash == hash && ((k = p.key) == key || (key != null && key.equals(k))))\n            //如果一样，证明为修改操作，该节点的数据赋值给e,后边会用到\n            e = p;\n        //判断是不是红黑树\n        else if (p instanceof TreeNode)\n            //如果是红黑树的话，进行红黑树的操作\n            e = ((TreeNode<K,V>)p).putTreeVal(this, tab, hash, key, value);\n        //新数据和当前数组既不相同，也不是红黑树节点，证明是链表\n        else {\n            //遍历链表\n            for (int binCount = 0; ; ++binCount) {\n                //判断next节点，如果为空的话，证明遍历到链表尾部了\n                if ((e = p.next) == null) {\n                    //把新值放入链表尾部\n                    p.next = newNode(hash, key, value, null);\n                    //因为新插入了一条数据，所以判断链表长度是不是大于等于8\n                    if (binCount >= TREEIFY_THRESHOLD - 1) // -1 for 1st\n                        //如果是，进行转换红黑树操作\n                        treeifyBin(tab, hash);\n                    break;\n                }\n                //判断链表当中有数据相同的值，如果一样，证明为修改操作\n                if (e.hash == hash &&\n                    ((k = e.key) == key || (key != null && key.equals(k))))\n                    break;\n                //把下一个节点赋值为当前节点\n                p = e;\n            }\n        }\n        //判断e是否为空（e值为修改操作存放原数据的变量）\n        if (e != null) { // existing mapping for key\n            //不为空的话证明是修改操作，取出老值\n            V oldValue = e.value;\n            //一定会执行  onlyIfAbsent传进来的是false\n            if (!onlyIfAbsent || oldValue == null)\n                //将新值赋值当前节点\n                e.value = value;\n            afterNodeAccess(e);\n            //返回老值\n            return oldValue;\n        }\n    }\n    //计数器，计算当前节点的修改次数\n    ++modCount;\n    //当前数组中的数据数量如果大于扩容阈值\n    if (++size > threshold)\n        //进行扩容操作\n        resize();\n    //空方法\n    afterNodeInsertion(evict);\n    //添加操作时 返回空值\n    return null;\n}\n```\nput的逻辑是：\n- 首先判断是不是没有进行初始化，如果是第一次加入，那么resize，这里的默认长度是16，阈值为16*0.75\n- 如果初始化了，看tab[(n - 1) & hash]是否有数据，如果没有，就直接放这里；如果有那么就进行后面的逻辑\n- 判断p.hash == hash && ((k = p.key) == key || (key != null && key.equals(k)))看跟当前的这个是不是一样的，如果是一样的那就进行修改，如果不是，那就要进行链表或者红黑树的添加了\n- 如果是红黑树，那么就走红黑树的添加逻辑\n- 如果是链表，就遍历链表，每一个都进行判断p.hash == hash && ((k = p.key) == key || (key != null && key.equals(k)))，如果是那就修改，如果遍历到最后了，那就添加到末尾\n- 边走边判断是不是要超过8个了，如果遍历到的节点到第七个了，那就要树化了\n- 最后遍历完了看是否超过扩容阈值了，如果要扩容还要扩\n> 关于这里链表的尾插法，后续还有相关知识点\n## HashMap的扩容\n```java\n//扩容、初始化数组\nfinal Node<K,V>[] resize() {\n        Node<K,V>[] oldTab = table;\n    \t//如果当前数组为null的时候，把oldCap老数组容量设置为0\n        int oldCap = (oldTab == null) ? 0 : oldTab.length;\n        //老的扩容阈值\n    \tint oldThr = threshold;\n        int newCap, newThr = 0;\n        //判断数组容量是否大于0，大于0说明数组已经初始化\n    \tif (oldCap > 0) {\n            //判断当前数组长度是否大于最大数组长度\n            if (oldCap >= MAXIMUM_CAPACITY) {\n                //如果是，将扩容阈值直接设置为int类型的最大数值并直接返回\n                /**\n                 * 如果都已经到最大限制了不能再多了，那么阈值就要变得最大以免还要进行扩容操作\n                 */\n                threshold = Integer.MAX_VALUE;\n                return oldTab;\n            }\n            //如果在最大长度范围内，则需要扩容  OldCap << 1等价于oldCap*2\n            //运算过后判断是不是最大值并且oldCap需要大于16\n            else if ((newCap = oldCap << 1) < MAXIMUM_CAPACITY &&\n                     oldCap >= DEFAULT_INITIAL_CAPACITY)\n                newThr = oldThr << 1; // double threshold  等价于oldThr*2\n        }\n    \t//如果oldCap<0，但是已经初始化了，像把元素删除完之后的情况，那么它的临界值肯定还存在，       \t\t\t如果是首次初始化，它的临界值则为0\n        /**\n         * // 为什么这里的oldThr在未初始化数组的时候就有值呢？\n         // 这是因为HashMap有两个带参构造器，可以指定初始容量，\n         // 若你调用了这两个可以指定初始容量的构造器，\n         // 这两个构造器就会将阈值记录为第一个大于等于你指定容量，且满足2^n的数（可以看看这两个构造器）\n         */\n        else if (oldThr > 0) // initial capacity was placed in threshold\n            newCap = oldThr;\n        //数组未初始化的情况，将阈值和扩容因子都设置为默认值\n        //初始化走这个\n    \telse {               // zero initial threshold signifies using defaults\n            newCap = DEFAULT_INITIAL_CAPACITY;\n            newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY);\n        }\n    \t//初始化容量小于16的时候，扩容阈值是没有赋值的\n        if (newThr == 0) {\n            //创建阈值\n            float ft = (float)newCap * loadFactor;\n            //判断新容量和新阈值是否大于最大容量\n            newThr = (newCap < MAXIMUM_CAPACITY && ft < (float)MAXIMUM_CAPACITY ?\n                      (int)ft : Integer.MAX_VALUE);\n        }\n    \t//计算出来的阈值赋值\n        threshold = newThr;\n        @SuppressWarnings({\"rawtypes\",\"unchecked\"})\n        //根据上边计算得出的容量 创建新的数组       \n    \tNode<K,V>[] newTab = (Node<K,V>[])new Node[newCap];\n    \t//赋值\n    \ttable = newTab;\n    \t//扩容操作，判断不为空证明不是初始化数组\n        if (oldTab != null) {\n            //遍历数组\n            for (int j = 0; j < oldCap; ++j) {\n                Node<K,V> e;\n                //判断当前下标为j的数组如果不为空的话赋值个e，进行下一步操作\n                if ((e = oldTab[j]) != null) {\n                    //将数组位置置空\n                    oldTab[j] = null;\n                    //判断是否有下个节点\n                    if (e.next == null)\n                        //如果没有，就重新计算在新数组中的下标并放进去\n                        newTab[e.hash & (newCap - 1)] = e;\n                   \t//有下个节点的情况，并且判断是否已经树化\n                    else if (e instanceof TreeNode)\n                        //进行红黑树的操作\n                        ((TreeNode<K,V>)e).split(this, newTab, j, oldCap);\n                    //有下个节点的情况，并且没有树化（链表形式）\n                    else {\n                        //比如老数组容量是16，那下标就为0-15\n                        //扩容操作*2，容量就变为32，下标为0-31\n                        //低位：0-15，高位16-31\n                        //定义了四个变量\n                        //        低位头          低位尾\n                        Node<K,V> loHead = null, loTail = null;\n                        //        高位头\t\t   高位尾\n                        Node<K,V> hiHead = null, hiTail = null;\n                        //下个节点\n                        Node<K,V> next;\n                        //循环遍历\n                        do {\n                            //取出next节点\n                            next = e.next;\n                            //通过 与操作 计算得出结果为0\n                            if ((e.hash & oldCap) == 0) {\n                                //如果低位尾为null，证明当前数组位置为空，没有任何数据\n                                if (loTail == null)\n                                    //将e值放入低位头\n                                    loHead = e;\n                                //低位尾不为null，证明已经有数据了\n                                else\n                                    //将数据放入next节点\n                                    loTail.next = e;\n                                //记录低位尾数据\n                                loTail = e;\n                            }\n                            //通过 与操作 计算得出结果不为0\n                            else {\n                                 //如果高位尾为null，证明当前数组位置为空，没有任何数据\n                                if (hiTail == null)\n                                    //将e值放入高位头\n                                    hiHead = e;\n                                //高位尾不为null，证明已经有数据了\n                                else\n                                    //将数据放入next节点\n                                    hiTail.next = e;\n                               //记录高位尾数据\n                               \thiTail = e;\n                            }\n                            \n                        } \n                        //如果e不为空，证明没有到链表尾部，继续执行循环\n                        while ((e = next) != null);\n                        //低位尾如果记录的有数据，是链表\n                        if (loTail != null) {\n                            //将下一个元素置空\n                            loTail.next = null;\n                            //将低位头放入新数组的原下标位置\n                            newTab[j] = loHead;\n                        }\n                        //高位尾如果记录的有数据，是链表\n                        if (hiTail != null) {\n                            //将下一个元素置空\n                            hiTail.next = null;\n                            //将高位头放入新数组的(原下标+原数组容量)位置\n                            newTab[j + oldCap] = hiHead;\n                        }\n                    }\n                }\n            }\n        }\n    \t//返回新的数组对象\n        return newTab;\n    }\n```\n  resize()的逻辑是：\n- 首先判断oldCap是否大于0，如果是大于0那么说明已经初始化了，如果是等于0但是有阈值，说明可能是通过remove光了，也可能是初始化的时候使用有参构造方法，最后是等于0但是也没有阈值，对应的是无参构造方法，这个时候还没有真正给hashmap空间\n- 大于0，那么就是扩展hashmap长度为原来两倍（这里是左移一位，也就是*2），那么阈值也跟着扩展，如果等于0但是有阈值，那么新的阈值也是原来的，如果是无参的初始化，那么就是默认的16，阈值16*0，75\n- 完成了新数组长度和新阈值的计算，接着就是移动数组了，新建一个新长度的数组\n- 在旧的数组里面遍历，如果不为空，说明要移动了，如果后续没有，那么直接进新数组，如果还有后续，先判断是否是红黑树的TreeNode节点，是的话就走红黑树的逻辑，如果不是那就说明是链表了\n- 链表的添加有两种，首先新建两个链表，一个是需要原封不动放在新链表对应节点的，另一组是放在$newTab[j + oldCap] = hiHead$中\n- 如何判断移动呢？进行$e.hash & oldCap$的判断，这个计算的结果是oldcap最高位的值，我们放在table数组里面的哈希碰撞都是通过取余操作，这个计算的是oldcap的最高位后面的位数，最高位被忽视了，比如101010 & 001111和111010 & 001111结果都是1010，放在10的位置，对于这个链表来说最高位10和11的效果是一样的，因此为了缩短链表，把这一部分进行分开是很有必要的，这样就可以有效缩短链表。\n- 然后通过判断遍历这个链表，按照上面的条件分别进入两个待移动链表。\n- 最后移动链表，完成扩容\n### 引申：为什么HashMap的数组是2的次幂\n回答这个问题可以看两个判断条件\n1. (n - 1) & hash，n为数组长度\n2. e.hash & oldCap，oldCap为数组长度\n参考回答：\n- 对于取余操作：计算索引的效率更高，如果是2的n次幂可以用位运算代替取模运算\n- 对于计算新索引，e.hash & oldCap可以计算高位，判断是否转移到新表下。\n## Java1.7的HashMap死循环问题\njdk7的的数据结构是：数组+链表\n\n在数组进行扩容的时候，因为链表是头插法，在进行数据迁移的过程中，有可能导致死循环\n\n![image-20230428213115071](../img/Java/image-20230428213115071.png)\n\n- 变量e指向的是需要迁移的对象\n- 变量next指向的是下一个需要迁移的对象\n- Jdk1.7中的链表采用的头插法\n- 在数据迁移的过程中并没有新的对象产生，只是改变了对象的引用\n\n\n\n产生死循环的过程：\n\n线程1和线程2的变量e和next都引用了这个两个节点\n\n![image-20230428213533483](../img/Java/image-20230428213533483.png)\n\n线程2扩容后，由于头插法，链表顺序颠倒，但是线程1的临时变量e和next还引用了这两个节点\n\n![image-20230428214732877](../img/Java/image-20230428214732877.png)\n\n第一次循环\n\n由于线程2迁移的时候，已经把B的next执行了A\n\n![image-20230428214806072](../img/Java/image-20230428214806072.png)\n\n第二次循环\n\n![image-20230428214908652](../img/Java/image-20230428214908652.png)\n\n第三次循环\n\n![image-20230428214937231](../img/Java/image-20230428214937231.png)\n\n参考回答：\n\n在jdk1.7的hashmap中在数组进行扩容的时候，因为链表是头插法，在进行数据迁移的过程中，有可能导致死循环\n\n比如说，现在有两个线程\n\n线程一：读取到当前的hashmap数据，数据中一个链表，在准备扩容时，线程二介入\n\n线程二：也读取hashmap，直接进行扩容。因为是头插法，链表的顺序会进行颠倒过来。比如原来的顺序是AB，扩容后的顺序是BA，线程二执行结束。\n\n线程一：继续执行的时候就会出现死循环的问题。\n\n线程一先将A移入新的链表，再将B插入到链头，由于另外一个线程的原因，B的next指向了A，\n\n所以B->A->B,形成循环。\n\n当然，JDK 8 将扩容算法做了调整，不再将元素加入链表头（而是保持与扩容前一样的顺序），**尾插法**，就避免了jdk7中死循环的问题。\n\n\n# LinkedHashMap\nLinkedHashMap继承自HashMap，实现了Map接口，HashMap是不记录顺序的，有的时候我们需要记住插入的顺序，LinkedHashMap通过维护一个链表来记录顺序，可以想象成每插入一个到HashMap，链表也要插入一个保留顺序。\n```java\npublic class LinkedHashMap<K,V>\n    extends HashMap<K,V>\n    implements Map<K,V>\n```\n\n## 数据结构\n  在1.8维护了一个双向链表，定义头指针和尾指针，每一个Entry都会有前驱和后继\n  ![](../img/Java/img_4.png)\n> 这是新的 Entry的数据结构，继承自HashMap的Node，多了前驱和后继\n```java\n    /**\n     * LinkedHashMap中的node直接继承自HashMap中的Node。并且增加了双向的指针\n     */\n    static class Entry<K,V> extends HashMap.Node<K,V> {\n        Entry<K,V> before, after;\n        Entry(int hash, K key, V value, Node<K,V> next) {\n            super(hash, key, value, next);\n        }\n    }\n```\n> LinkedHashMap定义了头尾指针，accessOrder是一个重要变量，设置为真的话，get也会使元素移动到链表末尾，可以用这个变量实现**LRU**\n```java\n  /**\n     * 头指针，指向第一个node\n     */\n    transient LinkedHashMap.Entry<K,V> head;\n\n    /**\n     * 尾指针，指向最后一个node\n     */\n    transient LinkedHashMap.Entry<K,V> tail;\n\n    /**\n     * 一个条件变量，它控制了是否在get操作后需要将新的get的节点重新放到链表的尾部\n     * LinkedHashMap可以维持了插入的顺序，但是这个顺序不是不变的，可以被get操作打乱。\n     *\n     * @serial\n     */\n    final boolean accessOrder;\n\n```\n## 构造函数\n\n```java\n  /**\n     * Constructs an empty insertion-ordered <tt>LinkedHashMap</tt> instance\n     * with the specified initial capacity and load factor.\n     *\n     * @param  initialCapacity the initial capacity\n     * @param  loadFactor      the load factor\n     * @throws IllegalArgumentException if the initial capacity is negative\n     *         or the load factor is nonpositive\n     */\n    public LinkedHashMap(int initialCapacity, float loadFactor) {\n        super(initialCapacity, loadFactor);\n        accessOrder = false;\n    }\n\n    /**\n     * 构造一个空的，按插入序（accessOrder = false）的LinkedHashMap，使用默认初始大小和负载因子0.75\n     */\n    public LinkedHashMap(int initialCapacity) {\n        super(initialCapacity);\n        accessOrder = false;\n    }\n\n    /**\n     * 默认的无参构造函数继承自HashMap的无参构造，那么也是刚开始懒加载\n     * 直到第一个元素进来才会有分配空间resize一次\n     * 默认也是accessOrder为假，也就是说不能get改变顺序\n     */\n    public LinkedHashMap() {\n        super();\n        accessOrder = false;\n    }\n    \n    public LinkedHashMap(Map<? extends K, ? extends V> m) {\n        super();\n        accessOrder = false;\n        putMapEntries(m, false);\n    }\n\n    /**\n     * 这个构造方法指定了accessOrder\n     * 注意细节，要配置accessOrder只能用容量和负载因子的有参构造方法\n     */\n    public LinkedHashMap(int initialCapacity,\n                         float loadFactor,\n                         boolean accessOrder) {\n        super(initialCapacity, loadFactor);\n        this.accessOrder = accessOrder;\n    }\n\n```\n## Java1.7和1.8LinkedHashMap的区别\n- 1.7版本的是用的双向循环链表，相比于HashMap多了两个变量，一个是头节点，因为是双向循环链表，head既可以是头也可以是尾。还有个顺序的变量，true是顺序，false是逆序，对于链表来说就是头插法和尾插法的区别，头插法就是逆序，尾插法就是顺序\n- 1.8用的是双向链表，接下来重点介绍\n## 1.8维护链表的操作\n> 对于数组的插入和扩容继承自HashMap，不在赘述。主要研究双向链表如何维护。\n### afterNodeRemoval\n顾名思义，在节点移除之后要做的事情\n```java\n    //在节点删除后，维护链表，传入删除的节点\n    void afterNodeRemoval(Node<K,V> e) { // unlink\n        //p指向待删除元素，b执行前驱，a执行后驱\n        LinkedHashMap.Entry<K,V> p =\n            (LinkedHashMap.Entry<K,V>)e, b = p.before, a = p.after;\n        //这里执行双向链表删除p节点操作，很简单。\n        p.before = p.after = null;\n        //如果没有前面的节点，那这个就是第一个，这个时候全局的head指针要给当前节点的下一个作为头节点\n        if (b == null)\n            head = a;\n        //如果不是第一个节点，那就前面节点的后继为当前节点的后继\n        else\n            b.after = a;\n        //如果没有后继的节点，那么前面的节点就要接替成为尾节点\n        if (a == null)\n            tail = b;\n        //同理\n        else\n            a.before = b;\n    }\n\n```\n### afterNodeAccess\n根据accessOrder判断是否要在get之后进行调整\n```java\n  //在节点被访问后根据accessOrder判断是否需要调整链表顺序\n//将节点放在最后面\n    void afterNodeAccess(Node<K,V> e) { // move node to last\n        LinkedHashMap.Entry<K,V> last;\n        //如果accessOrder为false或者只有一个节点了，什么都不做\n        if (accessOrder && (last = tail) != e) {\n            //p指向待删除元素，b执行前驱，a执行后驱\n            LinkedHashMap.Entry<K,V> p =\n                (LinkedHashMap.Entry<K,V>)e, b = p.before, a = p.after;\n            //这里执行双向链表删除操作\n            p.after = null;\n            if (b == null)\n                head = a;\n            else\n                b.after = a;\n            if (a != null)\n                a.before = b;\n            else\n                last = b;\n            //这里执行将p放到尾部\n            if (last == null)\n                head = p;\n            else {\n                p.before = last;\n                last.after = p;\n            }\n            tail = p;\n            //保证并发读安全。\n            ++modCount;\n        }\n    }\n```\n### afterNodeInsertion\n这个方法是在添加之后的操作，可以想象成LRU，当容量超过阈值的时候可以重写removeEldestEntry方法返回true，就会删除最老的一个元素\n```java\nvoid afterNodeInsertion(boolean evict) { // possibly remove eldest\n    LinkedHashMap.Entry<K,V> first;\n    //removeEldestEntry(first)默认返回false，所以afterNodeInsertion这个方法其实并不会执行\n    if (evict && (first = head) != null && removeEldestEntry(first)) {\n        K key = first.key;\n        removeNode(hash(key), key, null, false, true);\n    }\n}\n\nprotected boolean removeEldestEntry(Map.Entry<K,V> eldest) {\n    return false;\n}\n\n```\n\n## get操作\n\n```java\n/**\n * 调用hashmap的getNode方法获取到值之后，维护链表\n * @param key\n * @return\n */\npublic V get(Object key) {\n    Node<K,V> e;\n    if ((e = getNode(hash(key), key)) == null)\n        return null;\n    //根据这个accessOrder来看是否要更新队伍\n    if (accessOrder)\n        afterNodeAccess(e);\n    return e.value;\n}\n\n```\n\n## put操作","tags":["Java"]},{"title":"Collections集合篇-List","url":"/2024/03/26/Collections集合篇-List/","content":"> 今天开始学习Java基础八股文，先从我用的最多的一个数据结构开始看起。虽然我学Java接触了这个概念快两三年了，但是不看源码还是不知道ArrayList的具体实现。\n\n# Collections\n\n集合分为两大类，Collection是单列集合，包含常用的Set，List，Queue等，其中Set里面使用HashSet比较多，List里面使用Arraylist比较多，Queue中有一个优先队列PriorityQueue的概念。\n\n- Set：HashSet，LinkedHashSet，SortedSet和继承其的TreeSet\n- List：ArrayList，LinkedList，Vector\n- Queue：PriorityQueue\n\n还有一类是双列集合Map，使用的比较多的有HashMap，LinkedHashMap，TreeMap，HashTable\n\n![](../img/Java/img.png)\n\n## List\n\n首先我们先复习一下顺序存储和链式存储的区别以及时间复杂度和空间复杂度。\n\n### 顺序存储：\n\n支持随机查找，空间连续，数据密度大（不像链表那样有额外的指针空间），删除和插入麻烦，不适合频繁在其中删除插入\n\n- 插入：如果在表尾就不需要移动元素为o(1)，如果是在内部，需要移动的期望为 (1+2+3+...+n)/(n+1) = n(n+1)/2(n+1) = n/2，时间复杂度为o(n)。\n- 删除：如果在表尾删除也不用移动，如果在内部，期望为 (1+2+...+n-1)/n = n(n-1)/2n = (n-1)/2 也是o(n)\n- 查找：平均查找期望 (1+2+..+n)/n = (n+1)/2 也为o(n)\n\n### 链式存储：\n\n不支持随机查找，但是相对于顺序的插入和删除效率还是高那么一点，空间不连续，数据的密度小，因为要存指针。\n\n- 插入：插入这个操作本身是o(1)的只需要改指针，但是要找到这个插入的位置是需要o(n)，删除同理。\n- 查找：不支持随机查找，每次找都需要从头开始找(双向链表可以解决这个问题)，也是o(n)\n\n### ArrayList\n\n> ArrayList和Vector是基于数组实现的，但是是动态的，每次添加之前都要判断是否下一个就要超过了，如果溢出就要重新开辟一个更长的数组。\n\n#### 成员变量\n\n```java\n/**\n * 默认的初始容量为10\n */\nprivate static final int DEFAULT_CAPACITY = 10;\n/**\n * 如果初始化为new ArrayList(n)，但是还没有在这个里面加东西的时候，elementData就是这个\n */\nprivate static final Object[] EMPTY_ELEMENTDATA = {};\n/**\n * 如果初始化为new ArrayList()，但是还没有元素添加的时候，就是这个Default，为了和上面的区分开\n */\nprivate static final Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = {};\n/**\n * 实际存储的数组结构\n */\ntransient Object[] elementData;\n\nprivate int size;\n```\n\n#### 构造方法\n>当initialCapacity给了是0或者没有提供的时候，不进行实例化，等到有元素进来了在进行扩容\n```java\n/**\n * 有参数的：只要有这个参数就是EMPTY_ELEMENTDATA，跟下面这个区分开\n * 可以按照指定容量初始化\n */\npublic ArrayList(int initialCapacity) {\n    if (initialCapacity > 0) {\n        this.elementData = new Object[initialCapacity];\n    } else if (initialCapacity == 0) {\n        this.elementData = EMPTY_ELEMENTDATA;\n    } else {\n        throw new IllegalArgumentException(\"Illegal Capacity: \"+\n                                           initialCapacity);\n    }\n}\n\n/**\n * 无参数构造方法：DEFAULTCAPACITY_EMPTY_ELEMENTDATA来区分\n */\npublic ArrayList() {\n    this.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA;\n}\n\n\n```\n\n#### add添加方法流程\n\n```java\npublic boolean add(E e) {\n    //每一次加之前先检查size+1不会大于数组最大值\n    ensureCapacityInternal(size + 1);  // Increments modCount!!\n    elementData[size++] = e;\n    return true;\n}\n```\n\n```java\nprivate void ensureCapacityInternal(int minCapacity) {\n    //calculateCapacity(elementData, minCapacity)如果是无参的返回就是10\n    //如果不是返回的就是minCapacity\n    ensureExplicitCapacity(calculateCapacity(elementData, minCapacity));\n}\n```\n\n\n```java\nprivate static int calculateCapacity(Object[] elementData, int minCapacity) {\n    //如果是无参的初始化，那么就在当前这个容量和10之间选一个最大的\n    //但是一般来说第一个元素size肯定是0吧，这里传来的minCapacity估计是1\n    //所以第一次应该是初始化10个，对于无参构造方法而言\n    if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) {\n        return Math.max(DEFAULT_CAPACITY, minCapacity);\n    }\n    return minCapacity;\n}\n```\n\n\n```java\nprivate void ensureExplicitCapacity(int minCapacity) {\n    //操作次数，一个内部变量\n    modCount++;\n\n    // overflow-conscious code\n    //如果有增大的需求，即现在的需求已经比现长度大了\n    if (minCapacity - elementData.length > 0)\n        grow(minCapacity);\n}\n```\n\n```java\nprivate void grow(int minCapacity) {\n    // overflow-conscious code\n    int oldCapacity = elementData.length;\n    //1.5倍，oldCapacity右移一位代表/2，1+0.5\n    int newCapacity = oldCapacity + (oldCapacity >> 1);\n    //只有第一次会这样minCapacity给的是10，这个肯定是<0\n    if (newCapacity - minCapacity < 0)\n        newCapacity = minCapacity;\n    if (newCapacity - MAX_ARRAY_SIZE > 0)\n        newCapacity = hugeCapacity(minCapacity);\n    // minCapacity is usually close to size, so this is a win:\n    //然后拷贝一份这个数组\n    elementData = Arrays.copyOf(elementData, newCapacity);\n}\n```\n\n#### ArrayList的线程不安全问题\n\n> ArrayList不是线程安全的\n> \n> - 非同步操作： ArrayList 的方法并没有进行同步处理，因此在多线程环境下，多个线程可以同时访问和修改 ArrayList 的状态。\n> - 不保证操作的原子性： ArrayList 的操作（例如添加、删除、修改元素等）并不是原子操作，它们可能会分解成多个步骤。在多线程环境下，如果一个线程在执行操作的过程中被另一个线程中断，可能会导致数据不一致的情况发生\n> - 迭代器不支持并发修改： 在使用迭代器遍历 ArrayList 的过程中，如果其他线程对 ArrayList 进行了结构性修改（如添加或删除元素），则会抛出 ConcurrentModificationException 异常\n\n>举个例子，这里会报错，CopyOnWriteArrayList就不会\n```java\npublic static void main(String[] args) {\n    ArrayListTest arrayListTest = new ArrayListTest();\n    VectorTest vectorTest = new VectorTest();\n    for (int i = 0; i < 10; i++) {\n        new Thread(() -> {\n            for (int j = 0; j < 1000; j++) {\n                arrayListTest.insert(j);\n            }\n            System.out.println(arrayListTest.getSize());\n        }).start();\n    }\n\n    // 等待所有线程执行完毕\n    try {\n        Thread.sleep(1000);\n    } catch (InterruptedException e) {\n        e.printStackTrace();\n    }\n}\n```\n有几种解决方法\n\n- CopyOnWriteArrayList：使用 java.util.concurrent 包中提供的线程安全的集合类，例如 CopyOnWriteArrayList，它通过在写操作时复制整个数组来实现线程安全，适用于读多写少的场景。\n\n```java\nList<String> threadSafeList = new CopyOnWriteArrayList<>();\n\n```\n- 使用同步机制： 使用 Collections 工具类提供的 synchronizedList 方法，将 ArrayList 包装成一个同步的 List，这样可以保证在多线程环境下对 ArrayList 的操作是线程安全的，但性能可能会受到影响。\n```java\nList<String> synchronizedList = Collections.synchronizedList(new ArrayList<>());\n\n```\n\n### 几个面试题\n\n#### new ArrayList(10)grow了几次\n\n答：0次，因为这个在有参构造函数里面已经有了\n\n#### new ArrayList(0)和new ArrayList()在第一次扩容后都是多少\n\nnew ArrayList()和new ArrayList(0)执行完之后elementData都是空数组，但是这两个空数组的内存地址是不一样的。if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) 这一段代码在new(0)的时候是不会走的，因为比的是地址，所以返回值是1，最后结果也是1\n所以new ArrayList(0)第一次扩容是1，new ArrayList()第一次扩容是10\n\n#### 数组和list之间的转换？\n\n>数组->list:Arrays.asList()\n> \n>list->数组: list.toArray(n)\n\n- 数组转List ，使用JDK中java.util.Arrays工具类的asList方法\n\n- List转数组，使用List的toArray方法。无参toArray方法返回 Object数组，传入初始化长度的数组对象，返回该对象数组\n\n#### 用Arrays.asList转List后，如果修改了数组内容，list受影响吗\n\nArrays.asList转换list之后，如果修改了数组的内容，list会受影响，\n因为它的底层使用的Arrays类中的一个内部类ArrayList来构造的集合，\n在这个集合的构造器中，把我们传入的这个集合进行了包装而已，最终指向的都是同一个内存地址\n\n#### List用toArray转数组后，如果修改了List内容，数组受影响吗\n\nlist用了toArray转数组后，如果修改了list内容，数组不会影响，当调用了toArray以后，在底层是它是进行了数组的拷贝，跟原来的元素就没啥关系了，所以即使list修改了以后，数组也不受影响\n\n## LinkedList\n\n特点：\n- 基于双向链表实现\n- 也符合链式存储的一系列优缺点\n- 线程不安全，因为是链表，如何安全呢，跟上面的ArrayList一样使用Collections.synchronizedList(new ArrayList<>());\n\n## Vector\n\n特点：\n- 也是基于顺序存储（数组结构），但是增长的策略与ArrayList不同，而且每次增长2倍\n- 线程安全，这使其性能略逊于ArrayList\n- Stack是基于Vector的\n\n## 对比\n\n\n| 名称         | 基于数据结构 | 线程是否安全 |\n|------------|--------|--------|\n| ArrayList  | 数组     | 否      |\n| Vector     | 数组     | 是      |\n| LinkedList | 双向链表   | 否      |\n","tags":["Java"]},{"title":"leetcode-2024-3-26","url":"/2024/03/26/leetcode-2024-3-26/","content":"# 2642 设计可以求最短路径的图类（Hard）\n>题目太长了就换成图片了\n\n![](../img/coding/2024_3_26_1.png)\n\n## 迪杰斯特拉\n> 我自己的做法是Dijkstra+邻接矩阵，还可以有Floyd，考虑到这个邻接矩阵都已经内存99%了，所以应该矩阵+Floyd是最佳方案\n\n> 就当是复习Dijkstra了\n```java\nclass Graph {\n\n    private int[][] matrix;\n    private int[] finalArr;\n    private int[] visited;\n    //用邻接矩阵表示图\n    public Graph(int n, int[][] edges) {\n        matrix = new int[n][n];\n        for (int i = 0; i < n; i++) {\n            for (int j = 0; j < n; j++) {\n                if (i != j){\n                    matrix[i][j] = Integer.MAX_VALUE;\n                }\n            }\n        }\n        for (int i = 0; i < edges.length; i++) {\n            matrix[edges[i][0]][edges[i][1]] = edges[i][2];\n        }\n    }\n    //增加一条边就是再改一个值\n    public void addEdge(int[] edge) {\n        matrix[edge[0]][edge[1]] = edge[2];\n    }\n\n    /**\n     * Dijkstra，n次找到n个最短路径，所以需要一个visited矩阵保存已经找到最短路径的点\n     * finalArr存放最短路径，初始化为初始点的邻接数组，每次找到其中最小的\n     * 更新距离\n     * @param node1\n     * @param node2\n     * @return\n     */\n    public int shortestPath(int node1, int node2) {\n        finalArr = matrix[node1].clone();\n        visited = new int[matrix.length];\n        //System.out.println(Arrays.toString(finalArr));\n        int k = node1;\n        for (int i = 0; i < matrix.length-1; i++) {\n            visited[k] = 1;\n            int[] adj = getMin(finalArr);\n            k = adj[0];\n            int price = adj[1];\n            //如果这里返回-1说明找不到最小值，最小值都已经是正无穷了\n            //说明此时其他的都已经最优，直接跳出循环\n            if (k ==-1){\n                break;\n            }\n            for (int j = 0; j < matrix.length; j++) {\n                if (matrix[k][j] != Integer.MAX_VALUE&&price != Integer.MAX_VALUE && visited[j] == 0 && matrix[k][j]+price < finalArr[j]){\n                    finalArr[j] = matrix[k][j]+price;\n                }\n            }\n            //System.out.println(Arrays.toString(finalArr));\n        }\n        if (finalArr[node2] == Integer.MAX_VALUE){\n            return -1;\n        }else {\n            return finalArr[node2];\n        }\n    }\n    public int[] getMin(int[] arr){\n        int min = Integer.MAX_VALUE;\n        int vexNum = -1;\n        for (int i = 0; i < arr.length; i++) {\n            if (visited[i] ==0 && arr[i] != 0){\n                if (arr[i] <min){\n                    min = arr[i];\n                    vexNum = i;\n                }\n            }\n        }\n        return new int[]{vexNum,min};\n    }\n}\n\n/**\n * Your Graph object will be instantiated and called as such:\n * Graph obj = new\n * Graph(n, edges);\n * obj.addEdge(edge);\n * int param_2 = obj.shortestPath(node1,node2);\n */\n```\n\n# 矩阵置零（Medium）\n![](../img/coding/2024_3_26_2.png)\n## 广度优先搜索\n>其实本来是想多源广度优先搜索的，后来发现好像不用这复杂。直接暴力每一行每一列都变成0就可以了\n```java\nclass Solution {\n    public void setZeroes(int[][] matrix) {\n        //存储在里面找到的0\n        List<int[]> points = new ArrayList<>();\n        for (int i = 0; i < matrix.length; i++) {\n            for (int j = 0; j < matrix[0].length; j++) {\n                if (matrix[i][j] == 0){\n                    points.add(new int[]{i,j});\n                }\n            }\n        }\n        //遍历每一个0，使其横竖都变为0\n        for(int[] point:points){\n            for (int i = 0; i < matrix.length; i++) {\n                matrix[i][point[1]] = 0;\n            }\n            for (int j = 0; j < matrix[0].length; j++) {\n                matrix[point[0]][j] = 0;\n            }\n        }\n    }\n}\n```\nrow为行，col为列\n\n时间复杂度：o(row*col)\n\n空间复杂度：o(row+col)\n\n# 130 被围绕的区域（Medium）\n![](../img/coding/2024_3_26_3.png)\n## 深度优先搜索\n> 主要思路是，首先找到一个区域先把其全部变为X，用一个数组保存，如果这个区域里面包含在边界的块，那么这个数组用于后续再把他变为O\n```java\nclass Solution {\n    List<int[]> points = new ArrayList<>();\n    List<List<int[]>> list = new ArrayList<>();\n    public void solve(char[][] board) {\n        for (int i = 0; i < board.length; i++) {\n            for (int j = 0; j < board[0].length; j++) {\n                if (board[i][j] == 'O'){\n                    points = new ArrayList<>();\n                    //如果有链接到边界,随后把这些改回来\n                    if (!dfs(board,i,j)){\n                        list.add(points);\n                    }\n                }\n            }\n        }\n        for (List<int[]> points:list){\n            for (int[] point:points){\n                board[point[0]][point[1]] = 'O';\n            }\n        }\n        //System.out.println(Arrays.deepToString(board));\n    }\n    //一个区域块，首先全部变为X，返回真在后续把他变回O\n    public boolean dfs(char[][] board,int i,int j){\n        if (board[i][j] == 'O'){\n            boolean temp = true;\n            points.add(new int[]{i,j});\n            board[i][j] = 'X';\n            if (i == 0||j==0||i==board.length-1||j==board[0].length-1){\n                temp = false;\n            }\n            if (i+1<=board.length-1){\n                temp &= dfs(board,i+1,j);\n            }\n            if (i-1 >=0){\n                temp &= dfs(board,i-1,j);\n            }\n            if (j+1 <= board[0].length-1){\n                temp &= dfs(board,i,j+1);\n            }\n            if (j-1 >= 0){\n                temp &= dfs(board,i,j-1);\n            }\n            return temp;\n        }\n        else {\n            return true;\n        }\n    }\n}\n```\n时间复杂度：o(m*n)\n空间复杂度：o(m*n)\n\n## 还有两题太简单了我就当复习了\n### 排序链表\n### 快速排序\n```java\nclass Solution {\n    public int[] sortArray(int[] nums) {\n        quickSort(nums,0,nums.length-1);\n    \n        return nums;\n    }\n    public void quickSort(int[] nums,int head,int rear){\n        if (head < rear){\n            int mid = partition(nums,head,rear);\n            quickSort(nums,head,mid-1);\n            quickSort(nums,mid+1,rear);\n        }\n    }\n    public int partition(int[] arr,int head,int rear){\n        int temp = arr[head];\n        while(head < rear){\n            //主要是要记得这里是每个都要带=，\n            while (head < rear && arr[rear] >= temp){\n                rear--;\n            }\n            arr[head] = arr[rear];\n            while (head < rear && arr[head] <= temp){\n                head++;\n            }\n            arr[rear] = arr[head];\n        }\n        arr[head] = temp;\n        //System.out.println(Arrays.toString(arr));\n        return head;\n    }\n}\n```","tags":["刷题笔记"]},{"title":"leetcode-2024-3-25","url":"/2024/03/25/leetcode-2024-3-25/","content":"# 518 零钱兑换2（Medium）\n给你一个整数数组 coins 表示不同面额的硬币，另给一个整数 amount 表示总金额。\n\n请你计算并返回可以凑成总金额的硬币组合数。如果任何硬币组合都无法凑出总金额，返回 0 。\n\n假设每一种面额的硬币有无限个。\n\n题目数据保证结果符合 32 位带符号整数。\n\n\n\n示例 1：\n\n输入：amount = 5, coins = [1, 2, 5]\n\n输出：4\n\n解释：有四种方式可以凑成总金额：\n\n5=5\n\n5=2+2+1\n\n5=2+1+1+1\n\n5=1+1+1+1+1\n\n## 很愚蠢的暴力解：DFS（8/28）\n> 最搞笑的是我点了一个运行，然后吃完饭都没有算出结果来，我愿称之为最暴力的一集\n\n> 本意是深度优先搜索，然后将路径上的值保存进哈希表，哈希表记录的是硬币和个数，如果最后能全部找完，就把当前哈希表存到set里面（其实让我意想不到的是这样子set也可以去重）\n\n> 这种方法要重复计算太多次了，可以优化成右边的有向无环图，但是就这个题而言完全没必要这么复杂。\n\n![](../img/coding/2024_3_25_1.png)\n```java\n//hashmap用来放具体找零\nprivate HashMap<Integer,Integer> hashMap = new HashMap<>();\n//set去重\nprivate Set<HashMap<Integer,Integer>> set = new HashSet<>();\npublic int change1(int amount, int[] coins) {\n    Arrays.sort(coins);\n    dfs(amount,coins);\n    return set.size();\n}\npublic void dfs(int amount,int[] coins){\n    //可以找完，放进set里面\n    if (amount == 0){\n        set.add(new HashMap<>(hashMap));\n        return;\n    }\n    //不能用已有的找零了，失败\n    if (amount < coins[0]){\n        return;\n    }\n    //递归遍历每一种情况\n    for (int i = 0; i < coins.length; i++) {\n        //先加入hashmap\n        if (!hashMap.containsKey(coins[i])){\n            hashMap.put(coins[i],1);\n        }\n        else {\n            Integer integer = hashMap.get(coins[i]);\n            hashMap.replace(coins[i],integer+1);\n        }\n\n        dfs(amount-coins[i],coins);\n        //后续递归再删除回到上一层，保证每个循环都是一样的\n        int temp = hashMap.get(coins[i]);\n        if (temp == 1){\n            hashMap.remove(coins[i]);\n        }\n        else {\n            hashMap.put(coins[i],temp-1);\n        }\n    }\n}\n```\n\n## 动态规划（其实就是跳房子游戏）\n\n> 问题一：我对这道题的主要纠结点在于，比如11，硬币有1，2，5，理应找dp[10],dp[9],dp[6],但是我们不知道这里面是不是可能造成重复，事实上dp[9]里面肯定也会只使用一个5，对于dp[6]的情况来说只是**改了顺序**\n\n> 如何解决？我们不再遍历11而遍历硬币，这样就能保证在遍历到当前硬币之前不会有这个路径出现。每一次遍历都是如何通过当前的这个硬币换到现在的钱，\n> 自然而然得到转移方程，dp[j] += dp[j-coins[i]];\n```java\nclass Solution {\n    public int change(int amount, int[] coins) {\n        int[] dp = new int[amount+1];\n        dp[0] = 1;\n        for (int i = 0; i < coins.length; i++) {\n            for (int j = coins[i]; j < dp.length; j++) {\n                dp[j] += dp[j-coins[i]];\n            }\n            \n        }\n\n        return dp[dp.length-1];\n    }\n}\n```\n>看完后其实是有恍然大雾的感觉，多看多学。\n\n# 合并K个有序链表（Hard）\n\n给你一个链表数组，每个链表都已经按升序排列。\n\n请你将所有链表合并到一个升序链表中，返回合并后的链表。\n\n>实在不明白这个题怎么会是hard，给我刷战绩的题\n\n>其实还不够快，如果用分治法可以logn\n\n## 队列\n> 两两合并然后入队，直到队里面只有一个，就是答案\n```java\nclass Solution {\n    public ListNode mergeKLists(ListNode[] lists) {\n        //判断是否为空\n        if (lists.length == 0){\n            return null;\n        }\n        Queue<ListNode> queue = new ArrayDeque<>();\n        for (int i = 0; i < lists.length; i++) {\n            if (lists[i] != null){\n                queue.add(lists[i]);\n            }\n        }\n        //这里也被卡了一下，如果长度是0就不能进行下面的步骤\n        if (queue.isEmpty()){\n            return null;\n        }\n        while (queue.size() != 1){\n            ListNode listNode1 = queue.poll();\n            ListNode listNode2 = queue.poll();\n            if (listNode1 != null && listNode2 != null){\n                queue.add(merge(listNode1,listNode2));\n            }\n        }\n        return queue.poll();\n    }\n\n    /**\n     * 两个链表合并，基本方法\n     * @param listNode1\n     * @param listNode2\n     * @return\n     */\n    public ListNode merge(ListNode listNode1,ListNode listNode2){\n        //用了一个头节点方便保存\n        ListNode temp = new ListNode();\n        ListNode ans = temp;\n        while (listNode1!=null && listNode2!=null){\n            if (listNode1.val <= listNode2.val){\n                temp.next = listNode1;\n                listNode1 = listNode1.next;\n            }\n            else {\n                temp.next = listNode2;\n                listNode2 = listNode2.next;\n            }\n            temp = temp.next;\n        }\n        //接上没遍历到的\n        if (listNode1 != null){\n            temp.next = listNode1;\n        }\n        if (listNode2 != null){\n            temp.next = listNode2;\n        }\n        return ans.next;\n    }\n\n}\n```\n\n# 搜索二维矩阵2（Medium）\n\n编写一个高效的算法来搜索 m x n 矩阵 matrix 中的一个目标值 target 。该矩阵具有以下特性：\n\n每行的元素从左到右升序排列。\n\n每列的元素从上到下升序排列。\n\n![](../img/coding/2024_3_25_2.png)\n\n## 二维双指针\n>我们可以观察到，从下往上数每一行第一个元素，如果比目标大就可以删除了，同理每一列从后往前数第一个元素，如果大也可以删除了。\n> 这样就得到了一个新矩阵。\n\n>接着找新矩阵最后一行的元素，从前往后数如果比目标小也可以删除了，然后最后一列，从上到下比目标小的也可以删除。\n \n> 这样完成一次循环，减少了很大的搜索范围。一直收敛到只有一行或者一列。\n\n>这里用了***小技巧***，对于找不到的元素肯定是会**数组下标抛异常**的，catch到了肯定是没有的，返回假\n\n> 还有有一个小问题，当我们面对[[5,6,9],[9,10,11],[11,14,18]]这样的数组，有两个9，这时候我们就永远满足不了循环条件出不来了，为了解决需要看循环里面四个指针是否变化，如果没变就说明发生了这种情况。直接跳出循环。而出现这种情况的原因就是因为有两个一样的目标值，那肯定返回真。\n```java\nclass Solution {\n    public boolean searchMatrix(int[][] matrix, int target) {\n        int row_top = 0,row_bottom = matrix.length-1,col_left = 0,col_right = matrix[0].length-1;\n        boolean flag = false;\n\n        try {\n            while (row_top < row_bottom && col_left < col_right){\n                //判断是否指针有变化\n                int temp = 0;\n                while (matrix[row_bottom][col_left] > target){\n                    temp++;\n                    row_bottom--;\n                }\n                while (matrix[row_top][col_right] > target){\n                    temp++;\n                    col_right--;\n                }\n                while (matrix[row_bottom][col_left] < target){\n                    temp++;\n                    col_left++;\n                }\n                while (matrix[row_top][col_right] <target){\n                    temp++;\n                    row_top++;\n                }\n                if (temp == 0){\n                    flag = true;\n                    break;\n                }\n\n            }\n            //System.out.println(row_top+\"  \"+row_bottom+\"  \"+col_left+\"  \"+col_right);\n            if (row_top == row_bottom){\n                for (int i = col_left; i <=col_right; i++) {\n                    if (matrix[row_bottom][i] == target){\n                        flag = true;\n                        break;\n                    }\n                }\n            }\n            if (col_left == col_right){\n                for (int i = row_top; i <= row_bottom; i++) {\n                    if (matrix[i][col_right] == target){\n                        flag = true;\n                        break;\n                    }\n                }\n            }\n            return flag;\n        }\n        //小技巧\n        catch (ArrayIndexOutOfBoundsException e){\n            return false;\n        }\n    }\n}\n```\n\n这道题做出来其实有点侥幸，很多情况都是错了以后试验出来的。据他们所说这样还不如遍历整体找结果。\n\n# 54 螺旋矩阵（Medium）\n\n给你一个 m 行 n 列的矩阵 matrix ，请按照 顺时针螺旋顺序 ，返回矩阵中的所有元素。\n\n![](../img/coding/2024_3_25_3.png)\n\n> 非常好理解的一道题，根据上一题的思路很快就能写出，但是这里不用频繁维护四个指针，只用在循环末尾使用就可以。\n\n> 就跟洋葱一样，每次都剥掉最外一层\n\n## 双指针\n\n```java\nclass Solution {\n    public List<Integer> spiralOrder(int[][] matrix) {\n        List<Integer> ans = new ArrayList<>();\n        int row_top = 0,row_bottom = matrix.length-1,col_left = 0,col_right = matrix[0].length-1;\n        while (row_top < row_bottom && col_left < col_right){\n            for (int i = col_left; i < col_right; i++) {\n                ans.add(matrix[row_top][i]);\n            }\n            for (int i = row_top; i < row_bottom; i++) {\n                ans.add(matrix[i][col_right]);\n            }\n            for (int i = col_right; i > col_left ; i--) {\n                ans.add(matrix[row_bottom][i]);\n            }\n            for (int i = row_bottom; i > row_top; i--) {\n                ans.add(matrix[i][col_left]);\n            }\n            //换到内层\n            row_top++;\n            row_bottom--;\n            col_left++;\n            col_right--;\n        }\n        //特判。如果都相等，说明中间只有一个元素\n        if (row_top == row_bottom && col_left == col_right){\n            ans.add(matrix[row_top][col_right]);\n        }\n        else {\n            //一行\n            if (row_top == row_bottom){\n                for (int i = col_left; i <= col_right; i++) {\n                    ans.add(matrix[row_top][i]);\n                }\n            }\n            //一列\n            if (col_left == col_right){\n                for (int i = row_top; i <= row_bottom; i++) {\n                    ans.add(matrix[i][col_left]);\n                }\n            }\n        }\n\n        return ans;\n    }\n}\n```\n\n# 59 螺旋矩阵2（Medium）\n给你一个正整数 n ，生成一个包含 1 到 n2 所有元素，且元素按顺时针顺序螺旋排列的 n x n 正方形矩阵 matrix 。\n\n![](../img/coding/2024_3_25_4.png)\n> 这个比上面的更简单了，一个计数器，然后根据上面的逻辑转圈，每次都+1，就可以了，甚至没有中间是一行或者一列的情况，要不就是没有要不就只有一个。\n```java\nclass Solution {\n    public int[][] generateMatrix(int n) {\n        int[][] matrix = new int[n][n];\n        int count = 1;\n        int row_top = 0,row_bottom = matrix.length-1,col_left = 0,col_right = matrix[0].length-1;\n        while (row_top < row_bottom && col_left < col_right){\n            for (int i = col_left; i < col_right; i++) {\n                matrix[row_top][i] = count++;\n            }\n            for (int i = row_top; i < row_bottom; i++) {\n                matrix[i][col_right]= count++;\n            }\n            for (int i = col_right; i > col_left ; i--) {\n                matrix[row_bottom][i]= count++;\n            }\n            for (int i = row_bottom; i > row_top; i--) {\n                matrix[i][col_left]= count++;\n            }\n            row_top++;\n            row_bottom--;\n            col_left++;\n            col_right--;\n        }\n        if (row_top == row_bottom && col_left == col_right){\n            matrix[row_top][col_right]= count++;\n        }\n        else {\n            if (row_top == row_bottom){\n                for (int i = col_left; i <= col_right; i++) {\n                    matrix[row_top][i]= count++;\n                }\n            }\n            if (col_left == col_right){\n                for (int i = row_top; i <= row_bottom; i++) {\n                    matrix[i][col_left]= count++;\n                }\n            }\n        }\n        return matrix;\n    }\n}\n```","tags":["刷题笔记"]},{"title":"周赛2024-3-24","url":"/2024/03/24/周赛2024-3-24/","content":">陆陆续续这是第三次周赛，第一次是虚拟的我瞎写ac了两题，上一次也是ac两题因为起得太晚了。这次稍微一点点进步，ac两个半，第三题有思路但是暴力超时，是因为我没见过这种数据结构。\n\n> Hard就跳了吧\n\n# 第一题：每个字符最多出现两次的最长子字符串\n给你一个字符串 s ，请找出满足每个字符最多出现两次的最长子字符串，并返回该 子字符串 的 最大 长度。\n\n示例 1：\n\n输入： s = \"bcbbbcba\"\n\n输出： 4\n\n解释：\n\n以下子字符串长度为 4，并且每个字符最多出现两次：\"bcbbbcba\"。\n>遍历每个字串，判断是否字符只出现两次\n```java\nclass Solution {\n    public int maximumLengthSubstring(String s) {\n        int max = 2;\n        for (int k = s.length(); k >=1; k--) {\n            for (int i = 0; i < s.length() - k; i++) {\n                String sub = s.substring(i,i+k+1);\n                //如果没有返回false\n                if (judge(sub)){\n                    max = Math.max(max,sub.length());\n                }\n            }\n        }\n        return max;\n    }\n    //判断字串是否只出现两次，用哈希表实现，如果哈希表超过2就返回false\n    public boolean judge(String s){\n        HashMap<Character,Integer> map = new HashMap<>();\n        boolean flag = true;\n        for (int i = 0; i < s.length(); i++) {\n            if (!map.containsKey(s.charAt(i))){\n                map.put(s.charAt(i),1);\n            }\n            else {\n                Integer integer = map.get(s.charAt(i));\n                if (integer >= 2){\n                    flag = false;\n                }\n                map.replace(s.charAt(i),integer+1);\n            }\n        }\n        return flag;\n    }\n\n}\n```\n# 第二题：执行操作使数据元素之和大于等于 K\n给你一个正整数 k 。最初，你有一个数组 nums = [1] 。\n\n你可以对数组执行以下 任意 操作 任意 次数（可能为零）：\n\n选择数组中的任何一个元素，然后将它的值 增加 1 。\n\n复制数组中的任何一个元素，然后将它附加到数组的末尾。\n\n返回使得最终数组元素之 和 大于或等于 k 所需的 最少 操作次数。\n\n\n\n示例 1：\n\n输入：k = 11\n\n输出：5\n\n解释：\n\n可以对数组 nums = [1] 执行以下操作：\n\n将元素的值增加 1 三次。结果数组为 nums = [4] 。\n复制元素两次。结果数组为 nums = [4,4,4] 。\n最终数组的和为 4 + 4 + 4 = 12 ，大于等于 k = 11 。\n执行的总操作次数为 3 + 2 = 5 。\n>数学题，假设+1的次数为x，复制的次数为y，要使得(y+1)*(x+1)>=n，而满足x+y最小\n\n>满足x+y+2最小其实也是满足x+y最小，那么就是开根号了，由基本不等式可得。\n \n>还有个问题就是这里的是整数，假如n=29开根号是5，这个时候用6 * 6=36就浪费了一次，只需要用5 * 6=30就可以，需要进行特殊判断\n\n>wa了一次，因为这里如果正好开根号，就直接返回x+y了，不需要-2\n```java\nclass Solution {\n    public int minOperations(int k) {\n        if (k ==1){\n            return 0;\n        }\n        int sqrt = (int) Math.sqrt(k);\n        if (k == sqrt*sqrt){\n            return sqrt*2-2;\n        }\n        else {\n            if (k <= sqrt *(sqrt+1)){\n                return sqrt+sqrt+1-2;\n            }\n            else {\n                return sqrt+1+sqrt+1-2;\n            }\n        }\n\n    }\n}\n```\n# 第三题：最高频率的 ID\n>高级的排序哈希集（TreeMap）不得不品\n> \n你需要在一个集合里动态记录 ID 的出现频率。给你两个长度都为 n 的整数数组 nums 和 freq ，nums 中每一个元素表示一个 ID ，对应的 freq 中的元素表示这个 ID 在集合中此次操作后需要增加或者减少的数目。\n\n增加 ID 的数目：如果 freq[i] 是正数，那么 freq[i] 个 ID 为 nums[i] 的元素在第 i 步操作后会添加到集合中。\n减少 ID 的数目：如果 freq[i] 是负数，那么 -freq[i] 个 ID 为 nums[i] 的元素在第 i 步操作后会从集合中删除。\n请你返回一个长度为 n 的数组 ans ，其中 ans[i] 表示第 i 步操作后出现频率最高的 ID 数目 ，如果在某次操作后集合为空，那么 ans[i] 为 0 。\n\n\n\n示例 1：\n\n输入：nums = [2,3,2,1], freq = [3,2,-3,1]\n\n输出：[3,3,2,2]\n\n解释：\n\n第 0 步操作后，有 3 个 ID 为 2 的元素，所以 ans[0] = 3 。\n第 1 步操作后，有 3 个 ID 为 2 的元素和 2 个 ID 为 3 的元素，所以 ans[1] = 3 。\n第 2 步操作后，有 2 个 ID 为 3 的元素，所以 ans[2] = 2 。\n第 3 步操作后，有 2 个 ID 为 3 的元素和 1 个 ID 为 1 的元素，所以 ans[3] = 2 。\n## 暴力超时\n>hashmap维护当前的频率，每次都对其进行最小值查找，时间复杂度o(n^2)\n```java\nclass Solution {\n    public long[] mostFrequentIDs(int[] nums, int[] freq) {\n        long[] ans = new long[nums.length];\n        HashMap<Integer,Long> hashMap = new HashMap<>();\n        for (int i = 0; i < nums.length; i++) {\n            if (!hashMap.containsKey(nums[i])){\n                hashMap.put(nums[i], (long) freq[i]);\n            }\n            else {\n                long integer = hashMap.get(nums[i]);\n                hashMap.replace(nums[i],integer+freq[i]);\n            }\n            ans[i] = getMost(hashMap);\n        }\n        return ans;\n    }\n    //遍历哈希找最小值\n    public long getMost(HashMap<Integer,Long> hashMap){\n        long max = Long.MIN_VALUE;\n        for (Map.Entry<Integer,Long> entry : hashMap.entrySet()){\n            max = Math.max(max,entry.getValue());\n        }\n        return max;\n    }\n}\n```\n## TreeMap（重要）\n\n>TreeMap是索引为键的有序哈希表，从小到大排序，每次维护treemap只用从最后面找到就是最大值。\n\n>这里treemap存的是频率的频率，每当有更新的时候就对应频率的值-1，如果为0了就删除，然后在加上新的\n```java\nclass Solution {\n    public long[] mostFrequentIDs(int[] nums, int[] freq) {\n        long[] ans = new long[nums.length];\n        HashMap<Integer,Long> hashMap = new HashMap<>();\n        TreeMap<Long,Integer> treeMap = new TreeMap<>();\n        for (int i = 0; i < nums.length; i++) {\n            if (!hashMap.containsKey(nums[i])){\n                hashMap.put(nums[i], (long) freq[i]);\n                //如果没有就默认为1，有就加上\n                treeMap.put((long) freq[i],treeMap.getOrDefault((long)freq[i],0)+1);\n            }\n            else {\n                long integer = hashMap.get(nums[i]);\n                if (treeMap.containsKey(integer)){\n                    int temp = treeMap.get(integer);\n                    if (temp == 1){\n                        //如果本来就只有1了，就移除\n                        treeMap.remove(integer);\n                    }\n                    else {\n                        //这里其实可以直接用put\n                        treeMap.replace(integer,temp-1);\n                    }\n                    treeMap.put(integer+freq[i],treeMap.getOrDefault(integer+freq[i],0)+1);\n                }\n                hashMap.replace(nums[i],integer+freq[i]);\n            }\n            ans[i] = treeMap.lastKey();\n        }\n        return ans;\n    }\n}\n```\n## [引申：前天的每日一题](./leetcode-2024-3-21.md)\n有一点相似\n","tags":["刷题笔记"]},{"title":"leetcode-2024-3-24","url":"/2024/03/24/leetcode-2024-3-24/","content":"# 323 零钱兑换(Medium)\n给你一个整数数组 coins ，表示不同面额的硬币；以及一个整数 amount ，表示总金额。\n\n计算并返回可以凑成总金额所需的 最少的硬币个数 。如果没有任何一种硬币组合能组成总金额，返回 -1 。\n\n你可以认为每种硬币的数量是无限的。\n\n\n\n示例 1：\n\n输入：coins = [1, 2, 5], amount = 11\n\n输出：3\n\n解释：11 = 5 + 5 + 1\n>今天的每日一题在十天前做过，过一遍就不重新做了\n## 动态规划\n>dp数组存当前下标的钱可以用的最少兑换次数，如果用现在的零钱兑现不了，就为-1\n\n>dp[k] = min{dp[i]+dp[k-1-i]} 当且仅当dp[i]和dp[k-1-i]都不为-1\n> 但是如果这个本身就可以用零钱找开，就为1\n```java\nclass Solution {\n    public int coinChange(int[] coins, int amount) {\n        if (amount == 0){\n            return 0;\n        }\n        int[] dp = new int[amount+1];\n        for (int i = 0; i < coins.length; i++) {\n            if (coins[i] <= amount){\n                dp[coins[i]] = 1;\n            }\n        }\n        //System.out.println(Arrays.toString(dp));\n        for (int i = 1; i <= amount; i++) {\n            if (i != 1){\n                int head = 1;\n                int rear = i-1;\n                int min = Integer.MAX_VALUE;\n                //dp[k] = min{dp[i]+dp[k-1-i]} 当且仅当dp[i]和dp[k-1-i]都不为-1\n                while (head <= rear){\n                    if (dp[head] != -1 && dp[rear]!=-1 && dp[head] + dp[rear] < min){\n                        min = dp[head]+dp[rear];\n                    }\n                    //System.out.println(i+\":\"+dp[head] +\"  \"+ dp[rear]);\n                    head++;\n                    rear--;\n                }\n                //最小值没有改变，说明不能找的开\n                if (dp[i] == 0 && min == Integer.MAX_VALUE){\n                    dp[i] = -1;\n                }\n                //改变了就最小值\n                if (dp[i] == 0 && min != Integer.MAX_VALUE){\n                    dp[i] = min;\n                }\n                //如果这个本身就是1的话就不考虑，还是1\n            }\n            else {\n                if (dp[i] == 0){\n                    dp[i] = -1;\n                }\n            }\n        }\n        //System.out.println(Arrays.toString(dp));\n    \n        return dp[amount];\n    }\n}\n```\n\n# 238 除自身以外数组的乘积(Medium)\n给你一个整数数组 **nums**，返回 数组 **answer** ，其中 answer[i] 等于 nums 中除 nums[i] 之外其余各元素的乘积 。\n\n题目数据 **保证** 数组 nums之中任意元素的全部前缀元素和后缀的乘积都在  32 位 整数范围内。\n\n请 **不要使用除法**，且在 **O(n)** 时间复杂度内完成此题。\n\n## 偷懒方法（就是用了除法）\n>最简单方法，所有乘积起来，当前元素只要除掉这个就可以，遍历一次就可以。0的时候要特殊判断一下，这个时候还是傻方法，遍历其他的。\n\n```java\nclass Solution {\n    public int[] productExceptSelf(int[] nums) {\n        int[] ans = new int[nums.length];\n        if (nums.length == 1){\n            return nums;\n        }\n        else if (nums.length > 1){\n            int sum = nums[0];\n            //乘起来\n            for (int i = 1; i < nums.length; i++) {\n                sum *=nums[i];\n            }\n            \n            for (int i = 0; i < nums.length; i++) {\n                if (nums[i] != 0){\n                    ans[i] = sum /nums[i];\n                }\n                //如果是0特判\n                else {\n                    int front = i-1;\n                    int next = i+1;\n                    int anss = 1;\n                    while (front >=0){\n                        anss*=nums[front];\n                        front--;\n                    }\n                    while (next <=nums.length-1){\n                        anss*=nums[next];\n                        next++;\n                    }\n                    ans[i] = anss;\n                }\n            }\n        }\n        return ans;\n    }\n}\n```\n## 官方解：左右乘积列表\n>维护两个数组，L[i]是当前元素i左边的乘积，R[i]是i右边的乘积，返回的数组就是L[i]*R[i]，每次都是o（n）\n\n```java\nclass Solution {\n    public int[] productExceptSelf(int[] nums) {\n        int[] L = new int[nums.length];\n        int[] R = new int[nums.length];\n        int[] ans = new int[nums.length];\n        L[0] = 1;\n        R[nums.length-1] = 1;\n        for (int i = 1; i < nums.length; i++) {\n            L[i] = L[i-1]*nums[i-1];\n        }\n        for (int j = nums.length-2; j >=0 ; j--) {\n            R[j] = R[j+1]*nums[j+1];\n        }\n        for (int i = 0; i < nums.length; i++) {\n            ans[i] = L[i] * R[i];\n        }\n        return ans;\n    }\n}\n```\n# 11 盛最多水的容器(Medium)\n给定一个长度为 n 的整数数组 height 。有 n 条垂线，第 i 条线的两个端点是 (i, 0) 和 (i, height[i]) 。\n\n找出其中的两条线，使得它们与 x 轴共同构成的容器可以容纳最多的水。\n\n返回容器可以储存的最大水量。\n![](../img/coding/2024_3_24_1.png)\n## 暴力超时\n>遍历找最小\n```java\nclass Solution {\n    public int maxArea(int[] height) {\n        int max = Integer.MIN_VALUE;\n        for (int i = 0; i < height.length-1; i++) {\n            for (int j = i+1; j < height.length; j++) {\n                max = Math.max(max,(j-i)*Math.min(height[j],height[i]));\n            }\n        }\n        return max;\n    }\n}\n```\n## 官方解：双指针\n>感觉证明有待考察，初始左右指针一个在左边一个在右边，每次移动比较小的就可以找到。\n\n![](../img/coding/2024_3_24_2.png)\n![](../img/coding/2024_3_24_3.png)\n>简单理解为什么移动小的，因为本来就是取较小的，移动大的话铁定没之前大，因为距离缩短了1，移动小的还可能碰到大的使原来的变大。\n```java\nclass Solution {\n    public int maxArea(int[] height) {\n        int head = 0;\n        int rear = height.length-1;\n        int max = Integer.MIN_VALUE;\n        while (head < rear){\n            int length = rear-head;\n            max = Math.max(max,length*Math.min(height[head],height[rear]));\n            if (height[head]<height[rear]){\n                head++;\n            }\n            else if (height[head]>height[rear]){\n                rear--;\n            }\n            else {\n                head++;\n            }\n        }\n        return max;\n    }\n}\n```","tags":["刷题笔记"]},{"title":"短期的小目标吧","url":"/2024/03/23/短期的小目标吧/","content":"这几天晚上睡不好，总在想自己保研了但是感觉也不是很开心。同专业的同学们陆陆续续出成绩，一看考本校的寥寥无几，顿时觉得自己保研好像也没什么了不起的。\n\n总不可能一直麻痹自己，说我这两个月去做了什么事情都没干的实习，说去给学校打工赚了一些小钱。如何将这些东西变现才是最重要的，要不然我会一直不平衡。\n\n我打算先把黑马头条那个项目狠狠重新搞一下，南方电网的也好好包装一下。\n\n说实话，我对Redis和MongoDB和Kafka都挺感兴趣的，每天保持刷题的同时我也想多看看这方面的八股文。\n\n暑假可以先去试试水，没有就好好安心搞12306那个微服务项目。\n\n![](../img/IMG_8928.JPG)","tags":["闲谈"]},{"title":"leetcode-2024-3-23","url":"/2024/03/23/leetcode-2024-3-23/","content":"# 2549 统计桌面上的不同数字(Easy)\n给你一个正整数 n ，开始时，它放在桌面上。在 10^9 天内，每天都要执行下述步骤：\n\n对于出现在桌面上的每个数字 x ，找出符合 1 <= i <= n 且满足 x % i == 1 的所有数字 i 。\n然后，将这些数字放在桌面上。\n返回在 10^9 天之后，出现在桌面上的 不同 整数的数目。\n\n>简单题重拳出击\n\n## 正常解\n\n>发现可以进行递归，假设n=7，那么第一次递归就是6和3，因为7%3=1，7%6=1，那么怎么样解决重复的问题呢，用一个set就好了。\n\n```java\n//去重\nHashSet<Integer> hashSet = new HashSet<>();\npublic int distinctIntegers(int n) {\n    dp(n);\n    //最后返回唯一集合的长度就行\n    return hashSet.size();\n}\npublic void dp(int n){\n    hashSet.add(n);\n    for (int i = n-1; i >=1; i--) {\n        //遍历进行递归\n        if (n % i ==1){\n            distinctIntegers(i);\n        }\n    }\n}\n```\n\n那是比不上最快点方法的，其实找规律直接出来就是n-1，我这个方法还算是有点编程的，自然速度快不过那些n-1的\n\n# 146 LRU缓存（Medium）非常重要这道题\n\n实现LRU算法，**关键是要时间复杂度和空间都是o（1）**\n\n>虽然从前在os上还是手搓过这个算法的，但是这里要用常数的复杂度。\n\n>那必然是要哈希，但是哈希还不够，在搜寻在队列里面的时候必然还是要遍历，这个时候就要想到另一种数据结构，链表\n> 这里为什么用双向链表，因为前驱还是要找的，总不能每次都去遍历吧，那还是浪费一点空间吧。\n## 哈希表+双向链表\n```java\nclass LRUCache{\n    //双向链表数据结构\n    class LinkNode{\n        int key;\n        int value;\n        //前驱\n        LinkNode front = null;\n        //后继\n        LinkNode behind = null;\n        LinkNode(int key,int value){\n            this.key = key;\n            this.value = value;\n        }\n        LinkNode(){}\n    }\n    //容量\n    private int capacity;\n    //维护首尾指针\n    private LinkNode head,tail;\n    //记录链表长度\n    private int linkSize = 0;\n    //哈希表来根据键找到链表的具体位置\n    private HashMap<Integer,LinkNode> hashMap = new HashMap<>();\n    public LRUCache(int capacity) {\n        this.capacity = capacity;\n        head = new LinkNode();\n        tail = new LinkNode();\n    }\n    \n    public int get(int key) {\n        if (hashMap.containsKey(key)){\n            LinkNode linkNode = hashMap.get(key);\n            moveToHead(linkNode);\n            return linkNode.value;\n        }\n        else {\n            return -1;\n        }\n    }\n    \n    public void put(int key, int value) {\n        //如果没到容量，就不涉及释放位置\n        if (linkSize < capacity){\n            //如果map没有\n            if (!hashMap.containsKey(key)){\n                LinkNode linkNode = new LinkNode(key,value);\n                //头插法\n                handleInsert(linkNode);\n                linkSize++;\n                //保存这个节点\n                hashMap.put(key,linkNode);\n            }\n            else {\n                LinkNode linkNode = hashMap.get(key);\n                linkNode.value = value;\n                moveToHead(linkNode);\n                hashMap.replace(key,linkNode);\n            }\n        }\n        else {\n            if (!hashMap.containsKey(key)){\n                int removeKey = handleRemove();\n                hashMap.remove(removeKey);\n                LinkNode linkNode = new LinkNode(key,value);\n                handleInsert(linkNode);\n                hashMap.put(key,linkNode);\n            }\n            else {\n                LinkNode linkNode = hashMap.get(key);\n                linkNode.value = value;\n                moveToHead(linkNode);\n                hashMap.replace(key,linkNode);\n            }\n        }\n    }\n\n    /**\n     * 头插法\n     * @param linkNode 插入的节点\n     */\n    public void handleInsert(LinkNode linkNode){\n        LinkNode temp = head.behind;\n        //如果这是第一个节点，尾指针一直挂在这个元素上，直到被提起来rear再变\n        if (temp != null){\n            linkNode.behind = temp;\n            head.behind = linkNode;\n            temp.front = linkNode;\n            linkNode.front = head;\n        }\n        //如果不是第一个，那就头插法\n        else {\n            head.behind = linkNode;\n            linkNode.front = head;\n            tail = linkNode;\n        }\n    }\n\n    /**\n     * 出队，移动尾指针即可\n     * @return 返回尾元素的键\n     */\n    public int handleRemove(){\n        int ans = tail.key;\n        tail = tail.front;\n        tail.behind = null;\n        return ans;\n    }\n\n    /**\n     * 更新在队首\n     * @param linkNode\n     */\n    public void moveToHead(LinkNode linkNode){\n        //也就是在末尾的时候\n        if (linkNode.behind == null){\n            tail = linkNode.front;\n            linkNode.front.behind = null;\n            linkNode.front = null;\n            handleInsert(linkNode);\n\n        }\n        else {\n            LinkNode left = linkNode.front;\n            LinkNode right = linkNode.behind;\n            left.behind = right;\n            right.front = left;\n            linkNode.front = null;\n            linkNode.behind = null;\n            handleInsert(linkNode);\n        }\n    }\n}\n\n```\n\n大同小异这些答案，主要还是哈希+双向\n\n# 283 移动零(Easy)\n给定一个数组 nums，编写一个函数将所有 0 移动到数组的末尾，同时保持非零元素的相对顺序。\n\n请注意 ，必须在不复制数组的情况下原地对数组进行操作。\n\n示例 1:\n\n输入: nums = [0,1,0,3,12]\n\n输出: [1,3,12,0,0]\n\n>没什么好说的这个题\n\n## 直接插入排序思想\n\n```java\nclass Solution {\n    public void moveZeroes(int[] nums) {\n        for (int i = nums.length-2; i >=0; i--) {\n            if (nums[i] == 0){\n                int j=i+1;\n                while (j <= nums.length-1&&nums[j] != 0  ){\n                    nums[j-1] = nums[j];\n                    j++;\n                }\n                nums[j-1] = 0;\n            }\n        }\n    }\n}\n```\n\n# 236.二叉树的最近公共祖先(Medium)\n题目意思就是字面意思\n>这个题居然还没有考研那段时间做得好，这里思路就是找到这两个节点的路径序列，然后找到最近的那个重复元素就完事。\n## 2024.3.23\n```java\nclass Solution {\n    List<TreeNode> list1 = new ArrayList<>();\n    List<List<TreeNode>> list = new ArrayList<>();\n    public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) {\n        hxbl(root,p);\n        hxbl(root,q);\n        \n        return findSame(list.get(0),list.get(1));\n    }\n\n    /**\n     * 标准的（+left+{}+right+)结构\n     * @param root\n     * @param target\n     */\n    public void hxbl(TreeNode root,TreeNode target){\n    \n        list1.add(root);\n        if (root.val == target.val){\n        \n            list.add(new ArrayList<>(list1));\n        }\n    \n        if (root.left != null){\n            hxbl(root.left,target);\n        }\n        if (root.right != null){\n            hxbl(root.right,target);\n        }\n        list1.remove(list1.size()-1);\n\n    }\n\n    /**\n     * 找到这两个序列的最近元素，要从后往前，碰到了就break\n     * @param list1\n     * @param list2\n     * @return\n     */\n    public TreeNode findSame(List<TreeNode> list1, List<TreeNode> list2){\n        for (int i = list1.size()-1; i >=0; i--) {\n            for (int j = list2.size()-1; j >=0; j--) {\n                if (list1.get(i).val == list2.get(j).val){\n                    return list1.get(i);\n                }\n            }\n        }\n        return null;\n    }\n    \n}\n```\n\n## 2023.8.3\n\n```java\nclass Solution {\n    List<TreeNode> treeList = new ArrayList<>();\n    public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) {\n        find(root,p);\n        List<TreeNode> treeList1 = new ArrayList<>(treeList);\n        treeList = new ArrayList<>();\n        find(root,q);\n        List<TreeNode> treeList2 = new ArrayList<>(treeList);\n        boolean flag = true;\n        if (treeList1.size()>treeList2.size()){\n            flag = false;\n        }\n        for (int i = 0;i<Math.max(treeList1.size(),treeList2.size());i++){\n            if (!flag && treeList2.contains(treeList1.get(i))){\n                return treeList1.get(i);\n            }\n            if (flag && treeList1.contains(treeList2.get(i))){\n                return treeList2.get(i);\n            }\n        }\n        return null;\n    }\n    \n    public boolean find(TreeNode root,TreeNode p){\n          if (root!=null){\n              boolean left = find(root.left,p);\n              boolean right = find(root.right,p);\n              if (root == p||left||right){\n                  treeList.add(root);\n                  return true;\n              }\n              else return false;\n          }\n          else {\n              return false;\n          }\n    }\n}\n```","tags":["刷题笔记"]},{"title":"leetcode-2024-3-21","url":"/2024/03/21/leetcode-2024-3-21/","content":"# 2671 频率跟踪器(Medium)\n请你设计并实现一个能够对其中的值进行跟踪的数据结构，并支持对频率相关查询进行应答。\n\n实现 FrequencyTracker 类：\n\nFrequencyTracker()：使用一个空数组初始化 FrequencyTracker 对象。\n\nvoid add(int number)：添加一个 number 到数据结构中。\n\nvoid deleteOne(int number)：从数据结构中删除一个 number 。数据结构 可能不包含 number ，在这种情况下不删除任何内容。\n\nbool hasFrequency(int frequency): 如果数据结构中存在出现 frequency 次的数字，则返回 true，否则返回 false。\n\n\n示例 1：\n\n输入\n\n[\"FrequencyTracker\", \"add\", \"add\", \"hasFrequency\"]\n\n[[], [3], [3], [2]]\n\n输出\n\n[null, null, null, true]\n\n解释\n\nFrequencyTracker frequencyTracker = new FrequencyTracker();\n\nfrequencyTracker.add(3); // 数据结构现在包含 [3]\n\nfrequencyTracker.add(3); // 数据结构现在包含 [3, 3]\n\nfrequencyTracker.hasFrequency(2); // 返回 true ，因为 3 出现 2 次\n\n## 双哈希\n>一个用来存频率，一个用来存频率的频率\n```java\npublic class FrequencyTracker {\n    private HashMap<Integer,Integer> hashMap;\n    private TreeMap<Integer,Integer> treeMap;\n\n    public FrequencyTracker() {\n        hashMap = new HashMap<>();\n        treeMap = new TreeMap<>();\n    }\n\n    public void add(int number) {\n        if (!hashMap.containsKey(number)){\n            hashMap.put(number,1);\n            treeMap.put(1,treeMap.getOrDefault(1,0)+1);\n        }\n        else {\n            Integer integer = hashMap.get(number);\n            hashMap.put(number,integer+1);\n            if (treeMap.containsKey(integer)){\n                int count = treeMap.get(integer);\n                if (count == 1){\n                    treeMap.remove(integer);\n                }\n                else{\n                    treeMap.replace(integer,count-1);\n                }\n                treeMap.put(integer+1,treeMap.getOrDefault(integer+1,0)+1);\n            }\n        }\n    }\n\n    public void deleteOne(int number) {\n\n        if (hashMap.containsKey(number)){\n            int i1 = hashMap.get(number);\n            int temp = treeMap.get(i1);\n            if (temp == 1){\n                treeMap.remove(i1);\n            }\n            else {\n                treeMap.replace(i1,temp-1);\n            }\n            if (i1-1 !=0){\n                if (treeMap.containsKey(i1-1)){\n                    treeMap.put(i1-1,treeMap.get(i1-1)+1);\n                }\n                else {\n                    treeMap.put(i1-1,1);\n                }\n            }\n\n            if (i1 == 1){\n                hashMap.remove(number);\n            }\n            else {\n                hashMap.put(number,i1-1);\n            }\n\n        }\n    }\n\n    public boolean hasFrequency(int frequency) {\n\n        if (treeMap.containsKey(frequency)){\n            int temp = treeMap.get(frequency);\n            if (temp > 0){\n                return true;\n            }\n            else {\n                return false;\n            }\n        }\n        else {\n            return false;\n        }\n    }\n\n    public static void main(String[] args) {\n        FrequencyTracker frequencyTracker = new FrequencyTracker();\n\n        frequencyTracker.add(2);\n\n        frequencyTracker.add(7);\n\n        frequencyTracker.add(7);\n        frequencyTracker.add(3);\n        frequencyTracker.add(3);\n\n        frequencyTracker.deleteOne(7);\n        frequencyTracker.deleteOne(7);\n        System.out.println(frequencyTracker.hasFrequency(2));\n        System.out.println(frequencyTracker.hasFrequency(1));\n    }\n\n}\n\n```","tags":["刷题笔记"]},{"title":"redis实现分布式锁-初见分布式锁","url":"/2024/03/20/redis实现分布式锁-初见分布式锁/","content":"# 什么是分布式锁\n\n分布式锁是控制分布式系统之间同步访问共享资源的一种方式，通过互斥来保持一致性。\n\n了解分布式锁之前先了解下线程锁和进程锁：\n\n**线程锁**：主要用来给方法、代码块加锁。当某个方法或代码使用锁，在同一时刻仅有一个线程执行该方法或该代码段。线程锁只在同一JVM中有效果，因为线程锁的实现在根本上是依靠线程之间共享内存实现的，比如Synchronized、Lock等\n\n**进程锁**：控制同一操作系统中多个进程访问某个共享资源，因为进程具有独立性，各个进程无法访问其他进程的资源，因此无法通过synchronized等线程锁实现进程锁\n\n比如Golang语言中的sync包就提供了基本的同步基元，如互斥锁\n\n但是以上两种适合在单体架构应用，但是分布式系统中多个服务节点，多个进程分散部署在不同节点机器中，此时对于资源的竞争，上诉两种对节点本地资源的锁就无效了。\n\n这个时候就需要分布式锁来对分布式系统多进程访问资源进行控制，因此分布式锁是为了解决分布式互斥问题！\n\n![img.png](../img/coding/2024_3_20_2.png)\n","tags":["Redis"]},{"title":"leetcode-2024-3-20","url":"/2024/03/20/leetcode-2024-3-20/","content":"# 1969.数组元素的最小非零乘积（Medium）\n给你一个正整数 **p** 。你有一个下标从 **1** 开始的数组 **nums** ，这个数组包含范围 [1, 2p - 1] 内所有整数的二进制形式（两端都 **包含**）。你可以进行以下操作 **任意** 次：\n\n从 nums 中选择两个元素 x 和 y  。\n选择 x 中的一位与 y 对应位置的位交换。对应位置指的是两个整数 相同位置 的二进制位。\n比方说，如果 x = 11**0**1 且 y = 00**1**1 ，交换右边数起第 2 位后，我们得到 x = 11**1**1 和 y = 00**0**1 。\n\n请你算出进行以上操作 **任意次** 以后，nums 能得到的 **最小非零** 乘积。将乘积对 109 + 7 取余 后返回。\n\n注意：答案应为取余 **之前** 的最小值。\n\n> ~~自己没想出来~~，其实有思路了就是总感觉不对劲，看了答案如果继续想应该能想出来，但是问题是这个取余肯定也会困扰我很久，还有我肯定只会暴力求幂，这里的**快速幂**其实是很值得学习一下的\n\n## 官方解：贪心+快速幂\n\n>   x * y肯定是要比(x-1) * (y+1)大的，那么什么时候会有最小值呢，就是当x=1，y变得最大的时候，对于都是二进制来说，能通过交换位来变大只能是x=1,y=2^(p-1)-2了，比如3位的时候，最小就是1，最大就是6，要保持x和y互为**反码**\n\n>那这道题就是一个纯数学问题了，每次取两个互为**反码**的数都能将其变成x=1，y=2^p-2，那么一共有多少对这样的反码对呢。\n>n=(2^p-2)/2，除了最后面那个数其他都可以凑对，就有2^(p-1)-1对。\n> p=3的时候，就有3对，p=4时，就有7对\n \n>也就是说，我们的最终答案是**最后一个元素*（最后一个元素-1）^(2^(p-1)-1)**\n> 即：![](../img/coding/2024-3-20-1.png)\n\n>**快速幂**：通过观察 ***p=3的时候，就有3对，p=4时，就有7对***，可以发现3就是11，7就是111，根据幂的公式x^(111)=x^(100)*x^(010)*x^(001),\n> 答案里巧妙地用快速幂解决，也就是说，不再是暴力连乘，而是x也自己进行幂运算，就可以将复杂度降为o(log)\n\n```java\nprivate static final int MOD = 1_000_000_007;\n//这里的p就是已经log后的，一共有几位，每一位都循环一次\nprivate long pow(long x, int p) {\n    x %= MOD;\n    long res = 1;\n    while (p-- > 0) {\n        res = res * x % MOD;\n        x = x * x % MOD;\n    }\n    return res;\n}\n\npublic int minNonZeroProduct(int p) {\n    //移位，1L是long单位下的1，将其像左边移动p位\n    //得到的k就是最后一个元素\n    long k = (1L << p) - 1;\n    //最后一个元素*（最后一个元素-1）^(2^(p-1)-1)\n    return (int) (k % MOD * pow(k - 1, p - 1) % MOD);\n}\n```\n\n没想出来这种方法，贪心还是练习少了\n\n# 49.字母异位词分组（Medium）\n\n给你一个字符串数组，请你将 字母异位词 组合在一起。可以按任意顺序返回结果列表。\n\n字母异位词 是由重新排列源单词的所有字母得到的一个新单词。\n\n示例 1:\n\n输入: strs = [\"eat\", \"tea\", \"tan\", \"ate\", \"nat\", \"bat\"]\n输出: [[\"bat\"],[\"nat\",\"tan\"],[\"ate\",\"eat\",\"tea\"]]\n\n## 自己解（也是官方解）：哈希表+排序\n\n>遍历每个字符串，然后在对其排序，如果哈希表没有就加进去，list把这个排序之前的字符串放进去，有就在哈希表已有的键里面添加这个字符串。\n\n```java\npublic List<List<String>> groupAnagrams(String[] strs) {\n    HashMap<String,List<String>> hashMap = new HashMap<>();\n    List<List<String>> ans = new ArrayList<>();\n    for (String s:strs){\n        String temp = getAllStr(s);\n        //如果表里没有就新增键值对\n        if (!hashMap.containsKey(temp)){\n            List<String> list = new ArrayList<>();\n            list.add(s);\n            hashMap.put(temp,list);\n        }\n        //有就在末尾添加\n        else {\n            List<String> list = hashMap.get(temp);\n            list.add(s);\n            hashMap.replace(temp,list);\n        }\n    }\n    //遍历map存在list中\n    for (Map.Entry<String,List<String>> entry : hashMap.entrySet()){\n        ans.add(entry.getValue());\n    }\n    return ans;\n}\n//字符串变成char数组，排序，这样就能得到唯一\npublic String getAllStr(String s){\n    char[] temp = s.toCharArray();\n    Arrays.sort(temp);\n    return Arrays.toString(temp);\n}\n```\n\n比较简单\n\n# 128.最长连续序列（Medium）\n\n给定一个未排序的整数数组 nums ，找出数字连续的最长序列（不要求序列元素在原数组中连续）的长度。\n\n请你设计并实现时间复杂度为 **(n)** 的算法解决此问题。\n\n示例 1：\n\n输入：nums = [100,4,200,1,3,2]\n\n输出：4\n\n解释：最长数字连续序列是 [1, 2, 3, 4]。它的长度为 4。\n\n## 暴力解：（其实是超时的，因为复杂度为o(nlogn)）\n\n> 最傻逼的一集，这里**Arrays.sort**就已经超出复杂度了，但是居然也没超时，甚至比超过90%，感觉有点。\n\n> 最简单的思路，排序然后指针，从小到大如果一直递增就计数器一直加，如果相等那就计数器不动，如果不是，那就计数器重新变为1 \n```java\npublic int longestConsecutive(int[] nums) {\n    if (nums == null){\n        return 0;\n    }\n    //搞笑的排序\n    Arrays.sort(nums);\n    int count = 1;\n    int max = 1;\n    for (int i = 1; i < nums.length; i++) {\n        //当且仅当当前元素是之前元素的多1\n        if (nums[i]==nums[i-1]+1){\n            count++;\n        }\n        //如果相等计数器空过\n        else if (nums[i] == nums[i-1]) {\n\n        }\n        //如果不是这样的情况就不是连续，计数器变成1\n        else {\n            count = 1;\n        }\n        max = Math.max(max,count);\n        System.out.println(count);\n    }\n\n    return max;\n}\n```\n\n## 官方解：哈希表\n\n>我们考虑枚举数组中的每个数 x，考虑以其为起点，不断尝试匹配 x+1,x+2... 是否存在，假设最长匹配到了 x+y，那么以 x 为起点的最长连续序列即为 x,x+1,x+2,x+y，其长度为 y+1，我们不断枚举并更新答案即可。\n\n>对于匹配的过程，暴力的方法是 O(n) 遍历数组去看是否存在这个数，但其实更高效的方法是用一个哈希表存储数组中的数，这样查看一个数是否存在即能优化至 O(1) 的时间复杂度。\n\n>仅仅是这样我们的算法时间复杂度最坏情况下还是会达到 O(n2)\n即外层需要枚举 O(n) 个数，内层需要暴力匹配 O(n) 次），无法满足题目的要求。但仔细分析这个过程，我们会发现其中执行了很多不必要的枚举，如果已知有一个 x,x+1,x+2,⋯,x+y 的连续序列，而我们却重新从 x+1，x+2 或者是 x+y 处开始尝试匹配，那么得到的结果肯定不会优于枚举 x 为起点的答案，因此我们在外层循环的时候碰到这种情况跳过即可。\n\n>那么怎么判断是否跳过呢？由于我们要枚举的数 x 一定是在数组中不存在前驱数 x−1 的，不然按照上面的分析我们会从 x−1 开始尝试匹配，因此我们每次在哈希表中检查是否存在 x−1 即能判断是否需要跳过了。\n\n```java\npublic int longestConsecutive(int[] nums) {\n    Set<Integer> num_set = new HashSet<Integer>();\n    //先去重，用集合\n    for (int num : nums) {\n        num_set.add(num);\n    }\n\n    int longestStreak = 0;\n    //有下一个值的时候就跳掉，一定要没有下一个，这样遍历才是o(n)\n    for (int num : num_set) {\n        if (!num_set.contains(num - 1)) {\n            int currentNum = num;\n            int currentStreak = 1;\n            //然后比较一下\n            while (num_set.contains(currentNum + 1)) {\n                currentNum += 1;\n                currentStreak += 1;\n            }\n\n            longestStreak = Math.max(longestStreak, currentStreak);\n        }\n    }\n\n    return longestStreak;\n}\n\n```\n\n非常巧妙的o(n),这里set查询是o(1)，需要好好学习一下，多用用哈希","tags":["刷题笔记"]},{"title":"leetcode-2024-3-19","url":"/2024/03/19/leetcode-2024-3-19/","content":"# 1793.好子数组的最大分数（Hard）\n\n给你一个整数数组 nums （下标从 0 开始）和一个整数 k 。\n\n一个子数组 (i, j) 的 **分数** 定义为 min(nums[i], nums[i+1], ..., nums[j]) * (j - i + 1) 。一个 好 子数组的两个端点下标需要满足 i <= k <= j 。\n\n请你返回 **好** 子数组的最大可能 **分数** 。\n\n## 暴力解：超出内存限制\n\n> 思路：用一个二维数组，dp[i][j]表示num[i]到num[j]的最小值，起始dp[i][i]都为num[i],状态转移方程为：dp[i][j] = min{dp[i][j-1],num[i]},\n> 其实是可以做出来的。但是这是hard，空间复杂度要求有点高，就会超内存。我的评价是动态规划很好，下次别用了\n\n```java\npublic int maximumScore(int[] nums, int k) {\n    //二维动态规划\n    int[][] dp = new int[nums.length][nums.length];\n    //初始对角线都为自身，自己肯定是最小值\n    for (int i = 0; i < nums.length; i++) {\n        dp[i][i] = nums[i];\n    }\n    for (int i = 0; i < nums.length-1; i++) {\n        for (int j = i+1; j < nums.length; j++) {\n            //动态转移方程\n            dp[i][j] = Math.min(nums[j], dp[i][j - 1]);\n        }\n    }\n    //k要在中间，那么就是矩阵的右上角那块，i<=k<=j\n    int max = Integer.MIN_VALUE;\n    for (int i = 0; i <= k; i++) {\n        for (int j = k; j < nums.length; j++) {\n            int length = j-i+1;\n            max = Math.max(max,length*dp[i][j]);\n        }\n    }\n    //返回在右上角的最大值\n    return max;\n}\n```\n## 官方解 ：双指针\n> 以k为原点，head=k-1，rear=k+1，左开右开区间，数据长度rear-head+1，初始时长度只有1即为nums[k]本身，也为最小值。\n> 那么怎么样找包含他的最小值呢，只要找左右两边比他大的，整体的最小值就是不变的，那么长度*最小值就会有效变大，直到双指针遇到比当前最小值还要小的元素\n> 那么由于初始最小值就是本身，那么只要循环本身这个值不断递减就可以了。\n```java\npublic int maximumScore(int[] nums, int k) {\n    int length = nums.length;\n    int head = k-1,rear = k+1;\n    int max = Integer.MIN_VALUE;\n    for (int i = nums[k];;i--){\n        while (head >=0 && nums[head] >= i){\n            head--;\n        }\n        while (rear < nums.length && nums[rear] >= i){\n            rear++;\n        }\n        max = Math.max(max,(rear-head-1)*i);\n        if (head == -1 && rear == length){\n            break;\n        }\n    }\n    return max;\n}\n```\n复杂度分析\n\n时间复杂度：O(n+C)，其中 n 是数组nums 的长度，C 是数组 nums 中元素的范围。\n\n空间复杂度：O(1)。明显比我自己写的暴力解好\n\n","tags":["刷题笔记"]},{"title":"我的第一篇blog","url":"/2024/03/19/我的第一篇blog/","content":"  做这个博客的目的是为了能激励自己，不要看了一遍就觉得自己会了。好记性不如烂笔头，刷题的思路尽量多写，很难说下次碰到一样的题目会不会忘掉。\n\n  还有25天蓝桥杯，让我这个大四老狗随便得个奖吧~~\n\n  ![上海外国语大学校花](../img/coding/2024_3_19_1.JPG)\n","tags":["闲谈"]}]